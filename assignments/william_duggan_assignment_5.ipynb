{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tajfsk_7JY3E"
      },
      "source": [
        "**Note to grader:** Each question consists of parts, e.g. Q1(i), Q1(ii), etc. Each part must be graded  on a 0-4 scale, following the standard NJIT convention (A:4, B+: 3.5, B:3, C+: 2.5, C: 2, D:1, F:0). \n",
        "The total score must be re-scaled to 100 -- that should apply to all future assignments so that Canvas assigns the same weight on all assignments. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zPnHTf9MfT5X"
      },
      "outputs": [],
      "source": [
        "# Grader's area\n",
        "import numpy as np\n",
        "G = np.zeros([10,10])\n",
        "maxScore = 0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1NnOFlqPI1qn"
      },
      "source": [
        "# **Assignment 5**\n",
        "<br>\n",
        "\n",
        "<font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "zSyX_FIzI1qn"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Math\n",
        "from IPython.display import Latex\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import struct\n",
        "import time\n",
        "\n",
        "from sklearn.metrics import classification_report,confusion_matrix\n",
        "from sklearn.neural_network import MLPClassifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cc6kqRYyliiK"
      },
      "source": [
        "----------------\n",
        "----------------\n",
        "----------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXayK-GxI1qo"
      },
      "source": [
        "## <font color='blue'> Question 0. MLP with scikit-learn: Explore outputs for the MNIST dataset </font>\n",
        "\n",
        "The purpose of this question is to get some practice time with MLP classifiers. We will be using the the MNIST data set. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "xscTgg2dI1qo"
      },
      "outputs": [],
      "source": [
        "# Let's first load the MNIST data set. \n",
        "\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "X, y = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
        "y = y.astype(int)\n",
        "X = ((X / 255.) - .5) * 2\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=10000, random_state=123, stratify=y)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYAL83hpI1qp"
      },
      "source": [
        "<br> **Q0-0**: This the classifier we built in class. Find a setting for *batch_size* and *max_iter*, so that the subsequent fit function **converges**. \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "as0JIU6mlL9L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43dcf8fe-d68e-4400-d1fb-507f0c2e719e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For batches = 1:\n",
            "============================\n",
            "Iteration 1, loss = 0.36409075\n",
            "Iteration 2, loss = 0.21798108\n",
            "Iteration 3, loss = 0.18618860\n",
            "Iteration 4, loss = 0.17054453\n",
            "Iteration 5, loss = 0.15971247\n",
            "Iteration 6, loss = 0.15268641\n",
            "Iteration 7, loss = 0.14885618\n",
            "Iteration 8, loss = 0.14533525\n",
            "Iteration 9, loss = 0.14052036\n",
            "Iteration 10, loss = 0.13990915\n",
            "Iteration 11, loss = 0.13648716\n",
            "Iteration 12, loss = 0.13539818\n",
            "Iteration 13, loss = 0.13337102\n",
            "Iteration 14, loss = 0.13243079\n",
            "Iteration 15, loss = 0.13060399\n",
            "Iteration 16, loss = 0.12905245\n",
            "Iteration 17, loss = 0.13019904\n",
            "Iteration 18, loss = 0.13038744\n",
            "Iteration 19, loss = 0.12916254\n",
            "Iteration 20, loss = 0.12822849\n",
            "Iteration 21, loss = 0.12739004\n",
            "Iteration 22, loss = 0.12650114\n",
            "Iteration 23, loss = 0.12705125\n",
            "Iteration 24, loss = 0.12572560\n",
            "Iteration 25, loss = 0.12447591\n",
            "Iteration 26, loss = 0.12371491\n",
            "Iteration 27, loss = 0.12405413\n",
            "Iteration 28, loss = 0.12306656\n",
            "Iteration 29, loss = 0.12157982\n",
            "Iteration 30, loss = 0.12119172\n",
            "Iteration 31, loss = 0.12163601\n",
            "Iteration 32, loss = 0.12179452\n",
            "Iteration 33, loss = 0.12091317\n",
            "Iteration 34, loss = 0.11984255\n",
            "Iteration 35, loss = 0.11984527\n",
            "Iteration 36, loss = 0.11971556\n",
            "Iteration 37, loss = 0.11997399\n",
            "Iteration 38, loss = 0.11866100\n",
            "Iteration 39, loss = 0.12043109\n",
            "Iteration 40, loss = 0.11851602\n",
            "Iteration 41, loss = 0.11906552\n",
            "Iteration 42, loss = 0.11825320\n",
            "Iteration 43, loss = 0.11958027\n",
            "Iteration 44, loss = 0.11747205\n",
            "Iteration 45, loss = 0.11784991\n",
            "Iteration 46, loss = 0.11824698\n",
            "Iteration 47, loss = 0.11846459\n",
            "Iteration 48, loss = 0.11800584\n",
            "Iteration 49, loss = 0.11658446\n",
            "Iteration 50, loss = 0.11671869\n",
            "\n",
            "For batches = 2:\n",
            "============================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 0.41816270\n",
            "Iteration 2, loss = 0.22834275\n",
            "Iteration 3, loss = 0.18660094\n",
            "Iteration 4, loss = 0.16223454\n",
            "Iteration 5, loss = 0.14608810\n",
            "Iteration 6, loss = 0.13496862\n",
            "Iteration 7, loss = 0.12731714\n",
            "Iteration 8, loss = 0.11992278\n",
            "Iteration 9, loss = 0.11414084\n",
            "Iteration 10, loss = 0.10973318\n",
            "Iteration 11, loss = 0.10538907\n",
            "Iteration 12, loss = 0.10230528\n",
            "Iteration 13, loss = 0.09933187\n",
            "Iteration 14, loss = 0.09619390\n",
            "Iteration 15, loss = 0.09400940\n",
            "Iteration 16, loss = 0.09186109\n",
            "Iteration 17, loss = 0.09002237\n",
            "Iteration 18, loss = 0.08873648\n",
            "Iteration 19, loss = 0.08685205\n",
            "Iteration 20, loss = 0.08553451\n",
            "Iteration 21, loss = 0.08377760\n",
            "Iteration 22, loss = 0.08277835\n",
            "Iteration 23, loss = 0.08179504\n",
            "Iteration 24, loss = 0.08034230\n",
            "Iteration 25, loss = 0.07928413\n",
            "Iteration 26, loss = 0.07855795\n",
            "Iteration 27, loss = 0.07752660\n",
            "Iteration 28, loss = 0.07669305\n",
            "Iteration 29, loss = 0.07601538\n",
            "Iteration 30, loss = 0.07440825\n",
            "Iteration 31, loss = 0.07439531\n",
            "Iteration 32, loss = 0.07426979\n",
            "Iteration 33, loss = 0.07338356\n",
            "Iteration 34, loss = 0.07227646\n",
            "Iteration 35, loss = 0.07204043\n",
            "Iteration 36, loss = 0.07196655\n",
            "Iteration 37, loss = 0.07146767\n",
            "Iteration 38, loss = 0.07058078\n",
            "Iteration 39, loss = 0.07020783\n",
            "Iteration 40, loss = 0.06957928\n",
            "Iteration 41, loss = 0.06944221\n",
            "Iteration 42, loss = 0.06915029\n",
            "Iteration 43, loss = 0.06920199\n",
            "Iteration 44, loss = 0.06813145\n",
            "Iteration 45, loss = 0.06814665\n",
            "Iteration 46, loss = 0.06777378\n",
            "Iteration 47, loss = 0.06741811\n",
            "Iteration 48, loss = 0.06688299\n",
            "Iteration 49, loss = 0.06747626\n",
            "Iteration 50, loss = 0.06681018\n",
            "\n",
            "For batches = 3:\n",
            "============================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 0.47391855\n",
            "Iteration 2, loss = 0.25022580\n",
            "Iteration 3, loss = 0.20430741\n",
            "Iteration 4, loss = 0.17650754\n",
            "Iteration 5, loss = 0.15716117\n",
            "Iteration 6, loss = 0.14360674\n",
            "Iteration 7, loss = 0.13370912\n",
            "Iteration 8, loss = 0.12535359\n",
            "Iteration 9, loss = 0.11829558\n",
            "Iteration 10, loss = 0.11282273\n",
            "Iteration 11, loss = 0.10763728\n",
            "Iteration 12, loss = 0.10352080\n",
            "Iteration 13, loss = 0.09962753\n",
            "Iteration 14, loss = 0.09613429\n",
            "Iteration 15, loss = 0.09302272\n",
            "Iteration 16, loss = 0.09037368\n",
            "Iteration 17, loss = 0.08770596\n",
            "Iteration 18, loss = 0.08560951\n",
            "Iteration 19, loss = 0.08341594\n",
            "Iteration 20, loss = 0.08171716\n",
            "Iteration 21, loss = 0.07939195\n",
            "Iteration 22, loss = 0.07795113\n",
            "Iteration 23, loss = 0.07668755\n",
            "Iteration 24, loss = 0.07479136\n",
            "Iteration 25, loss = 0.07344245\n",
            "Iteration 26, loss = 0.07219136\n",
            "Iteration 27, loss = 0.07083097\n",
            "Iteration 28, loss = 0.06955540\n",
            "Iteration 29, loss = 0.06880859\n",
            "Iteration 30, loss = 0.06740484\n",
            "Iteration 31, loss = 0.06632864\n",
            "Iteration 32, loss = 0.06611622\n",
            "Iteration 33, loss = 0.06478144\n",
            "Iteration 34, loss = 0.06382270\n",
            "Iteration 35, loss = 0.06336284\n",
            "Iteration 36, loss = 0.06226224\n",
            "Iteration 37, loss = 0.06171171\n",
            "Iteration 38, loss = 0.06114200\n",
            "Iteration 39, loss = 0.06046747\n",
            "Iteration 40, loss = 0.05928272\n",
            "Iteration 41, loss = 0.05903968\n",
            "Iteration 42, loss = 0.05892862\n",
            "Iteration 43, loss = 0.05795725\n",
            "Iteration 44, loss = 0.05747104\n",
            "Iteration 45, loss = 0.05710440\n",
            "Iteration 46, loss = 0.05657121\n",
            "Iteration 47, loss = 0.05625345\n",
            "Iteration 48, loss = 0.05563990\n",
            "Iteration 49, loss = 0.05544813\n",
            "Iteration 50, loss = 0.05500355\n",
            "\n",
            "For batches = 4:\n",
            "============================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 0.52538955\n",
            "Iteration 2, loss = 0.26966689\n",
            "Iteration 3, loss = 0.22237072\n",
            "Iteration 4, loss = 0.19286932\n",
            "Iteration 5, loss = 0.17169039\n",
            "Iteration 6, loss = 0.15652496\n",
            "Iteration 7, loss = 0.14489375\n",
            "Iteration 8, loss = 0.13530177\n",
            "Iteration 9, loss = 0.12725138\n",
            "Iteration 10, loss = 0.12085916\n",
            "Iteration 11, loss = 0.11497522\n",
            "Iteration 12, loss = 0.11010492\n",
            "Iteration 13, loss = 0.10565006\n",
            "Iteration 14, loss = 0.10174843\n",
            "Iteration 15, loss = 0.09828651\n",
            "Iteration 16, loss = 0.09498352\n",
            "Iteration 17, loss = 0.09186967\n",
            "Iteration 18, loss = 0.08924172\n",
            "Iteration 19, loss = 0.08678439\n",
            "Iteration 20, loss = 0.08450507\n",
            "Iteration 21, loss = 0.08213103\n",
            "Iteration 22, loss = 0.08024098\n",
            "Iteration 23, loss = 0.07874509\n",
            "Iteration 24, loss = 0.07647642\n",
            "Iteration 25, loss = 0.07487524\n",
            "Iteration 26, loss = 0.07340326\n",
            "Iteration 27, loss = 0.07195380\n",
            "Iteration 28, loss = 0.07051714\n",
            "Iteration 29, loss = 0.06938821\n",
            "Iteration 30, loss = 0.06806847\n",
            "Iteration 31, loss = 0.06658957\n",
            "Iteration 32, loss = 0.06589291\n",
            "Iteration 33, loss = 0.06451662\n",
            "Iteration 34, loss = 0.06335725\n",
            "Iteration 35, loss = 0.06283015\n",
            "Iteration 36, loss = 0.06149591\n",
            "Iteration 37, loss = 0.06072596\n",
            "Iteration 38, loss = 0.06001912\n",
            "Iteration 39, loss = 0.05907799\n",
            "Iteration 40, loss = 0.05795636\n",
            "Iteration 41, loss = 0.05735875\n",
            "Iteration 42, loss = 0.05699339\n",
            "Iteration 43, loss = 0.05596605\n",
            "Iteration 44, loss = 0.05536160\n",
            "Iteration 45, loss = 0.05489445\n",
            "Iteration 46, loss = 0.05404641\n",
            "Iteration 47, loss = 0.05369381\n",
            "Iteration 48, loss = 0.05304945\n",
            "Iteration 49, loss = 0.05246737\n",
            "Iteration 50, loss = 0.05173505\n",
            "\n",
            "For batches = 5:\n",
            "============================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 0.57329662\n",
            "Iteration 2, loss = 0.28629488\n",
            "Iteration 3, loss = 0.23811091\n",
            "Iteration 4, loss = 0.20762641\n",
            "Iteration 5, loss = 0.18553770\n",
            "Iteration 6, loss = 0.16929318\n",
            "Iteration 7, loss = 0.15646485\n",
            "Iteration 8, loss = 0.14587802\n",
            "Iteration 9, loss = 0.13694593\n",
            "Iteration 10, loss = 0.12970190\n",
            "Iteration 11, loss = 0.12320933\n",
            "Iteration 12, loss = 0.11772963\n",
            "Iteration 13, loss = 0.11267477\n",
            "Iteration 14, loss = 0.10835127\n",
            "Iteration 15, loss = 0.10453760\n",
            "Iteration 16, loss = 0.10073217\n",
            "Iteration 17, loss = 0.09726995\n",
            "Iteration 18, loss = 0.09425700\n",
            "Iteration 19, loss = 0.09151431\n",
            "Iteration 20, loss = 0.08889736\n",
            "Iteration 21, loss = 0.08643857\n",
            "Iteration 22, loss = 0.08418907\n",
            "Iteration 23, loss = 0.08249839\n",
            "Iteration 24, loss = 0.08000014\n",
            "Iteration 25, loss = 0.07834150\n",
            "Iteration 26, loss = 0.07665299\n",
            "Iteration 27, loss = 0.07502774\n",
            "Iteration 28, loss = 0.07348437\n",
            "Iteration 29, loss = 0.07206173\n",
            "Iteration 30, loss = 0.07059095\n",
            "Iteration 31, loss = 0.06904452\n",
            "Iteration 32, loss = 0.06804828\n",
            "Iteration 33, loss = 0.06670843\n",
            "Iteration 34, loss = 0.06531548\n",
            "Iteration 35, loss = 0.06463879\n",
            "Iteration 36, loss = 0.06318098\n",
            "Iteration 37, loss = 0.06233041\n",
            "Iteration 38, loss = 0.06147756\n",
            "Iteration 39, loss = 0.06044707\n",
            "Iteration 40, loss = 0.05937900\n",
            "Iteration 41, loss = 0.05860980\n",
            "Iteration 42, loss = 0.05802491\n",
            "Iteration 43, loss = 0.05700755\n",
            "Iteration 44, loss = 0.05620678\n",
            "Iteration 45, loss = 0.05553365\n",
            "Iteration 46, loss = 0.05483653\n",
            "Iteration 47, loss = 0.05419409\n",
            "Iteration 48, loss = 0.05357554\n",
            "Iteration 49, loss = 0.05269414\n",
            "Iteration 50, loss = 0.05199743\n",
            "\n",
            "For batches = 6:\n",
            "============================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 0.61822895\n",
            "Iteration 2, loss = 0.30108391\n",
            "Iteration 3, loss = 0.25182683\n",
            "Iteration 4, loss = 0.22089820\n",
            "Iteration 5, loss = 0.19821536\n",
            "Iteration 6, loss = 0.18117881\n",
            "Iteration 7, loss = 0.16758293\n",
            "Iteration 8, loss = 0.15625059\n",
            "Iteration 9, loss = 0.14671785\n",
            "Iteration 10, loss = 0.13882606\n",
            "Iteration 11, loss = 0.13184833\n",
            "Iteration 12, loss = 0.12588696\n",
            "Iteration 13, loss = 0.12044556\n",
            "Iteration 14, loss = 0.11572351\n",
            "Iteration 15, loss = 0.11157120\n",
            "Iteration 16, loss = 0.10742323\n",
            "Iteration 17, loss = 0.10374822\n",
            "Iteration 18, loss = 0.10040038\n",
            "Iteration 19, loss = 0.09739558\n",
            "Iteration 20, loss = 0.09459058\n",
            "Iteration 21, loss = 0.09195422\n",
            "Iteration 22, loss = 0.08948607\n",
            "Iteration 23, loss = 0.08757514\n",
            "Iteration 24, loss = 0.08491848\n",
            "Iteration 25, loss = 0.08316765\n",
            "Iteration 26, loss = 0.08124015\n",
            "Iteration 27, loss = 0.07948148\n",
            "Iteration 28, loss = 0.07786144\n",
            "Iteration 29, loss = 0.07622574\n",
            "Iteration 30, loss = 0.07465008\n",
            "Iteration 31, loss = 0.07293402\n",
            "Iteration 32, loss = 0.07181401\n",
            "Iteration 33, loss = 0.07042479\n",
            "Iteration 34, loss = 0.06892195\n",
            "Iteration 35, loss = 0.06806028\n",
            "Iteration 36, loss = 0.06659139\n",
            "Iteration 37, loss = 0.06559924\n",
            "Iteration 38, loss = 0.06462972\n",
            "Iteration 39, loss = 0.06355332\n",
            "Iteration 40, loss = 0.06238127\n",
            "Iteration 41, loss = 0.06148556\n",
            "Iteration 42, loss = 0.06074516\n",
            "Iteration 43, loss = 0.05975464\n",
            "Iteration 44, loss = 0.05883334\n",
            "Iteration 45, loss = 0.05800258\n",
            "Iteration 46, loss = 0.05724291\n",
            "Iteration 47, loss = 0.05648940\n",
            "Iteration 48, loss = 0.05581194\n",
            "Iteration 49, loss = 0.05479691\n",
            "Iteration 50, loss = 0.05407623\n",
            "\n",
            "For batches = 7:\n",
            "============================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 0.66077640\n",
            "Iteration 2, loss = 0.31461547\n",
            "Iteration 3, loss = 0.26389417\n",
            "Iteration 4, loss = 0.23271925\n",
            "Iteration 5, loss = 0.20979778\n",
            "Iteration 6, loss = 0.19220859\n",
            "Iteration 7, loss = 0.17806368\n",
            "Iteration 8, loss = 0.16620371\n",
            "Iteration 9, loss = 0.15619138\n",
            "Iteration 10, loss = 0.14779074\n",
            "Iteration 11, loss = 0.14040517\n",
            "Iteration 12, loss = 0.13405305\n",
            "Iteration 13, loss = 0.12829366\n",
            "Iteration 14, loss = 0.12320016\n",
            "Iteration 15, loss = 0.11879051\n",
            "Iteration 16, loss = 0.11432907\n",
            "Iteration 17, loss = 0.11045516\n",
            "Iteration 18, loss = 0.10681330\n",
            "Iteration 19, loss = 0.10360698\n",
            "Iteration 20, loss = 0.10061139\n",
            "Iteration 21, loss = 0.09775985\n",
            "Iteration 22, loss = 0.09515551\n",
            "Iteration 23, loss = 0.09304161\n",
            "Iteration 24, loss = 0.09022927\n",
            "Iteration 25, loss = 0.08837257\n",
            "Iteration 26, loss = 0.08626907\n",
            "Iteration 27, loss = 0.08436404\n",
            "Iteration 28, loss = 0.08262672\n",
            "Iteration 29, loss = 0.08087776\n",
            "Iteration 30, loss = 0.07918695\n",
            "Iteration 31, loss = 0.07732911\n",
            "Iteration 32, loss = 0.07606242\n",
            "Iteration 33, loss = 0.07462401\n",
            "Iteration 34, loss = 0.07302872\n",
            "Iteration 35, loss = 0.07202473\n",
            "Iteration 36, loss = 0.07055631\n",
            "Iteration 37, loss = 0.06941676\n",
            "Iteration 38, loss = 0.06837727\n",
            "Iteration 39, loss = 0.06720233\n",
            "Iteration 40, loss = 0.06596959\n",
            "Iteration 41, loss = 0.06498133\n",
            "Iteration 42, loss = 0.06410545\n",
            "Iteration 43, loss = 0.06315022\n",
            "Iteration 44, loss = 0.06212099\n",
            "Iteration 45, loss = 0.06120144\n",
            "Iteration 46, loss = 0.06037074\n",
            "Iteration 47, loss = 0.05955775\n",
            "Iteration 48, loss = 0.05878582\n",
            "Iteration 49, loss = 0.05770390\n",
            "Iteration 50, loss = 0.05694266\n",
            "\n",
            "For batches = 8:\n",
            "============================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 0.70119063\n",
            "Iteration 2, loss = 0.32742205\n",
            "Iteration 3, loss = 0.27470770\n",
            "Iteration 4, loss = 0.24332597\n",
            "Iteration 5, loss = 0.22027806\n",
            "Iteration 6, loss = 0.20237244\n",
            "Iteration 7, loss = 0.18787315\n",
            "Iteration 8, loss = 0.17557913\n",
            "Iteration 9, loss = 0.16523151\n",
            "Iteration 10, loss = 0.15641126\n",
            "Iteration 11, loss = 0.14867066\n",
            "Iteration 12, loss = 0.14198310\n",
            "Iteration 13, loss = 0.13595606\n",
            "Iteration 14, loss = 0.13054575\n",
            "Iteration 15, loss = 0.12586854\n",
            "Iteration 16, loss = 0.12115145\n",
            "Iteration 17, loss = 0.11710116\n",
            "Iteration 18, loss = 0.11321327\n",
            "Iteration 19, loss = 0.10978815\n",
            "Iteration 20, loss = 0.10664726\n",
            "Iteration 21, loss = 0.10358955\n",
            "Iteration 22, loss = 0.10086610\n",
            "Iteration 23, loss = 0.09852474\n",
            "Iteration 24, loss = 0.09560069\n",
            "Iteration 25, loss = 0.09362400\n",
            "Iteration 26, loss = 0.09138416\n",
            "Iteration 27, loss = 0.08933761\n",
            "Iteration 28, loss = 0.08748858\n",
            "Iteration 29, loss = 0.08564448\n",
            "Iteration 30, loss = 0.08384411\n",
            "Iteration 31, loss = 0.08184865\n",
            "Iteration 32, loss = 0.08045825\n",
            "Iteration 33, loss = 0.07892531\n",
            "Iteration 34, loss = 0.07728695\n",
            "Iteration 35, loss = 0.07611600\n",
            "Iteration 36, loss = 0.07464849\n",
            "Iteration 37, loss = 0.07337506\n",
            "Iteration 38, loss = 0.07227663\n",
            "Iteration 39, loss = 0.07099086\n",
            "Iteration 40, loss = 0.06969086\n",
            "Iteration 41, loss = 0.06863273\n",
            "Iteration 42, loss = 0.06762630\n",
            "Iteration 43, loss = 0.06670629\n",
            "Iteration 44, loss = 0.06557636\n",
            "Iteration 45, loss = 0.06459485\n",
            "Iteration 46, loss = 0.06368394\n",
            "Iteration 47, loss = 0.06283192\n",
            "Iteration 48, loss = 0.06195510\n",
            "Iteration 49, loss = 0.06086580\n",
            "Iteration 50, loss = 0.06006659\n",
            "\n",
            "For batches = 9:\n",
            "============================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 0.73973864\n",
            "Iteration 2, loss = 0.33970357\n",
            "Iteration 3, loss = 0.28454749\n",
            "Iteration 4, loss = 0.25288322\n",
            "Iteration 5, loss = 0.22974334\n",
            "Iteration 6, loss = 0.21168003\n",
            "Iteration 7, loss = 0.19688017\n",
            "Iteration 8, loss = 0.18434632\n",
            "Iteration 9, loss = 0.17370172\n",
            "Iteration 10, loss = 0.16456009\n",
            "Iteration 11, loss = 0.15653633\n",
            "Iteration 12, loss = 0.14955598\n",
            "Iteration 13, loss = 0.14331104\n",
            "Iteration 14, loss = 0.13761048\n",
            "Iteration 15, loss = 0.13269957\n",
            "Iteration 16, loss = 0.12773992\n",
            "Iteration 17, loss = 0.12353731\n",
            "Iteration 18, loss = 0.11943849\n",
            "Iteration 19, loss = 0.11583210\n",
            "Iteration 20, loss = 0.11254934\n",
            "Iteration 21, loss = 0.10930607\n",
            "Iteration 22, loss = 0.10644611\n",
            "Iteration 23, loss = 0.10392284\n",
            "Iteration 24, loss = 0.10087283\n",
            "Iteration 25, loss = 0.09878718\n",
            "Iteration 26, loss = 0.09644134\n",
            "Iteration 27, loss = 0.09424922\n",
            "Iteration 28, loss = 0.09228929\n",
            "Iteration 29, loss = 0.09036409\n",
            "Iteration 30, loss = 0.08845926\n",
            "Iteration 31, loss = 0.08634631\n",
            "Iteration 32, loss = 0.08484547\n",
            "Iteration 33, loss = 0.08320707\n",
            "Iteration 34, loss = 0.08151910\n",
            "Iteration 35, loss = 0.08019987\n",
            "Iteration 36, loss = 0.07871872\n",
            "Iteration 37, loss = 0.07731406\n",
            "Iteration 38, loss = 0.07613978\n",
            "Iteration 39, loss = 0.07476187\n",
            "Iteration 40, loss = 0.07336481\n",
            "Iteration 41, loss = 0.07229145\n",
            "Iteration 42, loss = 0.07115826\n",
            "Iteration 43, loss = 0.07022824\n",
            "Iteration 44, loss = 0.06900613\n",
            "Iteration 45, loss = 0.06798054\n",
            "Iteration 46, loss = 0.06698123\n",
            "Iteration 47, loss = 0.06609269\n",
            "Iteration 48, loss = 0.06515466\n",
            "Iteration 49, loss = 0.06403502\n",
            "Iteration 50, loss = 0.06319980\n",
            "\n",
            "For batches = 10:\n",
            "============================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 0.77659209\n",
            "Iteration 2, loss = 0.35169476\n",
            "Iteration 3, loss = 0.29370323\n",
            "Iteration 4, loss = 0.26160018\n",
            "Iteration 5, loss = 0.23835006\n",
            "Iteration 6, loss = 0.22020206\n",
            "Iteration 7, loss = 0.20524977\n",
            "Iteration 8, loss = 0.19250908\n",
            "Iteration 9, loss = 0.18166501\n",
            "Iteration 10, loss = 0.17223005\n",
            "Iteration 11, loss = 0.16398489\n",
            "Iteration 12, loss = 0.15674479\n",
            "Iteration 13, loss = 0.15032197\n",
            "Iteration 14, loss = 0.14438893\n",
            "Iteration 15, loss = 0.13925911\n",
            "Iteration 16, loss = 0.13410892\n",
            "Iteration 17, loss = 0.12973445\n",
            "Iteration 18, loss = 0.12544996\n",
            "Iteration 19, loss = 0.12168089\n",
            "Iteration 20, loss = 0.11826818\n",
            "Iteration 21, loss = 0.11486403\n",
            "Iteration 22, loss = 0.11185901\n",
            "Iteration 23, loss = 0.10913528\n",
            "Iteration 24, loss = 0.10600094\n",
            "Iteration 25, loss = 0.10378532\n",
            "Iteration 26, loss = 0.10133948\n",
            "Iteration 27, loss = 0.09901220\n",
            "Iteration 28, loss = 0.09697084\n",
            "Iteration 29, loss = 0.09496202\n",
            "Iteration 30, loss = 0.09295482\n",
            "Iteration 31, loss = 0.09074511\n",
            "Iteration 32, loss = 0.08912391\n",
            "Iteration 33, loss = 0.08738903\n",
            "Iteration 34, loss = 0.08565472\n",
            "Iteration 35, loss = 0.08420766\n",
            "Iteration 36, loss = 0.08272366\n",
            "Iteration 37, loss = 0.08120384\n",
            "Iteration 38, loss = 0.07996620\n",
            "Iteration 39, loss = 0.07851583\n",
            "Iteration 40, loss = 0.07702592\n",
            "Iteration 41, loss = 0.07591513\n",
            "Iteration 42, loss = 0.07467269\n",
            "Iteration 43, loss = 0.07376388\n",
            "Iteration 44, loss = 0.07244570\n",
            "Iteration 45, loss = 0.07137564\n",
            "Iteration 46, loss = 0.07032046\n",
            "Iteration 47, loss = 0.06939159\n",
            "Iteration 48, loss = 0.06840130\n",
            "Iteration 49, loss = 0.06723987\n",
            "Iteration 50, loss = 0.06638381\n",
            "\n",
            "============================\n",
            "time taken to run: 7290.07729013\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        }
      ],
      "source": [
        "# Question 0-0 (answer) here:\n",
        "# Using 1 - 10 batches and 50 iterations does not converge\n",
        "import time\n",
        "t1 = time.perf_counter()\n",
        "iterations = 50\n",
        "\n",
        "for batches in list(range(1,11)):\n",
        "    print('For batches =', str(batches) + ':')\n",
        "    print('============================')\n",
        "    mlp_mnist = MLPClassifier(verbose = True, hidden_layer_sizes=(50,), batch_size = batches, \\\n",
        "                                max_iter=iterations, solver='sgd', activation='logistic',\\\n",
        "                                learning_rate = 'constant', learning_rate_init = 0.001, random_state=1)\n",
        "    mlp_mnist.fit(X_train,y_train)\n",
        "    print('')\n",
        "\n",
        "print('============================')\n",
        "t2 = time.perf_counter()\n",
        "print('time taken to run:',t2-t1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "MW1lZJEF7UCo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c96928c0-7a4d-4254-a1ed-f7919fee3149"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For batches = 10:\n",
            "============================\n",
            "Iteration 1, loss = 0.77659209\n",
            "Iteration 2, loss = 0.35169476\n",
            "Iteration 3, loss = 0.29370323\n",
            "Iteration 4, loss = 0.26160018\n",
            "Iteration 5, loss = 0.23835006\n",
            "Iteration 6, loss = 0.22020206\n",
            "Iteration 7, loss = 0.20524977\n",
            "Iteration 8, loss = 0.19250908\n",
            "Iteration 9, loss = 0.18166501\n",
            "Iteration 10, loss = 0.17223005\n",
            "Iteration 11, loss = 0.16398489\n",
            "Iteration 12, loss = 0.15674479\n",
            "Iteration 13, loss = 0.15032197\n",
            "Iteration 14, loss = 0.14438893\n",
            "Iteration 15, loss = 0.13925911\n",
            "Iteration 16, loss = 0.13410892\n",
            "Iteration 17, loss = 0.12973445\n",
            "Iteration 18, loss = 0.12544996\n",
            "Iteration 19, loss = 0.12168089\n",
            "Iteration 20, loss = 0.11826818\n",
            "Iteration 21, loss = 0.11486403\n",
            "Iteration 22, loss = 0.11185901\n",
            "Iteration 23, loss = 0.10913528\n",
            "Iteration 24, loss = 0.10600094\n",
            "Iteration 25, loss = 0.10378532\n",
            "Iteration 26, loss = 0.10133948\n",
            "Iteration 27, loss = 0.09901220\n",
            "Iteration 28, loss = 0.09697084\n",
            "Iteration 29, loss = 0.09496202\n",
            "Iteration 30, loss = 0.09295482\n",
            "Iteration 31, loss = 0.09074511\n",
            "Iteration 32, loss = 0.08912391\n",
            "Iteration 33, loss = 0.08738903\n",
            "Iteration 34, loss = 0.08565472\n",
            "Iteration 35, loss = 0.08420766\n",
            "Iteration 36, loss = 0.08272366\n",
            "Iteration 37, loss = 0.08120384\n",
            "Iteration 38, loss = 0.07996620\n",
            "Iteration 39, loss = 0.07851583\n",
            "Iteration 40, loss = 0.07702592\n",
            "Iteration 41, loss = 0.07591513\n",
            "Iteration 42, loss = 0.07467269\n",
            "Iteration 43, loss = 0.07376388\n",
            "Iteration 44, loss = 0.07244570\n",
            "Iteration 45, loss = 0.07137564\n",
            "Iteration 46, loss = 0.07032046\n",
            "Iteration 47, loss = 0.06939159\n",
            "Iteration 48, loss = 0.06840130\n",
            "Iteration 49, loss = 0.06723987\n",
            "Iteration 50, loss = 0.06638381\n",
            "Iteration 51, loss = 0.06555810\n",
            "Iteration 52, loss = 0.06460751\n",
            "Iteration 53, loss = 0.06360214\n",
            "Iteration 54, loss = 0.06291040\n",
            "Iteration 55, loss = 0.06216025\n",
            "Iteration 56, loss = 0.06152016\n",
            "Iteration 57, loss = 0.06061969\n",
            "Iteration 58, loss = 0.05980390\n",
            "Iteration 59, loss = 0.05905587\n",
            "Iteration 60, loss = 0.05845287\n",
            "Iteration 61, loss = 0.05767261\n",
            "Iteration 62, loss = 0.05686698\n",
            "Iteration 63, loss = 0.05637241\n",
            "Iteration 64, loss = 0.05583489\n",
            "Iteration 65, loss = 0.05512632\n",
            "Iteration 66, loss = 0.05443720\n",
            "Iteration 67, loss = 0.05388250\n",
            "Iteration 68, loss = 0.05327557\n",
            "Iteration 69, loss = 0.05277201\n",
            "Iteration 70, loss = 0.05213810\n",
            "Iteration 71, loss = 0.05150209\n",
            "Iteration 72, loss = 0.05100463\n",
            "Iteration 73, loss = 0.05051139\n",
            "Iteration 74, loss = 0.04998574\n",
            "Iteration 75, loss = 0.04954391\n",
            "Iteration 76, loss = 0.04879035\n",
            "Iteration 77, loss = 0.04855296\n",
            "Iteration 78, loss = 0.04804704\n",
            "Iteration 79, loss = 0.04744306\n",
            "Iteration 80, loss = 0.04739455\n",
            "Iteration 81, loss = 0.04670309\n",
            "Iteration 82, loss = 0.04610933\n",
            "Iteration 83, loss = 0.04579732\n",
            "Iteration 84, loss = 0.04527204\n",
            "Iteration 85, loss = 0.04505492\n",
            "Iteration 86, loss = 0.04451223\n",
            "Iteration 87, loss = 0.04400939\n",
            "Iteration 88, loss = 0.04364029\n",
            "Iteration 89, loss = 0.04345397\n",
            "Iteration 90, loss = 0.04289662\n",
            "Iteration 91, loss = 0.04255368\n",
            "Iteration 92, loss = 0.04221875\n",
            "Iteration 93, loss = 0.04181239\n",
            "Iteration 94, loss = 0.04146468\n",
            "Iteration 95, loss = 0.04119931\n",
            "Iteration 96, loss = 0.04077622\n",
            "Iteration 97, loss = 0.04047715\n",
            "Iteration 98, loss = 0.04015814\n",
            "Iteration 99, loss = 0.03981521\n",
            "Iteration 100, loss = 0.03955621\n",
            "Iteration 101, loss = 0.03915517\n",
            "Iteration 102, loss = 0.03904632\n",
            "Iteration 103, loss = 0.03854489\n",
            "Iteration 104, loss = 0.03834123\n",
            "Iteration 105, loss = 0.03802273\n",
            "Iteration 106, loss = 0.03763450\n",
            "Iteration 107, loss = 0.03738968\n",
            "Iteration 108, loss = 0.03698652\n",
            "Iteration 109, loss = 0.03682003\n",
            "Iteration 110, loss = 0.03661497\n",
            "Iteration 111, loss = 0.03632661\n",
            "Iteration 112, loss = 0.03613755\n",
            "Iteration 113, loss = 0.03595112\n",
            "Iteration 114, loss = 0.03556792\n",
            "Iteration 115, loss = 0.03524888\n",
            "Iteration 116, loss = 0.03508341\n",
            "Iteration 117, loss = 0.03485294\n",
            "Iteration 118, loss = 0.03456703\n",
            "Iteration 119, loss = 0.03433396\n",
            "Iteration 120, loss = 0.03415643\n",
            "Iteration 121, loss = 0.03392089\n",
            "Iteration 122, loss = 0.03369334\n",
            "Iteration 123, loss = 0.03359918\n",
            "Iteration 124, loss = 0.03327821\n",
            "Iteration 125, loss = 0.03306742\n",
            "Iteration 126, loss = 0.03285904\n",
            "Iteration 127, loss = 0.03261904\n",
            "Iteration 128, loss = 0.03239006\n",
            "Iteration 129, loss = 0.03220186\n",
            "Iteration 130, loss = 0.03190238\n",
            "Iteration 131, loss = 0.03174641\n",
            "Iteration 132, loss = 0.03177540\n",
            "Iteration 133, loss = 0.03148466\n",
            "Iteration 134, loss = 0.03126944\n",
            "Iteration 135, loss = 0.03109514\n",
            "Iteration 136, loss = 0.03089911\n",
            "Iteration 137, loss = 0.03077134\n",
            "Iteration 138, loss = 0.03074872\n",
            "Iteration 139, loss = 0.03041143\n",
            "Iteration 140, loss = 0.03029596\n",
            "Iteration 141, loss = 0.03014552\n",
            "Iteration 142, loss = 0.02993112\n",
            "Iteration 143, loss = 0.02974024\n",
            "Iteration 144, loss = 0.02957005\n",
            "Iteration 145, loss = 0.02950086\n",
            "Iteration 146, loss = 0.02925733\n",
            "Iteration 147, loss = 0.02917756\n",
            "Iteration 148, loss = 0.02902424\n",
            "Iteration 149, loss = 0.02887811\n",
            "Iteration 150, loss = 0.02867300\n",
            "\n",
            "For batches = 50:\n",
            "============================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.57303042\n",
            "Iteration 2, loss = 0.80506969\n",
            "Iteration 3, loss = 0.57329567\n",
            "Iteration 4, loss = 0.47348702\n",
            "Iteration 5, loss = 0.41854137\n",
            "Iteration 6, loss = 0.38374330\n",
            "Iteration 7, loss = 0.35914457\n",
            "Iteration 8, loss = 0.34046999\n",
            "Iteration 9, loss = 0.32577086\n",
            "Iteration 10, loss = 0.31348999\n",
            "Iteration 11, loss = 0.30299833\n",
            "Iteration 12, loss = 0.29379657\n",
            "Iteration 13, loss = 0.28571295\n",
            "Iteration 14, loss = 0.27833818\n",
            "Iteration 15, loss = 0.27157451\n",
            "Iteration 16, loss = 0.26537749\n",
            "Iteration 17, loss = 0.25949766\n",
            "Iteration 18, loss = 0.25394727\n",
            "Iteration 19, loss = 0.24899406\n",
            "Iteration 20, loss = 0.24414146\n",
            "Iteration 21, loss = 0.23959783\n",
            "Iteration 22, loss = 0.23518094\n",
            "Iteration 23, loss = 0.23110986\n",
            "Iteration 24, loss = 0.22718884\n",
            "Iteration 25, loss = 0.22342351\n",
            "Iteration 26, loss = 0.21981596\n",
            "Iteration 27, loss = 0.21629698\n",
            "Iteration 28, loss = 0.21294556\n",
            "Iteration 29, loss = 0.20980084\n",
            "Iteration 30, loss = 0.20658268\n",
            "Iteration 31, loss = 0.20362258\n",
            "Iteration 32, loss = 0.20060514\n",
            "Iteration 33, loss = 0.19790947\n",
            "Iteration 34, loss = 0.19516267\n",
            "Iteration 35, loss = 0.19261536\n",
            "Iteration 36, loss = 0.19009477\n",
            "Iteration 37, loss = 0.18753109\n",
            "Iteration 38, loss = 0.18531424\n",
            "Iteration 39, loss = 0.18283960\n",
            "Iteration 40, loss = 0.18053713\n",
            "Iteration 41, loss = 0.17843156\n",
            "Iteration 42, loss = 0.17634545\n",
            "Iteration 43, loss = 0.17425051\n",
            "Iteration 44, loss = 0.17216933\n",
            "Iteration 45, loss = 0.17036949\n",
            "Iteration 46, loss = 0.16835334\n",
            "Iteration 47, loss = 0.16648132\n",
            "Iteration 48, loss = 0.16466195\n",
            "Iteration 49, loss = 0.16285893\n",
            "Iteration 50, loss = 0.16114983\n",
            "Iteration 51, loss = 0.15950507\n",
            "Iteration 52, loss = 0.15785728\n",
            "Iteration 53, loss = 0.15631748\n",
            "Iteration 54, loss = 0.15478627\n",
            "Iteration 55, loss = 0.15322691\n",
            "Iteration 56, loss = 0.15177209\n",
            "Iteration 57, loss = 0.15025142\n",
            "Iteration 58, loss = 0.14884824\n",
            "Iteration 59, loss = 0.14754695\n",
            "Iteration 60, loss = 0.14604005\n",
            "Iteration 61, loss = 0.14484781\n",
            "Iteration 62, loss = 0.14350766\n",
            "Iteration 63, loss = 0.14227884\n",
            "Iteration 64, loss = 0.14094612\n",
            "Iteration 65, loss = 0.13976765\n",
            "Iteration 66, loss = 0.13852669\n",
            "Iteration 67, loss = 0.13744664\n",
            "Iteration 68, loss = 0.13622061\n",
            "Iteration 69, loss = 0.13515677\n",
            "Iteration 70, loss = 0.13396542\n",
            "Iteration 71, loss = 0.13284287\n",
            "Iteration 72, loss = 0.13181835\n",
            "Iteration 73, loss = 0.13079554\n",
            "Iteration 74, loss = 0.12977735\n",
            "Iteration 75, loss = 0.12869959\n",
            "Iteration 76, loss = 0.12775710\n",
            "Iteration 77, loss = 0.12679645\n",
            "Iteration 78, loss = 0.12571359\n",
            "Iteration 79, loss = 0.12486425\n",
            "Iteration 80, loss = 0.12405573\n",
            "Iteration 81, loss = 0.12302729\n",
            "Iteration 82, loss = 0.12221776\n",
            "Iteration 83, loss = 0.12134884\n",
            "Iteration 84, loss = 0.12043885\n",
            "Iteration 85, loss = 0.11959293\n",
            "Iteration 86, loss = 0.11878492\n",
            "Iteration 87, loss = 0.11790380\n",
            "Iteration 88, loss = 0.11721624\n",
            "Iteration 89, loss = 0.11635429\n",
            "Iteration 90, loss = 0.11559577\n",
            "Iteration 91, loss = 0.11488681\n",
            "Iteration 92, loss = 0.11407016\n",
            "Iteration 93, loss = 0.11320013\n",
            "Iteration 94, loss = 0.11254752\n",
            "Iteration 95, loss = 0.11188673\n",
            "Iteration 96, loss = 0.11109161\n",
            "Iteration 97, loss = 0.11041572\n",
            "Iteration 98, loss = 0.10974204\n",
            "Iteration 99, loss = 0.10902281\n",
            "Iteration 100, loss = 0.10838486\n",
            "Iteration 101, loss = 0.10765136\n",
            "Iteration 102, loss = 0.10700543\n",
            "Iteration 103, loss = 0.10633182\n",
            "Iteration 104, loss = 0.10571047\n",
            "Iteration 105, loss = 0.10507528\n",
            "Iteration 106, loss = 0.10451802\n",
            "Iteration 107, loss = 0.10386434\n",
            "Iteration 108, loss = 0.10330628\n",
            "Iteration 109, loss = 0.10267993\n",
            "Iteration 110, loss = 0.10209475\n",
            "Iteration 111, loss = 0.10150193\n",
            "Iteration 112, loss = 0.10081759\n",
            "Iteration 113, loss = 0.10033345\n",
            "Iteration 114, loss = 0.09978522\n",
            "Iteration 115, loss = 0.09922035\n",
            "Iteration 116, loss = 0.09858122\n",
            "Iteration 117, loss = 0.09810911\n",
            "Iteration 118, loss = 0.09758092\n",
            "Iteration 119, loss = 0.09700136\n",
            "Iteration 120, loss = 0.09651121\n",
            "Iteration 121, loss = 0.09597708\n",
            "Iteration 122, loss = 0.09548715\n",
            "Iteration 123, loss = 0.09485757\n",
            "Iteration 124, loss = 0.09449986\n",
            "Iteration 125, loss = 0.09402754\n",
            "Iteration 126, loss = 0.09340272\n",
            "Iteration 127, loss = 0.09302260\n",
            "Iteration 128, loss = 0.09246727\n",
            "Iteration 129, loss = 0.09200477\n",
            "Iteration 130, loss = 0.09158477\n",
            "Iteration 131, loss = 0.09109633\n",
            "Iteration 132, loss = 0.09073141\n",
            "Iteration 133, loss = 0.09019890\n",
            "Iteration 134, loss = 0.08973895\n",
            "Iteration 135, loss = 0.08933460\n",
            "Iteration 136, loss = 0.08892762\n",
            "Iteration 137, loss = 0.08841261\n",
            "Iteration 138, loss = 0.08803249\n",
            "Iteration 139, loss = 0.08758494\n",
            "Iteration 140, loss = 0.08712761\n",
            "Iteration 141, loss = 0.08674111\n",
            "Iteration 142, loss = 0.08627499\n",
            "Iteration 143, loss = 0.08588055\n",
            "Iteration 144, loss = 0.08552983\n",
            "Iteration 145, loss = 0.08508575\n",
            "Iteration 146, loss = 0.08482797\n",
            "Iteration 147, loss = 0.08438812\n",
            "Iteration 148, loss = 0.08388307\n",
            "Iteration 149, loss = 0.08363193\n",
            "Iteration 150, loss = 0.08324888\n",
            "\n",
            "For batches = 100:\n",
            "============================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.90171918\n",
            "Iteration 2, loss = 1.24288859\n",
            "Iteration 3, loss = 0.89519912\n",
            "Iteration 4, loss = 0.71369258\n",
            "Iteration 5, loss = 0.60706049\n",
            "Iteration 6, loss = 0.53806866\n",
            "Iteration 7, loss = 0.49023213\n",
            "Iteration 8, loss = 0.45507184\n",
            "Iteration 9, loss = 0.42833894\n",
            "Iteration 10, loss = 0.40721514\n",
            "Iteration 11, loss = 0.39005000\n",
            "Iteration 12, loss = 0.37557416\n",
            "Iteration 13, loss = 0.36349458\n",
            "Iteration 14, loss = 0.35299721\n",
            "Iteration 15, loss = 0.34371970\n",
            "Iteration 16, loss = 0.33561821\n",
            "Iteration 17, loss = 0.32816447\n",
            "Iteration 18, loss = 0.32147245\n",
            "Iteration 19, loss = 0.31542737\n",
            "Iteration 20, loss = 0.30980405\n",
            "Iteration 21, loss = 0.30450158\n",
            "Iteration 22, loss = 0.29970410\n",
            "Iteration 23, loss = 0.29513071\n",
            "Iteration 24, loss = 0.29090426\n",
            "Iteration 25, loss = 0.28671756\n",
            "Iteration 26, loss = 0.28293855\n",
            "Iteration 27, loss = 0.27916022\n",
            "Iteration 28, loss = 0.27571620\n",
            "Iteration 29, loss = 0.27231345\n",
            "Iteration 30, loss = 0.26904947\n",
            "Iteration 31, loss = 0.26594336\n",
            "Iteration 32, loss = 0.26282144\n",
            "Iteration 33, loss = 0.25997759\n",
            "Iteration 34, loss = 0.25712760\n",
            "Iteration 35, loss = 0.25457793\n",
            "Iteration 36, loss = 0.25194586\n",
            "Iteration 37, loss = 0.24928969\n",
            "Iteration 38, loss = 0.24688708\n",
            "Iteration 39, loss = 0.24437434\n",
            "Iteration 40, loss = 0.24211030\n",
            "Iteration 41, loss = 0.23980251\n",
            "Iteration 42, loss = 0.23763808\n",
            "Iteration 43, loss = 0.23536895\n",
            "Iteration 44, loss = 0.23330288\n",
            "Iteration 45, loss = 0.23132240\n",
            "Iteration 46, loss = 0.22924426\n",
            "Iteration 47, loss = 0.22725472\n",
            "Iteration 48, loss = 0.22522418\n",
            "Iteration 49, loss = 0.22338248\n",
            "Iteration 50, loss = 0.22149461\n",
            "Iteration 51, loss = 0.21979607\n",
            "Iteration 52, loss = 0.21796950\n",
            "Iteration 53, loss = 0.21620303\n",
            "Iteration 54, loss = 0.21454019\n",
            "Iteration 55, loss = 0.21278369\n",
            "Iteration 56, loss = 0.21119253\n",
            "Iteration 57, loss = 0.20953187\n",
            "Iteration 58, loss = 0.20797997\n",
            "Iteration 59, loss = 0.20649241\n",
            "Iteration 60, loss = 0.20479971\n",
            "Iteration 61, loss = 0.20338915\n",
            "Iteration 62, loss = 0.20192368\n",
            "Iteration 63, loss = 0.20053876\n",
            "Iteration 64, loss = 0.19894925\n",
            "Iteration 65, loss = 0.19758612\n",
            "Iteration 66, loss = 0.19624643\n",
            "Iteration 67, loss = 0.19492737\n",
            "Iteration 68, loss = 0.19363509\n",
            "Iteration 69, loss = 0.19231442\n",
            "Iteration 70, loss = 0.19094556\n",
            "Iteration 71, loss = 0.18965048\n",
            "Iteration 72, loss = 0.18842637\n",
            "Iteration 73, loss = 0.18718504\n",
            "Iteration 74, loss = 0.18604099\n",
            "Iteration 75, loss = 0.18475808\n",
            "Iteration 76, loss = 0.18360946\n",
            "Iteration 77, loss = 0.18242719\n",
            "Iteration 78, loss = 0.18123367\n",
            "Iteration 79, loss = 0.18014229\n",
            "Iteration 80, loss = 0.17914794\n",
            "Iteration 81, loss = 0.17792768\n",
            "Iteration 82, loss = 0.17691197\n",
            "Iteration 83, loss = 0.17581937\n",
            "Iteration 84, loss = 0.17474834\n",
            "Iteration 85, loss = 0.17376284\n",
            "Iteration 86, loss = 0.17266376\n",
            "Iteration 87, loss = 0.17170339\n",
            "Iteration 88, loss = 0.17073842\n",
            "Iteration 89, loss = 0.16966772\n",
            "Iteration 90, loss = 0.16877402\n",
            "Iteration 91, loss = 0.16785833\n",
            "Iteration 92, loss = 0.16686776\n",
            "Iteration 93, loss = 0.16586725\n",
            "Iteration 94, loss = 0.16510979\n",
            "Iteration 95, loss = 0.16413177\n",
            "Iteration 96, loss = 0.16325156\n",
            "Iteration 97, loss = 0.16238685\n",
            "Iteration 98, loss = 0.16149205\n",
            "Iteration 99, loss = 0.16064223\n",
            "Iteration 100, loss = 0.15981951\n",
            "Iteration 101, loss = 0.15892688\n",
            "Iteration 102, loss = 0.15809097\n",
            "Iteration 103, loss = 0.15721478\n",
            "Iteration 104, loss = 0.15649934\n",
            "Iteration 105, loss = 0.15566211\n",
            "Iteration 106, loss = 0.15495870\n",
            "Iteration 107, loss = 0.15415995\n",
            "Iteration 108, loss = 0.15338820\n",
            "Iteration 109, loss = 0.15255815\n",
            "Iteration 110, loss = 0.15187007\n",
            "Iteration 111, loss = 0.15113418\n",
            "Iteration 112, loss = 0.15028760\n",
            "Iteration 113, loss = 0.14963934\n",
            "Iteration 114, loss = 0.14894294\n",
            "Iteration 115, loss = 0.14820003\n",
            "Iteration 116, loss = 0.14745676\n",
            "Iteration 117, loss = 0.14674231\n",
            "Iteration 118, loss = 0.14618837\n",
            "Iteration 119, loss = 0.14542168\n",
            "Iteration 120, loss = 0.14476274\n",
            "Iteration 121, loss = 0.14412687\n",
            "Iteration 122, loss = 0.14346200\n",
            "Iteration 123, loss = 0.14271551\n",
            "Iteration 124, loss = 0.14219885\n",
            "Iteration 125, loss = 0.14153890\n",
            "Iteration 126, loss = 0.14087081\n",
            "Iteration 127, loss = 0.14030906\n",
            "Iteration 128, loss = 0.13964637\n",
            "Iteration 129, loss = 0.13897631\n",
            "Iteration 130, loss = 0.13841274\n",
            "Iteration 131, loss = 0.13783206\n",
            "Iteration 132, loss = 0.13724688\n",
            "Iteration 133, loss = 0.13665197\n",
            "Iteration 134, loss = 0.13608607\n",
            "Iteration 135, loss = 0.13544662\n",
            "Iteration 136, loss = 0.13493229\n",
            "Iteration 137, loss = 0.13433109\n",
            "Iteration 138, loss = 0.13380839\n",
            "Iteration 139, loss = 0.13323424\n",
            "Iteration 140, loss = 0.13264195\n",
            "Iteration 141, loss = 0.13214517\n",
            "Iteration 142, loss = 0.13164215\n",
            "Iteration 143, loss = 0.13103024\n",
            "Iteration 144, loss = 0.13059980\n",
            "Iteration 145, loss = 0.13005646\n",
            "Iteration 146, loss = 0.12956671\n",
            "Iteration 147, loss = 0.12903659\n",
            "Iteration 148, loss = 0.12845368\n",
            "Iteration 149, loss = 0.12802191\n",
            "Iteration 150, loss = 0.12753910\n",
            "\n",
            "For batches = 250:\n",
            "============================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 2.14814826\n",
            "Iteration 2, loss = 1.81371665\n",
            "Iteration 3, loss = 1.51629286\n",
            "Iteration 4, loss = 1.27990888\n",
            "Iteration 5, loss = 1.10195378\n",
            "Iteration 6, loss = 0.96822355\n",
            "Iteration 7, loss = 0.86641806\n",
            "Iteration 8, loss = 0.78724122\n",
            "Iteration 9, loss = 0.72440567\n",
            "Iteration 10, loss = 0.67361578\n",
            "Iteration 11, loss = 0.63193455\n",
            "Iteration 12, loss = 0.59706272\n",
            "Iteration 13, loss = 0.56767096\n",
            "Iteration 14, loss = 0.54255277\n",
            "Iteration 15, loss = 0.52075172\n",
            "Iteration 16, loss = 0.50195438\n",
            "Iteration 17, loss = 0.48519649\n",
            "Iteration 18, loss = 0.47043398\n",
            "Iteration 19, loss = 0.45733145\n",
            "Iteration 20, loss = 0.44553905\n",
            "Iteration 21, loss = 0.43487880\n",
            "Iteration 22, loss = 0.42525566\n",
            "Iteration 23, loss = 0.41647474\n",
            "Iteration 24, loss = 0.40846958\n",
            "Iteration 25, loss = 0.40089909\n",
            "Iteration 26, loss = 0.39414335\n",
            "Iteration 27, loss = 0.38775200\n",
            "Iteration 28, loss = 0.38190544\n",
            "Iteration 29, loss = 0.37632792\n",
            "Iteration 30, loss = 0.37119842\n",
            "Iteration 31, loss = 0.36632995\n",
            "Iteration 32, loss = 0.36172626\n",
            "Iteration 33, loss = 0.35735042\n",
            "Iteration 34, loss = 0.35329580\n",
            "Iteration 35, loss = 0.34950958\n",
            "Iteration 36, loss = 0.34581929\n",
            "Iteration 37, loss = 0.34230467\n",
            "Iteration 38, loss = 0.33892829\n",
            "Iteration 39, loss = 0.33566541\n",
            "Iteration 40, loss = 0.33273701\n",
            "Iteration 41, loss = 0.32975648\n",
            "Iteration 42, loss = 0.32693799\n",
            "Iteration 43, loss = 0.32412226\n",
            "Iteration 44, loss = 0.32157489\n",
            "Iteration 45, loss = 0.31905089\n",
            "Iteration 46, loss = 0.31664464\n",
            "Iteration 47, loss = 0.31427589\n",
            "Iteration 48, loss = 0.31191211\n",
            "Iteration 49, loss = 0.30976672\n",
            "Iteration 50, loss = 0.30755276\n",
            "Iteration 51, loss = 0.30555277\n",
            "Iteration 52, loss = 0.30351944\n",
            "Iteration 53, loss = 0.30146337\n",
            "Iteration 54, loss = 0.29957883\n",
            "Iteration 55, loss = 0.29768641\n",
            "Iteration 56, loss = 0.29587743\n",
            "Iteration 57, loss = 0.29405588\n",
            "Iteration 58, loss = 0.29233604\n",
            "Iteration 59, loss = 0.29066831\n",
            "Iteration 60, loss = 0.28895166\n",
            "Iteration 61, loss = 0.28735573\n",
            "Iteration 62, loss = 0.28575182\n",
            "Iteration 63, loss = 0.28423006\n",
            "Iteration 64, loss = 0.28261587\n",
            "Iteration 65, loss = 0.28110938\n",
            "Iteration 66, loss = 0.27969904\n",
            "Iteration 67, loss = 0.27819913\n",
            "Iteration 68, loss = 0.27683858\n",
            "Iteration 69, loss = 0.27543652\n",
            "Iteration 70, loss = 0.27405739\n",
            "Iteration 71, loss = 0.27267425\n",
            "Iteration 72, loss = 0.27137460\n",
            "Iteration 73, loss = 0.27006502\n",
            "Iteration 74, loss = 0.26878799\n",
            "Iteration 75, loss = 0.26752285\n",
            "Iteration 76, loss = 0.26627118\n",
            "Iteration 77, loss = 0.26501784\n",
            "Iteration 78, loss = 0.26379055\n",
            "Iteration 79, loss = 0.26256901\n",
            "Iteration 80, loss = 0.26147388\n",
            "Iteration 81, loss = 0.26027703\n",
            "Iteration 82, loss = 0.25914492\n",
            "Iteration 83, loss = 0.25802717\n",
            "Iteration 84, loss = 0.25687589\n",
            "Iteration 85, loss = 0.25585132\n",
            "Iteration 86, loss = 0.25467678\n",
            "Iteration 87, loss = 0.25367390\n",
            "Iteration 88, loss = 0.25262345\n",
            "Iteration 89, loss = 0.25149578\n",
            "Iteration 90, loss = 0.25049301\n",
            "Iteration 91, loss = 0.24947823\n",
            "Iteration 92, loss = 0.24845895\n",
            "Iteration 93, loss = 0.24747529\n",
            "Iteration 94, loss = 0.24659533\n",
            "Iteration 95, loss = 0.24549455\n",
            "Iteration 96, loss = 0.24463420\n",
            "Iteration 97, loss = 0.24361592\n",
            "Iteration 98, loss = 0.24261981\n",
            "Iteration 99, loss = 0.24171510\n",
            "Iteration 100, loss = 0.24083089\n",
            "Iteration 101, loss = 0.23985567\n",
            "Iteration 102, loss = 0.23902551\n",
            "Iteration 103, loss = 0.23803159\n",
            "Iteration 104, loss = 0.23721397\n",
            "Iteration 105, loss = 0.23629469\n",
            "Iteration 106, loss = 0.23544395\n",
            "Iteration 107, loss = 0.23464587\n",
            "Iteration 108, loss = 0.23372813\n",
            "Iteration 109, loss = 0.23288297\n",
            "Iteration 110, loss = 0.23207918\n",
            "Iteration 111, loss = 0.23123030\n",
            "Iteration 112, loss = 0.23038879\n",
            "Iteration 113, loss = 0.22963240\n",
            "Iteration 114, loss = 0.22877773\n",
            "Iteration 115, loss = 0.22799365\n",
            "Iteration 116, loss = 0.22723659\n",
            "Iteration 117, loss = 0.22638440\n",
            "Iteration 118, loss = 0.22569256\n",
            "Iteration 119, loss = 0.22485619\n",
            "Iteration 120, loss = 0.22415085\n",
            "Iteration 121, loss = 0.22336518\n",
            "Iteration 122, loss = 0.22262709\n",
            "Iteration 123, loss = 0.22186006\n",
            "Iteration 124, loss = 0.22114691\n",
            "Iteration 125, loss = 0.22038462\n",
            "Iteration 126, loss = 0.21972985\n",
            "Iteration 127, loss = 0.21900185\n",
            "Iteration 128, loss = 0.21826607\n",
            "Iteration 129, loss = 0.21755253\n",
            "Iteration 130, loss = 0.21684945\n",
            "Iteration 131, loss = 0.21614210\n",
            "Iteration 132, loss = 0.21543614\n",
            "Iteration 133, loss = 0.21477333\n",
            "Iteration 134, loss = 0.21409093\n",
            "Iteration 135, loss = 0.21330530\n",
            "Iteration 136, loss = 0.21271844\n",
            "Iteration 137, loss = 0.21205096\n",
            "Iteration 138, loss = 0.21142911\n",
            "Iteration 139, loss = 0.21072386\n",
            "Iteration 140, loss = 0.21009754\n",
            "Iteration 141, loss = 0.20941936\n",
            "Iteration 142, loss = 0.20886098\n",
            "Iteration 143, loss = 0.20815379\n",
            "Iteration 144, loss = 0.20758033\n",
            "Iteration 145, loss = 0.20692593\n",
            "Iteration 146, loss = 0.20627386\n",
            "Iteration 147, loss = 0.20572440\n",
            "Iteration 148, loss = 0.20507782\n",
            "Iteration 149, loss = 0.20447193\n",
            "Iteration 150, loss = 0.20384270\n",
            "\n",
            "============================\n",
            "time taken to run: 1599.1607491260002\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        }
      ],
      "source": [
        "# Question 0-0 (cont'd) here:\n",
        "# Attempting multiple batches using 150 iterations\n",
        "import time\n",
        "t1 = time.perf_counter()\n",
        "iterations = 150\n",
        "\n",
        "for batches in [10,50,100,250]:\n",
        "    print('For batches =', str(batches) + ':')\n",
        "    print('============================')\n",
        "    mlp_mnist = MLPClassifier(verbose = True, hidden_layer_sizes=(50,), batch_size = batches, \\\n",
        "                                max_iter=iterations, solver='sgd', activation='logistic',\\\n",
        "                                learning_rate = 'constant', learning_rate_init = 0.001, random_state=1)\n",
        "    mlp_mnist.fit(X_train,y_train)\n",
        "    print('')\n",
        "\n",
        "print('============================')\n",
        "t2 = time.perf_counter()\n",
        "print('time taken to run:',t2-t1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "2XvplDwY7dQG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77777b90-8367-4ce8-b525-d6f6465cd6ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 0.77659209\n",
            "Iteration 2, loss = 0.35169476\n",
            "Iteration 3, loss = 0.29370323\n",
            "Iteration 4, loss = 0.26160018\n",
            "Iteration 5, loss = 0.23835006\n",
            "Iteration 6, loss = 0.22020206\n",
            "Iteration 7, loss = 0.20524977\n",
            "Iteration 8, loss = 0.19250908\n",
            "Iteration 9, loss = 0.18166501\n",
            "Iteration 10, loss = 0.17223005\n",
            "Iteration 11, loss = 0.16398489\n",
            "Iteration 12, loss = 0.15674479\n",
            "Iteration 13, loss = 0.15032197\n",
            "Iteration 14, loss = 0.14438893\n",
            "Iteration 15, loss = 0.13925911\n",
            "Iteration 16, loss = 0.13410892\n",
            "Iteration 17, loss = 0.12973445\n",
            "Iteration 18, loss = 0.12544996\n",
            "Iteration 19, loss = 0.12168089\n",
            "Iteration 20, loss = 0.11826818\n",
            "Iteration 21, loss = 0.11486403\n",
            "Iteration 22, loss = 0.11185901\n",
            "Iteration 23, loss = 0.10913528\n",
            "Iteration 24, loss = 0.10600094\n",
            "Iteration 25, loss = 0.10378532\n",
            "Iteration 26, loss = 0.10133948\n",
            "Iteration 27, loss = 0.09901220\n",
            "Iteration 28, loss = 0.09697084\n",
            "Iteration 29, loss = 0.09496202\n",
            "Iteration 30, loss = 0.09295482\n",
            "Iteration 31, loss = 0.09074511\n",
            "Iteration 32, loss = 0.08912391\n",
            "Iteration 33, loss = 0.08738903\n",
            "Iteration 34, loss = 0.08565472\n",
            "Iteration 35, loss = 0.08420766\n",
            "Iteration 36, loss = 0.08272366\n",
            "Iteration 37, loss = 0.08120384\n",
            "Iteration 38, loss = 0.07996620\n",
            "Iteration 39, loss = 0.07851583\n",
            "Iteration 40, loss = 0.07702592\n",
            "Iteration 41, loss = 0.07591513\n",
            "Iteration 42, loss = 0.07467269\n",
            "Iteration 43, loss = 0.07376388\n",
            "Iteration 44, loss = 0.07244570\n",
            "Iteration 45, loss = 0.07137564\n",
            "Iteration 46, loss = 0.07032046\n",
            "Iteration 47, loss = 0.06939159\n",
            "Iteration 48, loss = 0.06840130\n",
            "Iteration 49, loss = 0.06723987\n",
            "Iteration 50, loss = 0.06638381\n",
            "Iteration 51, loss = 0.06555810\n",
            "Iteration 52, loss = 0.06460751\n",
            "Iteration 53, loss = 0.06360214\n",
            "Iteration 54, loss = 0.06291040\n",
            "Iteration 55, loss = 0.06216025\n",
            "Iteration 56, loss = 0.06152016\n",
            "Iteration 57, loss = 0.06061969\n",
            "Iteration 58, loss = 0.05980390\n",
            "Iteration 59, loss = 0.05905587\n",
            "Iteration 60, loss = 0.05845287\n",
            "Iteration 61, loss = 0.05767261\n",
            "Iteration 62, loss = 0.05686698\n",
            "Iteration 63, loss = 0.05637241\n",
            "Iteration 64, loss = 0.05583489\n",
            "Iteration 65, loss = 0.05512632\n",
            "Iteration 66, loss = 0.05443720\n",
            "Iteration 67, loss = 0.05388250\n",
            "Iteration 68, loss = 0.05327557\n",
            "Iteration 69, loss = 0.05277201\n",
            "Iteration 70, loss = 0.05213810\n",
            "Iteration 71, loss = 0.05150209\n",
            "Iteration 72, loss = 0.05100463\n",
            "Iteration 73, loss = 0.05051139\n",
            "Iteration 74, loss = 0.04998574\n",
            "Iteration 75, loss = 0.04954391\n",
            "Iteration 76, loss = 0.04879035\n",
            "Iteration 77, loss = 0.04855296\n",
            "Iteration 78, loss = 0.04804704\n",
            "Iteration 79, loss = 0.04744306\n",
            "Iteration 80, loss = 0.04739455\n",
            "Iteration 81, loss = 0.04670309\n",
            "Iteration 82, loss = 0.04610933\n",
            "Iteration 83, loss = 0.04579732\n",
            "Iteration 84, loss = 0.04527204\n",
            "Iteration 85, loss = 0.04505492\n",
            "Iteration 86, loss = 0.04451223\n",
            "Iteration 87, loss = 0.04400939\n",
            "Iteration 88, loss = 0.04364029\n",
            "Iteration 89, loss = 0.04345397\n",
            "Iteration 90, loss = 0.04289662\n",
            "Iteration 91, loss = 0.04255368\n",
            "Iteration 92, loss = 0.04221875\n",
            "Iteration 93, loss = 0.04181239\n",
            "Iteration 94, loss = 0.04146468\n",
            "Iteration 95, loss = 0.04119931\n",
            "Iteration 96, loss = 0.04077622\n",
            "Iteration 97, loss = 0.04047715\n",
            "Iteration 98, loss = 0.04015814\n",
            "Iteration 99, loss = 0.03981521\n",
            "Iteration 100, loss = 0.03955621\n",
            "Iteration 101, loss = 0.03915517\n",
            "Iteration 102, loss = 0.03904632\n",
            "Iteration 103, loss = 0.03854489\n",
            "Iteration 104, loss = 0.03834123\n",
            "Iteration 105, loss = 0.03802273\n",
            "Iteration 106, loss = 0.03763450\n",
            "Iteration 107, loss = 0.03738968\n",
            "Iteration 108, loss = 0.03698652\n",
            "Iteration 109, loss = 0.03682003\n",
            "Iteration 110, loss = 0.03661497\n",
            "Iteration 111, loss = 0.03632661\n",
            "Iteration 112, loss = 0.03613755\n",
            "Iteration 113, loss = 0.03595112\n",
            "Iteration 114, loss = 0.03556792\n",
            "Iteration 115, loss = 0.03524888\n",
            "Iteration 116, loss = 0.03508341\n",
            "Iteration 117, loss = 0.03485294\n",
            "Iteration 118, loss = 0.03456703\n",
            "Iteration 119, loss = 0.03433396\n",
            "Iteration 120, loss = 0.03415643\n",
            "Iteration 121, loss = 0.03392089\n",
            "Iteration 122, loss = 0.03369334\n",
            "Iteration 123, loss = 0.03359918\n",
            "Iteration 124, loss = 0.03327821\n",
            "Iteration 125, loss = 0.03306742\n",
            "Iteration 126, loss = 0.03285904\n",
            "Iteration 127, loss = 0.03261904\n",
            "Iteration 128, loss = 0.03239006\n",
            "Iteration 129, loss = 0.03220186\n",
            "Iteration 130, loss = 0.03190238\n",
            "Iteration 131, loss = 0.03174641\n",
            "Iteration 132, loss = 0.03177540\n",
            "Iteration 133, loss = 0.03148466\n",
            "Iteration 134, loss = 0.03126944\n",
            "Iteration 135, loss = 0.03109514\n",
            "Iteration 136, loss = 0.03089911\n",
            "Iteration 137, loss = 0.03077134\n",
            "Iteration 138, loss = 0.03074872\n",
            "Iteration 139, loss = 0.03041143\n",
            "Iteration 140, loss = 0.03029596\n",
            "Iteration 141, loss = 0.03014552\n",
            "Iteration 142, loss = 0.02993112\n",
            "Iteration 143, loss = 0.02974024\n",
            "Iteration 144, loss = 0.02957005\n",
            "Iteration 145, loss = 0.02950086\n",
            "Iteration 146, loss = 0.02925733\n",
            "Iteration 147, loss = 0.02917756\n",
            "Iteration 148, loss = 0.02902424\n",
            "Iteration 149, loss = 0.02887811\n",
            "Iteration 150, loss = 0.02867300\n",
            "Iteration 151, loss = 0.02855578\n",
            "Iteration 152, loss = 0.02844942\n",
            "Iteration 153, loss = 0.02834028\n",
            "Iteration 154, loss = 0.02813598\n",
            "Iteration 155, loss = 0.02800036\n",
            "Iteration 156, loss = 0.02792343\n",
            "Iteration 157, loss = 0.02771059\n",
            "Iteration 158, loss = 0.02768987\n",
            "Iteration 159, loss = 0.02752203\n",
            "Iteration 160, loss = 0.02734819\n",
            "Iteration 161, loss = 0.02724592\n",
            "Iteration 162, loss = 0.02708185\n",
            "Iteration 163, loss = 0.02702623\n",
            "Iteration 164, loss = 0.02692941\n",
            "Iteration 165, loss = 0.02676813\n",
            "Iteration 166, loss = 0.02673706\n",
            "Iteration 167, loss = 0.02654885\n",
            "Iteration 168, loss = 0.02638828\n",
            "Iteration 169, loss = 0.02636107\n",
            "Iteration 170, loss = 0.02626703\n",
            "Iteration 171, loss = 0.02611144\n",
            "Iteration 172, loss = 0.02601080\n",
            "Iteration 173, loss = 0.02599704\n",
            "Iteration 174, loss = 0.02575943\n",
            "Iteration 175, loss = 0.02575072\n",
            "Iteration 176, loss = 0.02561648\n",
            "Iteration 177, loss = 0.02560790\n",
            "Iteration 178, loss = 0.02537408\n",
            "Iteration 179, loss = 0.02533754\n",
            "Iteration 180, loss = 0.02532227\n",
            "Iteration 181, loss = 0.02521857\n",
            "Iteration 182, loss = 0.02508274\n",
            "Iteration 183, loss = 0.02497259\n",
            "Iteration 184, loss = 0.02487126\n",
            "Iteration 185, loss = 0.02475029\n",
            "Iteration 186, loss = 0.02474162\n",
            "Iteration 187, loss = 0.02459089\n",
            "Iteration 188, loss = 0.02453062\n",
            "Iteration 189, loss = 0.02445432\n",
            "Iteration 190, loss = 0.02437926\n",
            "Iteration 191, loss = 0.02429475\n",
            "Iteration 192, loss = 0.02425932\n",
            "Iteration 193, loss = 0.02417743\n",
            "Iteration 194, loss = 0.02396331\n",
            "Iteration 195, loss = 0.02391551\n",
            "Iteration 196, loss = 0.02396394\n",
            "Iteration 197, loss = 0.02385143\n",
            "Iteration 198, loss = 0.02376792\n",
            "Iteration 199, loss = 0.02374289\n",
            "Iteration 200, loss = 0.02360476\n",
            "Iteration 201, loss = 0.02357534\n",
            "Iteration 202, loss = 0.02351188\n",
            "Iteration 203, loss = 0.02344980\n",
            "Iteration 204, loss = 0.02331842\n",
            "Iteration 205, loss = 0.02326380\n",
            "Iteration 206, loss = 0.02316166\n",
            "Iteration 207, loss = 0.02318271\n",
            "Iteration 208, loss = 0.02309092\n",
            "Iteration 209, loss = 0.02301490\n",
            "Iteration 210, loss = 0.02293142\n",
            "Iteration 211, loss = 0.02290801\n",
            "Iteration 212, loss = 0.02279947\n",
            "Iteration 213, loss = 0.02272150\n",
            "Iteration 214, loss = 0.02266641\n",
            "Iteration 215, loss = 0.02269754\n",
            "Iteration 216, loss = 0.02261850\n",
            "Iteration 217, loss = 0.02249412\n",
            "Iteration 218, loss = 0.02249087\n",
            "Iteration 219, loss = 0.02239473\n",
            "Iteration 220, loss = 0.02234531\n",
            "Iteration 221, loss = 0.02227755\n",
            "Iteration 222, loss = 0.02226609\n",
            "Iteration 223, loss = 0.02223867\n",
            "Iteration 224, loss = 0.02218460\n",
            "Iteration 225, loss = 0.02211229\n",
            "Iteration 226, loss = 0.02202673\n",
            "Iteration 227, loss = 0.02199307\n",
            "Iteration 228, loss = 0.02193633\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "==========================\n",
            "Question 0-0 answered:\n",
            "==========================\n",
            "time taken to run: 1306.2878746459992\n"
          ]
        }
      ],
      "source": [
        "# Question 0-0 (cont'd) here:\n",
        "# Increasing iterations and using 10 batches as model stopped at 10 batches\n",
        "# for 250 iterations.\n",
        "import time\n",
        "t1 = time.perf_counter()\n",
        "iterations = 250\n",
        "\n",
        "mlp_mnist = MLPClassifier(verbose = True, hidden_layer_sizes=(50,), batch_size = 10, \\\n",
        "                          max_iter=iterations, solver='sgd', activation='logistic',\\\n",
        "                          learning_rate = 'constant', learning_rate_init = 0.001, random_state=1)\n",
        "mlp_mnist.fit(X_train,y_train)\n",
        "print('==========================')\n",
        "print(\"Question 0-0 answered:\")\n",
        "print('==========================')\n",
        "t2 = time.perf_counter()\n",
        "print('time taken to run:',t2-t1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "mDpqVYN1I1qq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68946800-103d-4890-af0d-9813551bff98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10000,)\n",
            "==========================\n"
          ]
        }
      ],
      "source": [
        "# At iteration 228: Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. \n",
        "predictions = mlp_mnist.predict(X_test)\n",
        "probability_predictions = mlp_mnist.predict_proba(X_test)\n",
        "print(predictions.shape)\n",
        "print('==========================')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qaKRY5l7I1qq"
      },
      "source": [
        "**Q0-1**: Write a short function that outputs the **second best** prediction for each point, i.e. the label which gets the second highest probability in the softmax output. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "y_rtIEcV7rGr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "67506d3b-a636-4929-c4f8-1f7d0c32a879"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'softmax'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "# Checking the activation function\n",
        "mlp_mnist.out_activation_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "4sKDv_669RYl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b01d691-a68b-4b87-e2b7-1cc67f8f4eb6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "0\n",
            "0\n",
            "7.258289918900998e-09\n",
            "7.258289918900998e-09\n",
            "1.3410481385701389e-07\n",
            "9.229837886005287e-05\n",
            "9.229837886005287e-05\n",
            "9.229837886005287e-05\n",
            "9.229837886005287e-05\n",
            "9.229837886005287e-05\n",
            "0\n",
            "1.074491963753215e-06\n",
            "1.3428721681960633e-06\n",
            "1.3428721681960633e-06\n",
            "1.3428721681960633e-06\n",
            "1.3428721681960633e-06\n",
            "1.3428721681960633e-06\n",
            "1.3428721681960633e-06\n",
            "1.3428721681960633e-06\n",
            "1.3428721681960633e-06\n",
            "0\n",
            "2.4353307328029525e-08\n",
            "1.5144334413381458e-05\n",
            "1.5144334413381458e-05\n",
            "1.5144334413381458e-05\n",
            "1.5144334413381458e-05\n",
            "1.5144334413381458e-05\n",
            "1.5144334413381458e-05\n",
            "1.5144334413381458e-05\n",
            "1.5144334413381458e-05\n",
            "0\n",
            "0\n",
            "3.801117654018949e-08\n",
            "0.00019997595865598199\n",
            "0.00019997595865598199\n",
            "0.00019997595865598199\n",
            "0.00019997595865598199\n",
            "0.00019997595865598199\n",
            "0.00019997595865598199\n",
            "0.00019997595865598199\n",
            "0\n",
            "2.6945886245701235e-08\n",
            "8.43574942572238e-06\n",
            "8.43574942572238e-06\n",
            "8.43574942572238e-06\n",
            "1.1338712926917656e-05\n",
            "1.1338712926917656e-05\n",
            "1.1338712926917656e-05\n",
            "2.0298638642168393e-05\n",
            "2.0298638642168393e-05\n",
            "0\n",
            "0\n",
            "0\n",
            "6.496413910599232e-07\n",
            "6.496413910599232e-07\n",
            "6.496413910599232e-07\n",
            "6.496413910599232e-07\n",
            "6.496413910599232e-07\n",
            "6.496413910599232e-07\n",
            "6.496413910599232e-07\n",
            "0\n",
            "5.2138523121656774e-09\n",
            "2.089552344340274e-06\n",
            "2.089552344340274e-06\n",
            "2.089552344340274e-06\n",
            "5.81192428847944e-06\n",
            "5.81192428847944e-06\n",
            "5.81192428847944e-06\n",
            "5.81192428847944e-06\n",
            "5.81192428847944e-06\n",
            "0\n",
            "0\n",
            "0\n",
            "5.051127710428636e-05\n",
            "5.051127710428636e-05\n",
            "5.051127710428636e-05\n",
            "5.051127710428636e-05\n",
            "5.051127710428636e-05\n",
            "0.00023621005130021973\n",
            "0.00023621005130021973\n",
            "0\n",
            "0\n",
            "6.756379179017908e-07\n",
            "6.756379179017908e-07\n",
            "6.756379179017908e-07\n",
            "6.756379179017908e-07\n",
            "7.635583175091645e-07\n",
            "5.181311697152246e-05\n",
            "5.181311697152246e-05\n",
            "0.00029539508684312503\n",
            "0\n",
            "1.2511603039587892e-10\n",
            "1.4354823887736288e-09\n",
            "1.4354823887736288e-09\n",
            "2.048309041324301e-06\n",
            "2.048309041324301e-06\n",
            "2.048309041324301e-06\n",
            "2.048309041324301e-06\n",
            "2.048309041324301e-06\n",
            "2.048309041324301e-06\n",
            "0\n",
            "4.545480576743615e-09\n",
            "2.6263050236981106e-08\n",
            "2.6263050236981106e-08\n",
            "4.1610563003474526e-05\n",
            "4.1610563003474526e-05\n",
            "4.1610563003474526e-05\n",
            "4.1610563003474526e-05\n",
            "4.1610563003474526e-05\n",
            "4.1610563003474526e-05\n",
            "0\n",
            "0\n",
            "2.4739568113153523e-08\n",
            "2.8500398025711223e-08\n",
            "2.8500398025711223e-08\n",
            "2.8500398025711223e-08\n",
            "2.8500398025711223e-08\n",
            "2.8500398025711223e-08\n",
            "2.8500398025711223e-08\n",
            "2.7677457259316317e-07\n",
            "0\n",
            "6.421110045624179e-08\n",
            "6.421110045624179e-08\n",
            "6.421110045624179e-08\n",
            "6.421110045624179e-08\n",
            "6.421110045624179e-08\n",
            "6.421110045624179e-08\n",
            "6.421110045624179e-08\n",
            "6.421110045624179e-08\n",
            "4.3125919279516534e-05\n",
            "0\n",
            "0\n",
            "7.716355882108535e-07\n",
            "1.2097796011288395e-05\n",
            "1.2097796011288395e-05\n",
            "0.0005780735637568304\n",
            "0.0005780735637568304\n",
            "0.0015191962454371621\n",
            "0.00797408146562596\n",
            "0.01829970319969412\n",
            "0\n",
            "0\n",
            "0\n",
            "0.03505089844869317\n",
            "0.03505089844869317\n",
            "0.03505089844869317\n",
            "0.03505089844869317\n",
            "0.33741741024684896\n",
            "0.33741741024684896\n",
            "0.33741741024684896\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "4.010635241965789e-09\n",
            "1.994929959978543e-08\n",
            "1.994929959978543e-08\n",
            "1.994929959978543e-08\n",
            "1.994929959978543e-08\n",
            "1.9914169077310772e-06\n",
            "0\n",
            "0\n",
            "0\n",
            "1.2466503323642115e-08\n",
            "1.2466503323642115e-08\n",
            "1.2466503323642115e-08\n",
            "1.2466503323642115e-08\n",
            "1.2466503323642115e-08\n",
            "1.2466503323642115e-08\n",
            "1.2466503323642115e-08\n",
            "0\n",
            "4.774906574073639e-08\n",
            "4.774906574073639e-08\n",
            "4.774906574073639e-08\n",
            "4.774906574073639e-08\n",
            "6.029518117961453e-06\n",
            "6.029518117961453e-06\n",
            "5.417927499404382e-05\n",
            "5.417927499404382e-05\n",
            "0.00011702063100087648\n",
            "0\n",
            "1.5242102760834634e-08\n",
            "3.042599156074983e-06\n",
            "3.042599156074983e-06\n",
            "3.042599156074983e-06\n",
            "3.0895225991137935e-06\n",
            "3.0895225991137935e-06\n",
            "3.0895225991137935e-06\n",
            "3.0895225991137935e-06\n",
            "3.0895225991137935e-06\n",
            "0\n",
            "0\n",
            "0\n",
            "1.2232437135190102e-06\n",
            "1.2232437135190102e-06\n",
            "1.2232437135190102e-06\n",
            "1.2232437135190102e-06\n",
            "1.2232437135190102e-06\n",
            "1.2232437135190102e-06\n",
            "1.2232437135190102e-06\n",
            "0\n",
            "0\n",
            "1.2703553666369084e-09\n",
            "1.2703553666369084e-09\n",
            "1.2703553666369084e-09\n",
            "7.712296338185052e-07\n",
            "7.712296338185052e-07\n",
            "7.712296338185052e-07\n",
            "7.712296338185052e-07\n",
            "7.712296338185052e-07\n",
            "0\n",
            "7.92211657308701e-12\n",
            "1.1099060786276491e-09\n",
            "1.3638618389873175e-09\n",
            "1.3638618389873175e-09\n",
            "3.2214545860446156e-08\n",
            "6.549857368930489e-08\n",
            "4.4281300102465825e-06\n",
            "4.4281300102465825e-06\n",
            "4.57120699968836e-05\n",
            "0\n",
            "7.298785391817271e-10\n",
            "5.404832078337909e-07\n",
            "5.404832078337909e-07\n",
            "5.404832078337909e-07\n",
            "5.0949545471554415e-06\n",
            "5.0949545471554415e-06\n",
            "4.636918971348434e-05\n",
            "4.636918971348434e-05\n",
            "0.00031635390338450946\n",
            "0\n",
            "2.28896852723575e-06\n",
            "6.788148244103082e-06\n",
            "2.8761624389741592e-05\n",
            "2.8761624389741592e-05\n",
            "0.0022022815592635573\n",
            "0.0022022815592635573\n",
            "0.0022022815592635573\n",
            "0.0022022815592635573\n",
            "0.0022022815592635573\n",
            "0\n",
            "0\n",
            "2.3430209640677113e-05\n",
            "2.3430209640677113e-05\n",
            "2.3430209640677113e-05\n",
            "0.0015189415748600096\n",
            "0.0015189415748600096\n",
            "0.0015189415748600096\n",
            "0.0015189415748600096\n",
            "0.0015189415748600096\n",
            "0\n",
            "0\n",
            "2.7573448021235387e-09\n",
            "0.00016002323390137304\n",
            "0.00016002323390137304\n",
            "0.00016002323390137304\n",
            "0.00016002323390137304\n",
            "0.00016002323390137304\n",
            "0.00016002323390137304\n",
            "0.00016002323390137304\n",
            "0\n",
            "9.391513075553434e-12\n",
            "9.391513075553434e-12\n",
            "5.281790314444678e-08\n",
            "5.281790314444678e-08\n",
            "5.281790314444678e-08\n",
            "5.281790314444678e-08\n",
            "5.281790314444678e-08\n",
            "5.281790314444678e-08\n",
            "5.281790314444678e-08\n",
            "0\n",
            "0\n",
            "7.979263378184665e-05\n",
            "7.979263378184665e-05\n",
            "7.979263378184665e-05\n",
            "0.000604775654019175\n",
            "0.000604775654019175\n",
            "0.06161208138229973\n",
            "0.06161208138229973\n",
            "0.06161208138229973\n",
            "0\n",
            "0\n",
            "0\n",
            "2.350741295575737e-07\n",
            "2.350741295575737e-07\n",
            "2.350741295575737e-07\n",
            "2.350741295575737e-07\n",
            "2.350741295575737e-07\n",
            "2.3006921955773563e-06\n",
            "2.3006921955773563e-06\n",
            "0\n",
            "0\n",
            "1.5526468564933769e-06\n",
            "1.9077178296607418e-06\n",
            "1.9077178296607418e-06\n",
            "8.086073129164198e-06\n",
            "1.0333504971383758e-05\n",
            "1.0333504971383758e-05\n",
            "4.604438645666057e-05\n",
            "4.604438645666057e-05\n",
            "0\n",
            "1.3726907182000076e-10\n",
            "1.3726907182000076e-10\n",
            "4.917315736312637e-08\n",
            "4.917315736312637e-08\n",
            "4.917315736312637e-08\n",
            "4.917315736312637e-08\n",
            "5.183918633452886e-08\n",
            "5.183918633452886e-08\n",
            "5.183918633452886e-08\n",
            "0\n",
            "1.94573513221605e-07\n",
            "1.94573513221605e-07\n",
            "1.94573513221605e-07\n",
            "1.94573513221605e-07\n",
            "1.2211968719961696e-05\n",
            "1.2211968719961696e-05\n",
            "1.2211968719961696e-05\n",
            "1.2211968719961696e-05\n",
            "1.2211968719961696e-05\n",
            "0\n",
            "0\n",
            "4.685331412633073e-06\n",
            "4.946498380438694e-06\n",
            "4.946498380438694e-06\n",
            "4.946498380438694e-06\n",
            "4.946498380438694e-06\n",
            "1.3423456711477114e-05\n",
            "1.3423456711477114e-05\n",
            "1.3423456711477114e-05\n",
            "0\n",
            "6.386520021900773e-08\n",
            "0.020658660116854893\n",
            "0.020658660116854893\n",
            "0.020658660116854893\n",
            "0.020658660116854893\n",
            "0.020658660116854893\n",
            "0.020658660116854893\n",
            "0.02421319424178018\n",
            "0.02421319424178018\n",
            "0\n",
            "1.2608477632464171e-09\n",
            "1.2608477632464171e-09\n",
            "0.004132180858063643\n",
            "0.004132180858063643\n",
            "0.004132180858063643\n",
            "0.004132180858063643\n",
            "0.004132180858063643\n",
            "0.004132180858063643\n",
            "0.004132180858063643\n",
            "0\n",
            "0\n",
            "3.926854723884095e-07\n",
            "7.21382225788929e-07\n",
            "7.21382225788929e-07\n",
            "7.21382225788929e-07\n",
            "7.21382225788929e-07\n",
            "7.21382225788929e-07\n",
            "3.159742857029652e-06\n",
            "3.159742857029652e-06\n",
            "0\n",
            "3.4275948823872696e-08\n",
            "6.452995213472092e-07\n",
            "1.3927829892083736e-05\n",
            "1.3927829892083736e-05\n",
            "1.3927829892083736e-05\n",
            "1.3927829892083736e-05\n",
            "1.3927829892083736e-05\n",
            "1.3927829892083736e-05\n",
            "0.00011827162178111841\n",
            "0\n",
            "1.2782632010908972e-05\n",
            "1.2782632010908972e-05\n",
            "6.0305215335675526e-05\n",
            "0.006486513707329143\n",
            "0.006486513707329143\n",
            "0.006486513707329143\n",
            "0.2820304694886901\n",
            "0.2820304694886901\n",
            "0.2820304694886901\n",
            "0\n",
            "0\n",
            "9.111559344141887e-05\n",
            "9.111559344141887e-05\n",
            "9.111559344141887e-05\n",
            "9.111559344141887e-05\n",
            "9.111559344141887e-05\n",
            "0.0010328517499488144\n",
            "0.0010328517499488144\n",
            "0.0010328517499488144\n",
            "0\n",
            "1.8455469992344916e-06\n",
            "4.0034852913204625e-05\n",
            "4.0034852913204625e-05\n",
            "4.0034852913204625e-05\n",
            "4.0034852913204625e-05\n",
            "0.0029107969791482523\n",
            "0.0029107969791482523\n",
            "0.0029107969791482523\n",
            "0.0029107969791482523\n",
            "0\n",
            "5.173474051396958e-07\n",
            "1.4177711042738737e-06\n",
            "1.4177711042738737e-06\n",
            "1.4177711042738737e-06\n",
            "1.4177711042738737e-06\n",
            "1.4177711042738737e-06\n",
            "1.4177711042738737e-06\n",
            "1.4177711042738737e-06\n",
            "1.8105035557241766e-05\n",
            "0\n",
            "0\n",
            "3.15175724880839e-05\n",
            "0.00023369206446055126\n",
            "0.00023369206446055126\n",
            "0.00023369206446055126\n",
            "0.00023369206446055126\n",
            "0.00023369206446055126\n",
            "0.00023369206446055126\n",
            "0.0004282271891943951\n",
            "0\n",
            "2.8947371152493548e-09\n",
            "2.8947371152493548e-09\n",
            "5.905143373155098e-07\n",
            "5.905143373155098e-07\n",
            "1.2032070420857004e-06\n",
            "1.2032070420857004e-06\n",
            "2.124269545811769e-06\n",
            "2.124269545811769e-06\n",
            "2.124269545811769e-06\n",
            "0\n",
            "0\n",
            "4.0655032128189e-08\n",
            "4.0655032128189e-08\n",
            "3.5036294657592594e-06\n",
            "3.5036294657592594e-06\n",
            "3.5036294657592594e-06\n",
            "3.5036294657592594e-06\n",
            "6.115638487286212e-06\n",
            "6.115638487286212e-06\n",
            "0\n",
            "4.487632628814022e-08\n",
            "4.487632628814022e-08\n",
            "0.0017514712562922577\n",
            "0.0017514712562922577\n",
            "0.0017514712562922577\n",
            "0.0017514712562922577\n",
            "0.0017514712562922577\n",
            "0.0017514712562922577\n",
            "0.0017514712562922577\n",
            "0\n",
            "0\n",
            "0.0006989321956206474\n",
            "0.0006989321956206474\n",
            "0.0006989321956206474\n",
            "0.0006989321956206474\n",
            "0.0006989321956206474\n",
            "0.0006989321956206474\n",
            "0.0006989321956206474\n",
            "0.0006989321956206474\n",
            "0\n",
            "2.060581801663877e-10\n",
            "2.060581801663877e-10\n",
            "1.1141449350043095e-05\n",
            "1.1141449350043095e-05\n",
            "4.2649442152090146e-05\n",
            "4.309873552689106e-05\n",
            "4.309873552689106e-05\n",
            "4.309873552689106e-05\n",
            "4.309873552689106e-05\n",
            "0\n",
            "0\n",
            "1.1159105384739987e-10\n",
            "2.6974594797715586e-06\n",
            "2.6974594797715586e-06\n",
            "3.2919991134066314e-05\n",
            "3.2919991134066314e-05\n",
            "3.2919991134066314e-05\n",
            "3.2919991134066314e-05\n",
            "3.2919991134066314e-05\n",
            "0\n",
            "2.2664377702963896e-09\n",
            "2.2664377702963896e-09\n",
            "2.2664377702963896e-09\n",
            "2.2664377702963896e-09\n",
            "2.2664377702963896e-09\n",
            "2.2664377702963896e-09\n",
            "8.487041378242798e-08\n",
            "8.487041378242798e-08\n",
            "8.487041378242798e-08\n",
            "0\n",
            "0\n",
            "0\n",
            "1.5516145701307326e-06\n",
            "1.5516145701307326e-06\n",
            "1.5516145701307326e-06\n",
            "1.5516145701307326e-06\n",
            "1.5516145701307326e-06\n",
            "1.5516145701307326e-06\n",
            "1.5516145701307326e-06\n",
            "0\n",
            "0\n",
            "0\n",
            "1.805134919726292e-06\n",
            "7.914505331043778e-06\n",
            "7.914505331043778e-06\n",
            "7.914505331043778e-06\n",
            "7.914505331043778e-06\n",
            "7.914505331043778e-06\n",
            "7.914505331043778e-06\n",
            "0\n",
            "8.618766664844944e-10\n",
            "8.618766664844944e-10\n",
            "2.8522653643234843e-08\n",
            "9.244232657009937e-08\n",
            "9.244232657009937e-08\n",
            "9.244232657009937e-08\n",
            "9.244232657009937e-08\n",
            "9.244232657009937e-08\n",
            "9.244232657009937e-08\n",
            "0\n",
            "0\n",
            "4.756751764988356e-10\n",
            "4.756751764988356e-10\n",
            "4.756751764988356e-10\n",
            "0.0008159404584844035\n",
            "0.0008159404584844035\n",
            "0.0008159404584844035\n",
            "0.0008159404584844035\n",
            "0.0008159404584844035\n",
            "0\n",
            "2.5167694294267593e-08\n",
            "2.9182991561641056e-05\n",
            "2.9182991561641056e-05\n",
            "2.9182991561641056e-05\n",
            "4.2064841419723395e-05\n",
            "4.2064841419723395e-05\n",
            "4.2064841419723395e-05\n",
            "4.2064841419723395e-05\n",
            "4.2064841419723395e-05\n",
            "0\n",
            "0\n",
            "3.628872574036289e-10\n",
            "3.628872574036289e-10\n",
            "7.368014395396809e-10\n",
            "3.898959934826494e-07\n",
            "3.898959934826494e-07\n",
            "3.898959934826494e-07\n",
            "3.898959934826494e-07\n",
            "3.898959934826494e-07\n",
            "0\n",
            "3.991382994781398e-06\n",
            "3.445599183445843e-05\n",
            "3.445599183445843e-05\n",
            "3.445599183445843e-05\n",
            "3.445599183445843e-05\n",
            "3.445599183445843e-05\n",
            "3.445599183445843e-05\n",
            "3.445599183445843e-05\n",
            "3.445599183445843e-05\n",
            "0\n",
            "0\n",
            "9.215937628334822e-08\n",
            "9.215937628334822e-08\n",
            "9.215937628334822e-08\n",
            "9.215937628334822e-08\n",
            "2.982313715699746e-07\n",
            "3.63748495708489e-05\n",
            "0.0003460527960303292\n",
            "0.0003460527960303292\n",
            "0\n",
            "0\n",
            "1.3049474402585518e-07\n",
            "1.3049474402585518e-07\n",
            "1.3049474402585518e-07\n",
            "0.0008659556743937799\n",
            "0.0008659556743937799\n",
            "0.0008659556743937799\n",
            "0.0008948523584482016\n",
            "0.01754279509676089\n",
            "0\n",
            "1.864800623318572e-10\n",
            "1.937613185266427e-09\n",
            "1.937613185266427e-09\n",
            "9.791415374714622e-07\n",
            "9.791415374714622e-07\n",
            "9.791415374714622e-07\n",
            "1.069870778833872e-06\n",
            "1.069870778833872e-06\n",
            "1.069870778833872e-06\n",
            "0\n",
            "0\n",
            "6.21257862141021e-08\n",
            "4.3095586472245084e-06\n",
            "4.3095586472245084e-06\n",
            "4.3095586472245084e-06\n",
            "4.3095586472245084e-06\n",
            "4.3095586472245084e-06\n",
            "4.3095586472245084e-06\n",
            "0.001270499480875081\n",
            "0\n",
            "0\n",
            "1.0061396410937442e-08\n",
            "1.0061396410937442e-08\n",
            "0.003794786452545371\n",
            "0.01183085628846609\n",
            "0.01183085628846609\n",
            "0.01183085628846609\n",
            "0.01183085628846609\n",
            "0.01183085628846609\n",
            "0\n",
            "3.4689853831600656e-11\n",
            "6.785181237958142e-07\n",
            "6.785181237958142e-07\n",
            "6.785181237958142e-07\n",
            "6.785181237958142e-07\n",
            "6.785181237958142e-07\n",
            "6.785181237958142e-07\n",
            "6.785181237958142e-07\n",
            "1.6168825655300855e-05\n",
            "0\n",
            "0\n",
            "9.825171635871695e-09\n",
            "2.999270122163188e-08\n",
            "2.999270122163188e-08\n",
            "3.036480197868498e-07\n",
            "3.036480197868498e-07\n",
            "3.036480197868498e-07\n",
            "3.036480197868498e-07\n",
            "1.2484922077497764e-06\n",
            "0\n",
            "3.576068169174521e-05\n",
            "3.576068169174521e-05\n",
            "5.354278341695591e-05\n",
            "5.354278341695591e-05\n",
            "0.0013107611631115279\n",
            "0.0013107611631115279\n",
            "0.0013107611631115279\n",
            "0.0013107611631115279\n",
            "0.0016238851730063385\n",
            "0\n",
            "1.098493829435521e-09\n",
            "1.098493829435521e-09\n",
            "1.098493829435521e-09\n",
            "1.098493829435521e-09\n",
            "1.8904651317513218e-06\n",
            "1.8904651317513218e-06\n",
            "1.8904651317513218e-06\n",
            "1.8904651317513218e-06\n",
            "3.818645427469119e-06\n",
            "0\n",
            "0\n",
            "6.472449704834651e-07\n",
            "6.472449704834651e-07\n",
            "6.472449704834651e-07\n",
            "6.472449704834651e-07\n",
            "6.472449704834651e-07\n",
            "6.472449704834651e-07\n",
            "3.894389954417251e-06\n",
            "3.894389954417251e-06\n",
            "0\n",
            "0\n",
            "3.727139131097119e-07\n",
            "5.597005232678335e-07\n",
            "5.597005232678335e-07\n",
            "2.2324829613672896e-05\n",
            "2.2324829613672896e-05\n",
            "2.2324829613672896e-05\n",
            "2.2324829613672896e-05\n",
            "2.2324829613672896e-05\n",
            "0\n",
            "2.803776543886626e-08\n",
            "3.468089040279744e-08\n",
            "3.532141823889522e-08\n",
            "3.0420902850583216e-07\n",
            "0.00017945014144496923\n",
            "0.0002172361949243057\n",
            "0.0002172361949243057\n",
            "0.0002172361949243057\n",
            "0.0002172361949243057\n",
            "0\n",
            "0\n",
            "3.6050677621253204e-10\n",
            "3.6050677621253204e-10\n",
            "3.6050677621253204e-10\n",
            "3.6050677621253204e-10\n",
            "2.04734484854576e-09\n",
            "2.04734484854576e-09\n",
            "2.269796044618638e-07\n",
            "2.6751815943910816e-06\n",
            "0\n",
            "1.451290945483174e-08\n",
            "1.451290945483174e-08\n",
            "0.0005735231399407345\n",
            "0.0005735231399407345\n",
            "0.0005735231399407345\n",
            "0.0005735231399407345\n",
            "0.0005735231399407345\n",
            "0.0005735231399407345\n",
            "0.0005735231399407345\n",
            "0\n",
            "0\n",
            "7.631615957670587e-07\n",
            "1.0983977987231295e-05\n",
            "1.0983977987231295e-05\n",
            "1.0983977987231295e-05\n",
            "1.0983977987231295e-05\n",
            "3.0493476520670858e-05\n",
            "3.0493476520670858e-05\n",
            "3.0493476520670858e-05\n",
            "0\n",
            "1.0044459801430052e-06\n",
            "7.178552457214038e-06\n",
            "7.178552457214038e-06\n",
            "7.178552457214038e-06\n",
            "8.992074676609893e-05\n",
            "0.0010918845400699024\n",
            "0.0010918845400699024\n",
            "0.0010918845400699024\n",
            "0.0010918845400699024\n",
            "0\n",
            "0\n",
            "4.44303991434552e-08\n",
            "4.7649964254063846e-06\n",
            "4.7649964254063846e-06\n",
            "4.7649964254063846e-06\n",
            "4.7649964254063846e-06\n",
            "4.7649964254063846e-06\n",
            "4.7649964254063846e-06\n",
            "4.7649964254063846e-06\n",
            "0\n",
            "9.181017093299765e-11\n",
            "9.181017093299765e-11\n",
            "9.181017093299765e-11\n",
            "9.181017093299765e-11\n",
            "7.554474047090628e-06\n",
            "7.554474047090628e-06\n",
            "7.554474047090628e-06\n",
            "7.554474047090628e-06\n",
            "3.8186004444530556e-05\n",
            "0\n",
            "0\n",
            "1.0636878793774275e-07\n",
            "1.9888276486146037e-07\n",
            "1.9888276486146037e-07\n",
            "3.5441033940360354e-07\n",
            "3.5441033940360354e-07\n",
            "2.0686185283021376e-05\n",
            "2.0686185283021376e-05\n",
            "2.0686185283021376e-05\n",
            "0\n",
            "3.022597707832397e-09\n",
            "8.98733996911448e-06\n",
            "8.98733996911448e-06\n",
            "8.98733996911448e-06\n",
            "2.986506162155453e-05\n",
            "2.986506162155453e-05\n",
            "2.986506162155453e-05\n",
            "2.986506162155453e-05\n",
            "2.986506162155453e-05\n",
            "0\n",
            "1.4814836894771638e-09\n",
            "1.4814836894771638e-09\n",
            "2.2636417414808164e-09\n",
            "2.2636417414808164e-09\n",
            "3.6454834637790063e-07\n",
            "3.6454834637790063e-07\n",
            "2.5125235378627473e-06\n",
            "2.5125235378627473e-06\n",
            "2.5125235378627473e-06\n",
            "0\n",
            "4.262277033402787e-08\n",
            "4.2574460874151814e-07\n",
            "4.2574460874151814e-07\n",
            "4.2574460874151814e-07\n",
            "4.2574460874151814e-07\n",
            "4.2610115781622636e-07\n",
            "4.2610115781622636e-07\n",
            "9.657630135175436e-05\n",
            "9.657630135175436e-05\n",
            "0\n",
            "0\n",
            "2.6231767467890476e-07\n",
            "3.7369428143656745e-06\n",
            "3.7369428143656745e-06\n",
            "3.7369428143656745e-06\n",
            "3.7369428143656745e-06\n",
            "4.270414055139594e-06\n",
            "0.0001458031881929083\n",
            "0.00022177073186357385\n",
            "0\n",
            "2.7761798561573225e-10\n",
            "2.7761798561573225e-10\n",
            "3.345815048947947e-09\n",
            "2.070017605318113e-08\n",
            "2.9361828144346094e-07\n",
            "2.9361828144346094e-07\n",
            "2.9361828144346094e-07\n",
            "2.9361828144346094e-07\n",
            "2.9361828144346094e-07\n",
            "0\n",
            "1.0123212163803506e-09\n",
            "1.0123212163803506e-09\n",
            "1.0123212163803506e-09\n",
            "5.610519339018993e-07\n",
            "5.610519339018993e-07\n",
            "5.610519339018993e-07\n",
            "5.610519339018993e-07\n",
            "5.610519339018993e-07\n",
            "5.610519339018993e-07\n",
            "0\n",
            "0\n",
            "3.2124460914523094e-05\n",
            "3.2124460914523094e-05\n",
            "3.2124460914523094e-05\n",
            "3.2124460914523094e-05\n",
            "3.2124460914523094e-05\n",
            "7.624944410538148e-05\n",
            "7.624944410538148e-05\n",
            "7.624944410538148e-05\n",
            "0\n",
            "0\n",
            "1.3910409126082663e-06\n",
            "1.3910409126082663e-06\n",
            "1.3910409126082663e-06\n",
            "1.3910409126082663e-06\n",
            "1.3910409126082663e-06\n",
            "1.3910409126082663e-06\n",
            "1.3910409126082663e-06\n",
            "8.918998643531269e-06\n",
            "0\n",
            "2.5201446591449976e-11\n",
            "5.123260332124121e-08\n",
            "5.123260332124121e-08\n",
            "5.123260332124121e-08\n",
            "5.123260332124121e-08\n",
            "1.077548328552698e-07\n",
            "1.077548328552698e-07\n",
            "1.077548328552698e-07\n",
            "2.7096031157108405e-06\n",
            "0\n",
            "0\n",
            "3.31501197873666e-07\n",
            "3.31501197873666e-07\n",
            "3.31501197873666e-07\n",
            "3.31501197873666e-07\n",
            "3.31501197873666e-07\n",
            "3.31501197873666e-07\n",
            "3.31501197873666e-07\n",
            "6.607513776105614e-07\n",
            "0\n",
            "8.866336766910335e-05\n",
            "8.866336766910335e-05\n",
            "0.003296304474173662\n",
            "0.003296304474173662\n",
            "0.003296304474173662\n",
            "0.003296304474173662\n",
            "0.003296304474173662\n",
            "0.010642323503076555\n",
            "0.010642323503076555\n",
            "0\n",
            "0\n",
            "1.256712058930592e-09\n",
            "1.256712058930592e-09\n",
            "1.256712058930592e-09\n",
            "5.8300434458283285e-09\n",
            "5.8300434458283285e-09\n",
            "5.8300434458283285e-09\n",
            "5.8300434458283285e-09\n",
            "4.946735362012242e-06\n",
            "0\n",
            "2.287095173736715e-09\n",
            "2.287095173736715e-09\n",
            "3.2083814507897717e-06\n",
            "4.9898611108707775e-05\n",
            "4.9898611108707775e-05\n",
            "4.9898611108707775e-05\n",
            "4.9898611108707775e-05\n",
            "4.9898611108707775e-05\n",
            "4.9898611108707775e-05\n",
            "0\n",
            "0\n",
            "0\n",
            "0.0025691809578107\n",
            "0.0025691809578107\n",
            "0.0025691809578107\n",
            "0.0025691809578107\n",
            "0.0025691809578107\n",
            "0.0025691809578107\n",
            "0.0025691809578107\n",
            "0\n",
            "2.915854469460785e-07\n",
            "2.915854469460785e-07\n",
            "2.915854469460785e-07\n",
            "2.915854469460785e-07\n",
            "6.312546252648239e-07\n",
            "6.312546252648239e-07\n",
            "6.312546252648239e-07\n",
            "6.312546252648239e-07\n",
            "0.004815837089537088\n",
            "0\n",
            "4.075946005714912e-08\n",
            "4.075946005714912e-08\n",
            "3.7676566135368826e-05\n",
            "3.7676566135368826e-05\n",
            "3.7676566135368826e-05\n",
            "3.7676566135368826e-05\n",
            "0.007391020608431169\n",
            "0.007391020608431169\n",
            "0.007391020608431169\n",
            "0\n",
            "0\n",
            "0.0002915685770020063\n",
            "0.0002915685770020063\n",
            "0.0002915685770020063\n",
            "0.0002915685770020063\n",
            "0.0002915685770020063\n",
            "0.0010961164453612683\n",
            "0.0010961164453612683\n",
            "0.0010961164453612683\n",
            "0\n",
            "0\n",
            "1.2507250516216732e-11\n",
            "1.2507250516216732e-11\n",
            "5.543705172549381e-09\n",
            "5.543705172549381e-09\n",
            "5.543705172549381e-09\n",
            "3.119404232792718e-07\n",
            "3.119404232792718e-07\n",
            "0.0005924387734120152\n",
            "0\n",
            "0\n",
            "7.320412580959061e-12\n",
            "7.320412580959061e-12\n",
            "2.1368509088386315e-10\n",
            "2.1368509088386315e-10\n",
            "2.1368509088386315e-10\n",
            "2.1368509088386315e-10\n",
            "7.820351952875525e-08\n",
            "7.820351952875525e-08\n",
            "0\n",
            "0\n",
            "0\n",
            "4.170864562385669e-07\n",
            "5.24538537095241e-07\n",
            "5.24538537095241e-07\n",
            "5.24538537095241e-07\n",
            "5.24538537095241e-07\n",
            "5.24538537095241e-07\n",
            "5.24538537095241e-07\n",
            "0\n",
            "1.4742344685665295e-08\n",
            "1.4742344685665295e-08\n",
            "1.4742344685665295e-08\n",
            "1.4742344685665295e-08\n",
            "1.4742344685665295e-08\n",
            "4.580284714248168e-05\n",
            "4.580284714248168e-05\n",
            "4.915696572014732e-05\n",
            "4.915696572014732e-05\n",
            "0\n",
            "0\n",
            "0\n",
            "5.9403952212681734e-06\n",
            "5.9403952212681734e-06\n",
            "5.9403952212681734e-06\n",
            "5.9403952212681734e-06\n",
            "5.9403952212681734e-06\n",
            "5.9403952212681734e-06\n",
            "5.9403952212681734e-06\n",
            "0\n",
            "0\n",
            "0\n",
            "6.197880515947932e-08\n",
            "6.197880515947932e-08\n",
            "6.197880515947932e-08\n",
            "6.197880515947932e-08\n",
            "6.197880515947932e-08\n",
            "4.989796869268867e-07\n",
            "4.989796869268867e-07\n",
            "0\n",
            "7.902560699749032e-08\n",
            "0.00017483410214175728\n",
            "0.00017483410214175728\n",
            "0.00017483410214175728\n",
            "0.00017483410214175728\n",
            "0.00017483410214175728\n",
            "0.00017483410214175728\n",
            "0.00017483410214175728\n",
            "0.00017483410214175728\n",
            "0\n",
            "2.5945572626077073e-08\n",
            "2.5945572626077073e-08\n",
            "1.6589060838526617e-06\n",
            "0.0005475017217492389\n",
            "0.0005475017217492389\n",
            "0.0006478748405915472\n",
            "0.0006478748405915472\n",
            "0.0006478748405915472\n",
            "0.0006478748405915472\n",
            "0\n",
            "0\n",
            "9.881612168409538e-08\n",
            "3.8416122236329693e-05\n",
            "3.8416122236329693e-05\n",
            "3.8416122236329693e-05\n",
            "3.8416122236329693e-05\n",
            "3.8416122236329693e-05\n",
            "7.215778898821059e-05\n",
            "7.215778898821059e-05\n",
            "0\n",
            "3.844644270323474e-06\n",
            "5.302152501053464e-06\n",
            "1.523524739743618e-05\n",
            "2.734821773434682e-05\n",
            "2.734821773434682e-05\n",
            "0.029460952383230658\n",
            "0.029460952383230658\n",
            "0.029460952383230658\n",
            "0.029460952383230658\n",
            "0\n",
            "0\n",
            "2.1831506711500225e-10\n",
            "2.1831506711500225e-10\n",
            "2.1831506711500225e-10\n",
            "4.420967610938852e-07\n",
            "4.420967610938852e-07\n",
            "4.420967610938852e-07\n",
            "4.420967610938852e-07\n",
            "4.420967610938852e-07\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "4.952408032695433e-08\n",
            "9.152399596332684e-08\n",
            "9.152399596332684e-08\n",
            "0.0004247459724636529\n",
            "0.0004247459724636529\n",
            "0.0004247459724636529\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1.5688395674474574e-11\n",
            "7.414091914125083e-08\n",
            "7.414091914125083e-08\n",
            "7.414091914125083e-08\n",
            "2.587050754824004e-07\n",
            "4.856556395818178e-07\n",
            "0\n",
            "0\n",
            "3.1116089222625304e-10\n",
            "3.1116089222625304e-10\n",
            "3.1116089222625304e-10\n",
            "3.434323712043328e-07\n",
            "3.434323712043328e-07\n",
            "3.434323712043328e-07\n",
            "3.434323712043328e-07\n",
            "1.472136307629557e-05\n",
            "0\n",
            "0\n",
            "1.8047215217019202e-06\n",
            "0.00010010664179334508\n",
            "0.00010010664179334508\n",
            "0.00010010664179334508\n",
            "0.00010010664179334508\n",
            "0.00010010664179334508\n",
            "0.00010010664179334508\n",
            "0.00035646792146834847\n",
            "0\n",
            "0\n",
            "0.00044356744601445027\n",
            "0.00044356744601445027\n",
            "0.00044356744601445027\n",
            "0.00044356744601445027\n",
            "0.00044356744601445027\n",
            "0.00044356744601445027\n",
            "0.00044356744601445027\n",
            "0.00044356744601445027\n",
            "0\n",
            "0\n",
            "0\n",
            "4.5152009275090193e-07\n",
            "4.5152009275090193e-07\n",
            "4.5152009275090193e-07\n",
            "4.5152009275090193e-07\n",
            "4.5152009275090193e-07\n",
            "4.5152009275090193e-07\n",
            "1.4682626334247182e-05\n",
            "0\n",
            "0\n",
            "2.4374506103967047e-08\n",
            "4.943173619342394e-07\n",
            "0.0003750875114229786\n",
            "0.0003750875114229786\n",
            "0.0003750875114229786\n",
            "0.0003750875114229786\n",
            "0.0003750875114229786\n",
            "0.0003750875114229786\n",
            "0\n",
            "0\n",
            "0\n",
            "0.006354138927188698\n",
            "0.006354138927188698\n",
            "0.006354138927188698\n",
            "0.006354138927188698\n",
            "0.006354138927188698\n",
            "0.006354138927188698\n",
            "0.006354138927188698\n",
            "0\n",
            "0\n",
            "4.4291535516786534e-10\n",
            "4.4291535516786534e-10\n",
            "4.4291535516786534e-10\n",
            "2.8071368001321813e-08\n",
            "2.8071368001321813e-08\n",
            "2.2610542564506607e-07\n",
            "2.2610542564506607e-07\n",
            "2.2610542564506607e-07\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "2.384123103380557e-06\n",
            "5.487172140153662e-06\n",
            "5.487172140153662e-06\n",
            "5.487172140153662e-06\n",
            "5.487172140153662e-06\n",
            "1.0801619651123812e-05\n",
            "0\n",
            "0\n",
            "2.4655426784560063e-09\n",
            "2.4655426784560063e-09\n",
            "9.427601100001268e-09\n",
            "9.265903428113317e-07\n",
            "9.265903428113317e-07\n",
            "9.265903428113317e-07\n",
            "9.265903428113317e-07\n",
            "9.265903428113317e-07\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "3.930199678371285e-08\n",
            "3.930199678371285e-08\n",
            "3.930199678371285e-08\n",
            "3.930199678371285e-08\n",
            "3.930199678371285e-08\n",
            "2.137005994041334e-05\n",
            "0\n",
            "6.184783381862399e-10\n",
            "6.113724455500297e-08\n",
            "6.113724455500297e-08\n",
            "6.113724455500297e-08\n",
            "6.27721932985642e-06\n",
            "6.27721932985642e-06\n",
            "6.27721932985642e-06\n",
            "6.27721932985642e-06\n",
            "6.27721932985642e-06\n",
            "0\n",
            "3.659342796907126e-07\n",
            "3.869348545238822e-07\n",
            "3.869348545238822e-07\n",
            "3.642228779329032e-05\n",
            "3.642228779329032e-05\n",
            "0.006616022144858283\n",
            "0.006616022144858283\n",
            "0.006616022144858283\n",
            "0.006616022144858283\n",
            "0\n",
            "0\n",
            "4.129911091588663e-08\n",
            "1.4091524860550792e-06\n",
            "1.4091524860550792e-06\n",
            "1.4091524860550792e-06\n",
            "1.4091524860550792e-06\n",
            "1.4091524860550792e-06\n",
            "1.4091524860550792e-06\n",
            "0.0001955997334925923\n",
            "0\n",
            "6.519981048047964e-08\n",
            "9.371913367797631e-06\n",
            "9.371913367797631e-06\n",
            "9.371913367797631e-06\n",
            "2.8801162067741847e-05\n",
            "2.8801162067741847e-05\n",
            "2.8801162067741847e-05\n",
            "2.8801162067741847e-05\n",
            "2.8801162067741847e-05\n",
            "0\n",
            "0\n",
            "3.801663972684785e-07\n",
            "0.0005979162421235689\n",
            "0.0005979162421235689\n",
            "0.00260551397350155\n",
            "0.00260551397350155\n",
            "0.03461526314725388\n",
            "0.03461526314725388\n",
            "0.03581885742065284\n",
            "0\n",
            "1.1773970640088535e-09\n",
            "1.1773970640088535e-09\n",
            "5.3782548883798585e-08\n",
            "5.3782548883798585e-08\n",
            "1.5068745822861036e-05\n",
            "0.00021368562359987706\n",
            "0.00021368562359987706\n",
            "0.00021368562359987706\n",
            "0.00021368562359987706\n",
            "0\n",
            "2.02946810032733e-08\n",
            "2.02946810032733e-08\n",
            "2.02946810032733e-08\n",
            "2.02946810032733e-08\n",
            "2.02946810032733e-08\n",
            "6.933308184289835e-07\n",
            "6.933308184289835e-07\n",
            "6.933308184289835e-07\n",
            "6.933308184289835e-07\n",
            "0\n",
            "0\n",
            "2.209471663183676e-10\n",
            "2.209471663183676e-10\n",
            "2.209471663183676e-10\n",
            "2.5564856440981616e-07\n",
            "2.5564856440981616e-07\n",
            "2.5564856440981616e-07\n",
            "2.5564856440981616e-07\n",
            "5.082709220427916e-07\n",
            "0\n",
            "2.983316535603079e-08\n",
            "2.983316535603079e-08\n",
            "2.983316535603079e-08\n",
            "0.0009684235183981333\n",
            "0.0009684235183981333\n",
            "0.0009895701783541931\n",
            "0.0009895701783541931\n",
            "0.0009895701783541931\n",
            "0.0009895701783541931\n",
            "0\n",
            "2.3617862964020605e-09\n",
            "1.7966310118247857e-06\n",
            "1.7966310118247857e-06\n",
            "1.7966310118247857e-06\n",
            "8.902882193800255e-05\n",
            "8.902882193800255e-05\n",
            "8.902882193800255e-05\n",
            "8.902882193800255e-05\n",
            "8.902882193800255e-05\n",
            "0\n",
            "1.2307123001784594e-07\n",
            "2.3317595719578393e-05\n",
            "2.3317595719578393e-05\n",
            "2.3317595719578393e-05\n",
            "2.3317595719578393e-05\n",
            "4.9114221824505424e-05\n",
            "4.9114221824505424e-05\n",
            "4.9114221824505424e-05\n",
            "4.9114221824505424e-05\n",
            "0\n",
            "0\n",
            "3.0772103366627116e-08\n",
            "3.0772103366627116e-08\n",
            "0.0009938887633212244\n",
            "0.0009938887633212244\n",
            "0.0009938887633212244\n",
            "0.0009938887633212244\n",
            "0.0009938887633212244\n",
            "0.0009938887633212244\n",
            "0\n",
            "0\n",
            "0\n",
            "3.618333599040026e-05\n",
            "3.618333599040026e-05\n",
            "3.618333599040026e-05\n",
            "3.618333599040026e-05\n",
            "3.618333599040026e-05\n",
            "3.618333599040026e-05\n",
            "3.618333599040026e-05\n",
            "0\n",
            "0\n",
            "1.113241111011361e-09\n",
            "1.113241111011361e-09\n",
            "1.113241111011361e-09\n",
            "5.331125286038933e-07\n",
            "5.331125286038933e-07\n",
            "5.331125286038933e-07\n",
            "5.331125286038933e-07\n",
            "5.331125286038933e-07\n",
            "0\n",
            "3.610979742132586e-07\n",
            "0.00014061449389006492\n",
            "0.00034512527507606983\n",
            "0.00034512527507606983\n",
            "0.00034512527507606983\n",
            "0.00034512527507606983\n",
            "0.0018904550094360026\n",
            "0.0018904550094360026\n",
            "0.0018904550094360026\n",
            "0\n",
            "1.819612511507832e-08\n",
            "1.1159689753842683e-05\n",
            "1.1159689753842683e-05\n",
            "1.1159689753842683e-05\n",
            "1.362764696583386e-05\n",
            "3.349101671408421e-05\n",
            "3.349101671408421e-05\n",
            "3.349101671408421e-05\n",
            "9.811922935993232e-05\n",
            "0\n",
            "0\n",
            "1.086774102402066e-08\n",
            "1.086774102402066e-08\n",
            "1.086774102402066e-08\n",
            "0.0002811830017896566\n",
            "0.0002811830017896566\n",
            "0.0002811830017896566\n",
            "0.0002811830017896566\n",
            "0.05091767533937555\n",
            "0\n",
            "0\n",
            "8.338286009232048e-07\n",
            "0.0012339969591651672\n",
            "0.0012339969591651672\n",
            "0.0012339969591651672\n",
            "0.0012339969591651672\n",
            "0.0012339969591651672\n",
            "0.0012339969591651672\n",
            "0.0012339969591651672\n",
            "0\n",
            "4.929955772356918e-07\n",
            "6.324860480142358e-05\n",
            "6.324860480142358e-05\n",
            "6.324860480142358e-05\n",
            "6.324860480142358e-05\n",
            "0.0003480268686768572\n",
            "0.0003480268686768572\n",
            "0.0003480268686768572\n",
            "0.0003480268686768572\n",
            "0\n",
            "0\n",
            "4.203985215245125e-06\n",
            "6.130867218128138e-06\n",
            "6.130867218128138e-06\n",
            "6.130867218128138e-06\n",
            "6.130867218128138e-06\n",
            "9.286712943642535e-06\n",
            "9.286712943642535e-06\n",
            "9.286712943642535e-06\n",
            "0\n",
            "8.761995038719514e-10\n",
            "8.761995038719514e-10\n",
            "4.624424685791708e-09\n",
            "4.624424685791708e-09\n",
            "4.412850731394327e-08\n",
            "9.479443543185846e-07\n",
            "9.479443543185846e-07\n",
            "9.479443543185846e-07\n",
            "9.479443543185846e-07\n",
            "0\n",
            "0\n",
            "0\n",
            "2.5344747396090764e-11\n",
            "1.1428364900060488e-06\n",
            "1.3053787904583329e-06\n",
            "1.3053787904583329e-06\n",
            "1.3053787904583329e-06\n",
            "1.3053787904583329e-06\n",
            "1.3053787904583329e-06\n",
            "0\n",
            "5.501281928729249e-09\n",
            "0.00011661660339837094\n",
            "0.00011661660339837094\n",
            "0.00011661660339837094\n",
            "0.00011661660339837094\n",
            "0.00011661660339837094\n",
            "0.00011661660339837094\n",
            "0.00011661660339837094\n",
            "0.00011661660339837094\n",
            "0\n",
            "0\n",
            "7.93909991811857e-07\n",
            "1.6344541073385313e-05\n",
            "0.00012453949778667948\n",
            "0.00012453949778667948\n",
            "0.00012453949778667948\n",
            "0.00012453949778667948\n",
            "0.00012453949778667948\n",
            "0.00029898101831014347\n",
            "0\n",
            "0\n",
            "4.405533246267106e-09\n",
            "4.405533246267106e-09\n",
            "4.405533246267106e-09\n",
            "5.920549985242545e-08\n",
            "5.920549985242545e-08\n",
            "5.920549985242545e-08\n",
            "1.3027277118942154e-07\n",
            "1.3027277118942154e-07\n",
            "0\n",
            "1.640613353625651e-05\n",
            "0.009426775826220118\n",
            "0.009426775826220118\n",
            "0.009426775826220118\n",
            "0.1475283004796411\n",
            "0.1475283004796411\n",
            "0.1475283004796411\n",
            "0.1475283004796411\n",
            "0.1475283004796411\n",
            "0\n",
            "0\n",
            "9.664312206838929e-07\n",
            "9.664312206838929e-07\n",
            "9.664312206838929e-07\n",
            "0.0013716053912648947\n",
            "0.0013716053912648947\n",
            "0.0013716053912648947\n",
            "0.0013716053912648947\n",
            "0.18401901815088642\n",
            "0\n",
            "0\n",
            "0.00017213192536233058\n",
            "0.00017213192536233058\n",
            "0.00017213192536233058\n",
            "0.018843386975347168\n",
            "0.018843386975347168\n",
            "0.018843386975347168\n",
            "0.018843386975347168\n",
            "0.018843386975347168\n",
            "0\n",
            "1.3630227968785156e-07\n",
            "1.3630227968785156e-07\n",
            "1.3630227968785156e-07\n",
            "1.3630227968785156e-07\n",
            "0.0002514527223493719\n",
            "0.0002514527223493719\n",
            "0.0021572323469374764\n",
            "0.014681191499749925\n",
            "0.014681191499749925\n",
            "0\n",
            "0\n",
            "0\n",
            "1.1305255337546365e-06\n",
            "1.1305255337546365e-06\n",
            "1.1305255337546365e-06\n",
            "1.1305255337546365e-06\n",
            "1.1305255337546365e-06\n",
            "2.726780816773354e-06\n",
            "2.726780816773354e-06\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "7.766370077161971e-07\n",
            "7.766370077161971e-07\n",
            "7.766370077161971e-07\n",
            "7.766370077161971e-07\n",
            "7.766370077161971e-07\n",
            "7.766370077161971e-07\n",
            "0\n",
            "0\n",
            "0\n",
            "3.409679265089009e-06\n",
            "3.409679265089009e-06\n",
            "3.409679265089009e-06\n",
            "3.409679265089009e-06\n",
            "3.409679265089009e-06\n",
            "6.757104020616203e-06\n",
            "6.757104020616203e-06\n",
            "0\n",
            "0\n",
            "4.9278422667604835e-08\n",
            "6.287435976397184e-07\n",
            "6.287435976397184e-07\n",
            "6.287435976397184e-07\n",
            "6.287435976397184e-07\n",
            "6.287435976397184e-07\n",
            "1.4246843079910006e-05\n",
            "1.4246843079910006e-05\n",
            "0\n",
            "0\n",
            "5.608986883702919e-06\n",
            "1.4260508134325524e-05\n",
            "1.4260508134325524e-05\n",
            "1.4260508134325524e-05\n",
            "1.4260508134325524e-05\n",
            "1.4260508134325524e-05\n",
            "4.035593162224278e-05\n",
            "4.035593162224278e-05\n",
            "0\n",
            "0\n",
            "3.835216936241709e-08\n",
            "3.835216936241709e-08\n",
            "3.835216936241709e-08\n",
            "2.542230425717329e-07\n",
            "2.542230425717329e-07\n",
            "2.542230425717329e-07\n",
            "3.001837528051485e-07\n",
            "6.706221232435868e-06\n",
            "0\n",
            "2.5901858136155117e-08\n",
            "2.5901858136155117e-08\n",
            "2.5901858136155117e-08\n",
            "9.675494104752696e-08\n",
            "9.675494104752696e-08\n",
            "9.675494104752696e-08\n",
            "9.675494104752696e-08\n",
            "5.004112279150799e-07\n",
            "0.00012204031437595851\n",
            "0\n",
            "3.686187416789023e-11\n",
            "3.686187416789023e-11\n",
            "3.686187416789023e-11\n",
            "1.0567738290115533e-10\n",
            "1.4166571741122967e-09\n",
            "1.4166571741122967e-09\n",
            "1.4166571741122967e-09\n",
            "1.4166571741122967e-09\n",
            "3.452588122147539e-07\n",
            "0\n",
            "1.0004308421233299e-07\n",
            "5.657974287988119e-05\n",
            "5.657974287988119e-05\n",
            "5.657974287988119e-05\n",
            "5.657974287988119e-05\n",
            "5.657974287988119e-05\n",
            "5.657974287988119e-05\n",
            "5.657974287988119e-05\n",
            "5.657974287988119e-05\n",
            "0\n",
            "0\n",
            "4.239431228829218e-07\n",
            "4.239431228829218e-07\n",
            "4.239431228829218e-07\n",
            "2.9063013635212217e-06\n",
            "2.9063013635212217e-06\n",
            "2.9063013635212217e-06\n",
            "0.00021912834710117397\n",
            "0.00021912834710117397\n",
            "0\n",
            "0\n",
            "2.9278102165200774e-07\n",
            "2.9278102165200774e-07\n",
            "2.9278102165200774e-07\n",
            "0.009349899063298929\n",
            "0.009349899063298929\n",
            "0.009349899063298929\n",
            "0.009349899063298929\n",
            "0.009349899063298929\n",
            "0\n",
            "0\n",
            "3.391507835114689e-08\n",
            "3.391507835114689e-08\n",
            "1.2495267868153643e-05\n",
            "2.993185260074653e-05\n",
            "2.993185260074653e-05\n",
            "5.198608696985678e-05\n",
            "5.198608696985678e-05\n",
            "5.198608696985678e-05\n",
            "0\n",
            "0\n",
            "0\n",
            "0.00018335166482996848\n",
            "0.00018335166482996848\n",
            "0.00018335166482996848\n",
            "0.00018335166482996848\n",
            "0.00018335166482996848\n",
            "0.00018335166482996848\n",
            "0.00018335166482996848\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "8.216888557346194e-07\n",
            "8.216888557346194e-07\n",
            "8.216888557346194e-07\n",
            "8.216888557346194e-07\n",
            "8.216888557346194e-07\n",
            "1.6640189681566068e-05\n",
            "0\n",
            "2.2417067644582398e-08\n",
            "2.2417067644582398e-08\n",
            "3.0112224736032796e-07\n",
            "3.0112224736032796e-07\n",
            "3.0112224736032796e-07\n",
            "3.0112224736032796e-07\n",
            "3.0112224736032796e-07\n",
            "3.0112224736032796e-07\n",
            "6.899114226805665e-07\n",
            "0\n",
            "1.1845312916772659e-07\n",
            "4.2636664597953376e-07\n",
            "2.206377711001557e-05\n",
            "2.206377711001557e-05\n",
            "2.206377711001557e-05\n",
            "0.00030752724922616036\n",
            "0.00030752724922616036\n",
            "0.00030752724922616036\n",
            "0.00030752724922616036\n",
            "0\n",
            "5.745738335419569e-06\n",
            "5.745738335419569e-06\n",
            "5.745738335419569e-06\n",
            "5.745738335419569e-06\n",
            "5.745738335419569e-06\n",
            "5.745738335419569e-06\n",
            "5.745738335419569e-06\n",
            "0.00023670718814144105\n",
            "0.00023670718814144105\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1.8303890070934023e-07\n",
            "1.8303890070934023e-07\n",
            "1.8303890070934023e-07\n",
            "2.1154279694573445e-07\n",
            "2.1154279694573445e-07\n",
            "0\n",
            "0\n",
            "0\n",
            "0.0017606348292628939\n",
            "0.0017606348292628939\n",
            "0.0017606348292628939\n",
            "0.028313874432921306\n",
            "0.028313874432921306\n",
            "0.028313874432921306\n",
            "0.028313874432921306\n",
            "0\n",
            "0\n",
            "3.529537471275143e-06\n",
            "5.804437602873986e-05\n",
            "0.00013004832707607191\n",
            "0.00048664032990610004\n",
            "0.001010511015134564\n",
            "0.001010511015134564\n",
            "0.0017723817068397766\n",
            "0.0017723817068397766\n",
            "0\n",
            "0\n",
            "2.020986602962148e-09\n",
            "2.020986602962148e-09\n",
            "1.3331085071996073e-07\n",
            "1.874605065421824e-05\n",
            "1.874605065421824e-05\n",
            "1.874605065421824e-05\n",
            "1.874605065421824e-05\n",
            "0.00012695880419544834\n",
            "0\n",
            "1.5092849008386046e-07\n",
            "1.5092849008386046e-07\n",
            "1.5092849008386046e-07\n",
            "2.1164719279377344e-06\n",
            "2.1164719279377344e-06\n",
            "2.1164719279377344e-06\n",
            "2.1164719279377344e-06\n",
            "2.1164719279377344e-06\n",
            "0.001139450324389794\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "3.457345585129147e-05\n",
            "3.457345585129147e-05\n",
            "0.001511208175927127\n",
            "0.001511208175927127\n",
            "0.001511208175927127\n",
            "0\n",
            "0\n",
            "0.00022695455900970405\n",
            "0.00022695455900970405\n",
            "0.00022695455900970405\n",
            "0.06981220338376429\n",
            "0.06981220338376429\n",
            "0.06981220338376429\n",
            "0.06981220338376429\n",
            "0.06981220338376429\n",
            "0\n",
            "1.5931766376554037e-08\n",
            "0.00016611453876128014\n",
            "0.00016611453876128014\n",
            "0.00016611453876128014\n",
            "0.00016611453876128014\n",
            "0.00016611453876128014\n",
            "0.00016611453876128014\n",
            "0.00016611453876128014\n",
            "0.00016611453876128014\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "2.7248050248128853e-05\n",
            "2.7248050248128853e-05\n",
            "2.7248050248128853e-05\n",
            "2.7248050248128853e-05\n",
            "2.7248050248128853e-05\n",
            "4.883602731393336e-05\n",
            "0\n",
            "1.640911389451713e-05\n",
            "1.640911389451713e-05\n",
            "1.640911389451713e-05\n",
            "1.640911389451713e-05\n",
            "0.0006515263388699342\n",
            "0.0006515263388699342\n",
            "0.1260499794812333\n",
            "0.1260499794812333\n",
            "0.1260499794812333\n",
            "0\n",
            "7.044916067625079e-10\n",
            "2.3870736978077613e-06\n",
            "2.3870736978077613e-06\n",
            "2.3870736978077613e-06\n",
            "2.5385225624438785e-06\n",
            "3.880321350763083e-06\n",
            "3.880321350763083e-06\n",
            "3.880321350763083e-06\n",
            "3.880321350763083e-06\n",
            "0\n",
            "0\n",
            "0\n",
            "6.576420809282439e-09\n",
            "6.576420809282439e-09\n",
            "1.774165591821069e-08\n",
            "1.774165591821069e-08\n",
            "1.774165591821069e-08\n",
            "9.352331653409631e-08\n",
            "9.352331653409631e-08\n",
            "0\n",
            "2.6395450376862376e-09\n",
            "2.7452782473158965e-08\n",
            "2.7452782473158965e-08\n",
            "1.9470987673027112e-07\n",
            "1.9470987673027112e-07\n",
            "1.9470987673027112e-07\n",
            "1.9470987673027112e-07\n",
            "1.9470987673027112e-07\n",
            "1.9470987673027112e-07\n",
            "0\n",
            "7.239238457847172e-07\n",
            "7.239238457847172e-07\n",
            "7.239238457847172e-07\n",
            "7.239238457847172e-07\n",
            "0.00023398787852494038\n",
            "0.00023398787852494038\n",
            "0.00023398787852494038\n",
            "0.00023398787852494038\n",
            "0.00023398787852494038\n",
            "0\n",
            "3.1383002215604545e-08\n",
            "0.00011994413889090603\n",
            "0.00011994413889090603\n",
            "0.00011994413889090603\n",
            "0.00011994413889090603\n",
            "0.00011994413889090603\n",
            "0.0034071286630201897\n",
            "0.0034071286630201897\n",
            "0.0034071286630201897\n",
            "0\n",
            "0\n",
            "0\n",
            "1.6353236123669897e-06\n",
            "1.6353236123669897e-06\n",
            "1.6353236123669897e-06\n",
            "1.6353236123669897e-06\n",
            "0.035364049729862575\n",
            "0.035364049729862575\n",
            "0.035364049729862575\n",
            "0\n",
            "0\n",
            "8.352516659397714e-10\n",
            "1.1649449290704048e-07\n",
            "1.1649449290704048e-07\n",
            "1.1649449290704048e-07\n",
            "1.1649449290704048e-07\n",
            "1.1649449290704048e-07\n",
            "1.1649449290704048e-07\n",
            "1.1649449290704048e-07\n",
            "0\n",
            "0\n",
            "0\n",
            "3.320537103749355e-05\n",
            "3.320537103749355e-05\n",
            "3.320537103749355e-05\n",
            "3.320537103749355e-05\n",
            "4.863726136374324e-05\n",
            "4.863726136374324e-05\n",
            "4.863726136374324e-05\n",
            "0\n",
            "0\n",
            "6.341124848636216e-09\n",
            "6.341124848636216e-09\n",
            "6.341124848636216e-09\n",
            "6.341124848636216e-09\n",
            "6.341124848636216e-09\n",
            "6.341124848636216e-09\n",
            "1.2236462046979073e-07\n",
            "1.2236462046979073e-07\n",
            "0\n",
            "0\n",
            "0\n",
            "8.154044024330535e-08\n",
            "8.154044024330535e-08\n",
            "8.154044024330535e-08\n",
            "8.154044024330535e-08\n",
            "8.154044024330535e-08\n",
            "2.913864497118483e-06\n",
            "2.913864497118483e-06\n",
            "0\n",
            "0\n",
            "1.9096454005873733e-07\n",
            "1.9145377333819062e-07\n",
            "1.9145377333819062e-07\n",
            "1.9145377333819062e-07\n",
            "1.9145377333819062e-07\n",
            "1.9145377333819062e-07\n",
            "1.9145377333819062e-07\n",
            "1.9145377333819062e-07\n",
            "0\n",
            "0\n",
            "0\n",
            "8.160609719440306e-06\n",
            "8.160609719440306e-06\n",
            "7.462086796311134e-05\n",
            "7.462086796311134e-05\n",
            "7.462086796311134e-05\n",
            "7.462086796311134e-05\n",
            "7.462086796311134e-05\n",
            "0\n",
            "0\n",
            "1.5237292094107502e-05\n",
            "1.615005085589294e-05\n",
            "1.615005085589294e-05\n",
            "1.615005085589294e-05\n",
            "1.615005085589294e-05\n",
            "1.615005085589294e-05\n",
            "8.275809571598556e-05\n",
            "8.275809571598556e-05\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1.8066107095178622e-07\n",
            "1.8066107095178622e-07\n",
            "1.109825828670893e-05\n",
            "1.109825828670893e-05\n",
            "4.168807702435392e-05\n",
            "0\n",
            "0\n",
            "2.902889980837595e-07\n",
            "2.902889980837595e-07\n",
            "2.902889980837595e-07\n",
            "2.902889980837595e-07\n",
            "2.902889980837595e-07\n",
            "3.7931610577803393e-06\n",
            "3.7931610577803393e-06\n",
            "3.7931610577803393e-06\n",
            "0\n",
            "0\n",
            "0\n",
            "6.935179817409803e-06\n",
            "6.935179817409803e-06\n",
            "7.693879629384645e-06\n",
            "7.693879629384645e-06\n",
            "7.693879629384645e-06\n",
            "7.693879629384645e-06\n",
            "8.604998208988149e-06\n",
            "0\n",
            "0\n",
            "3.055209738017399e-08\n",
            "3.055209738017399e-08\n",
            "3.055209738017399e-08\n",
            "3.055209738017399e-08\n",
            "3.055209738017399e-08\n",
            "3.055209738017399e-08\n",
            "3.055209738017399e-08\n",
            "3.055209738017399e-08\n",
            "0\n",
            "2.5359751315821e-07\n",
            "2.5359751315821e-07\n",
            "2.5359751315821e-07\n",
            "2.5359751315821e-07\n",
            "0.0008011695841067699\n",
            "0.0008011695841067699\n",
            "0.0008011695841067699\n",
            "0.0008011695841067699\n",
            "0.0008011695841067699\n",
            "0\n",
            "0\n",
            "2.1771372094571356e-07\n",
            "7.42824548200686e-05\n",
            "7.42824548200686e-05\n",
            "7.42824548200686e-05\n",
            "7.42824548200686e-05\n",
            "7.42824548200686e-05\n",
            "0.0031401486450806713\n",
            "0.0031401486450806713\n",
            "0\n",
            "1.839182194808957e-07\n",
            "1.839182194808957e-07\n",
            "6.960175438152646e-05\n",
            "6.960175438152646e-05\n",
            "6.960175438152646e-05\n",
            "6.960175438152646e-05\n",
            "6.960175438152646e-05\n",
            "0.0009887083924009043\n",
            "0.0009887083924009043\n",
            "0\n",
            "1.646774328480278e-07\n",
            "1.646774328480278e-07\n",
            "1.646774328480278e-07\n",
            "1.646774328480278e-07\n",
            "7.575811475444397e-07\n",
            "7.575811475444397e-07\n",
            "7.575811475444397e-07\n",
            "7.575811475444397e-07\n",
            "7.575811475444397e-07\n",
            "0\n",
            "2.8113392259299835e-08\n",
            "2.8113392259299835e-08\n",
            "2.8113392259299835e-08\n",
            "6.370394683544768e-08\n",
            "6.370394683544768e-08\n",
            "5.1181958957408487e-05\n",
            "5.1181958957408487e-05\n",
            "5.1181958957408487e-05\n",
            "5.1181958957408487e-05\n",
            "0\n",
            "2.8809725474529542e-08\n",
            "2.8809725474529542e-08\n",
            "1.3805133810090443e-06\n",
            "1.3805133810090443e-06\n",
            "0.00030249035021326766\n",
            "0.00030249035021326766\n",
            "0.00030249035021326766\n",
            "0.00030249035021326766\n",
            "0.00030249035021326766\n",
            "0\n",
            "0\n",
            "1.0827141220239116e-05\n",
            "1.0827141220239116e-05\n",
            "1.0827141220239116e-05\n",
            "1.0827141220239116e-05\n",
            "1.0827141220239116e-05\n",
            "1.0827141220239116e-05\n",
            "1.0827141220239116e-05\n",
            "1.0827141220239116e-05\n",
            "0\n",
            "0\n",
            "0\n",
            "9.128323366381467e-06\n",
            "9.128323366381467e-06\n",
            "9.128323366381467e-06\n",
            "9.128323366381467e-06\n",
            "9.128323366381467e-06\n",
            "9.128323366381467e-06\n",
            "9.128323366381467e-06\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "8.513291186245717e-12\n",
            "2.820684131481653e-07\n",
            "2.820684131481653e-07\n",
            "2.6262506697981467e-06\n",
            "4.0511887402446855e-06\n",
            "4.0511887402446855e-06\n",
            "0\n",
            "0\n",
            "0\n",
            "5.144468723907976e-08\n",
            "5.144468723907976e-08\n",
            "5.144468723907976e-08\n",
            "5.273805831513642e-07\n",
            "5.273805831513642e-07\n",
            "4.280099113655452e-06\n",
            "4.280099113655452e-06\n",
            "0\n",
            "4.1375367939116925e-07\n",
            "4.1375367939116925e-07\n",
            "0.00630267723465239\n",
            "0.00630267723465239\n",
            "0.00630267723465239\n",
            "0.00630267723465239\n",
            "0.00630267723465239\n",
            "0.00630267723465239\n",
            "0.00630267723465239\n",
            "0\n",
            "9.787465777306764e-08\n",
            "9.787465777306764e-08\n",
            "1.0152025465031995e-07\n",
            "1.0152025465031995e-07\n",
            "9.969363300782368e-05\n",
            "0.017784777709850076\n",
            "0.017784777709850076\n",
            "0.017784777709850076\n",
            "0.017784777709850076\n",
            "0\n",
            "0\n",
            "1.1495818853782687e-05\n",
            "0.0004191734596080993\n",
            "0.0004191734596080993\n",
            "0.0004191734596080993\n",
            "0.0004191734596080993\n",
            "0.0004191734596080993\n",
            "0.0016514565168874137\n",
            "0.0016514565168874137\n",
            "0\n",
            "0\n",
            "1.964622170881964e-06\n",
            "1.964622170881964e-06\n",
            "1.964622170881964e-06\n",
            "1.964622170881964e-06\n",
            "1.964622170881964e-06\n",
            "1.964622170881964e-06\n",
            "1.964622170881964e-06\n",
            "2.332550218541863e-06\n",
            "0\n",
            "0\n",
            "0\n",
            "6.722134235681638e-05\n",
            "6.722134235681638e-05\n",
            "6.722134235681638e-05\n",
            "6.722134235681638e-05\n",
            "6.722134235681638e-05\n",
            "6.722134235681638e-05\n",
            "6.722134235681638e-05\n",
            "0\n",
            "0\n",
            "1.2300637780391577e-07\n",
            "0.002065231638332624\n",
            "0.002065231638332624\n",
            "0.002065231638332624\n",
            "0.002065231638332624\n",
            "0.002065231638332624\n",
            "0.002065231638332624\n",
            "0.002065231638332624\n",
            "0\n",
            "0\n",
            "4.5944369382658676e-05\n",
            "4.5944369382658676e-05\n",
            "0.00020154473695138054\n",
            "0.0014150208055549488\n",
            "0.0014150208055549488\n",
            "0.0014150208055549488\n",
            "0.0014150208055549488\n",
            "0.0014150208055549488\n",
            "0\n",
            "0\n",
            "0.0008282056834010638\n",
            "0.0008282056834010638\n",
            "0.0008282056834010638\n",
            "0.0008282056834010638\n",
            "0.03361105545493362\n",
            "0.03361105545493362\n",
            "0.03361105545493362\n",
            "0.03361105545493362\n",
            "0\n",
            "2.767976345820906e-08\n",
            "7.5473568882727855e-06\n",
            "7.5473568882727855e-06\n",
            "7.5473568882727855e-06\n",
            "0.0002478339686497145\n",
            "0.0004062119868476877\n",
            "0.0004062119868476877\n",
            "0.0005268600482685023\n",
            "0.01798428994748401\n",
            "0\n",
            "1.173919673237639e-10\n",
            "1.173919673237639e-10\n",
            "3.1011630007273036e-08\n",
            "3.1011630007273036e-08\n",
            "3.1011630007273036e-08\n",
            "3.60968761148193e-08\n",
            "1.7659741414719924e-06\n",
            "1.7659741414719924e-06\n",
            "1.7659741414719924e-06\n",
            "0\n",
            "0\n",
            "8.556896044564546e-08\n",
            "8.556896044564546e-08\n",
            "8.556896044564546e-08\n",
            "8.556896044564546e-08\n",
            "5.350580066843743e-07\n",
            "5.350580066843743e-07\n",
            "5.090539232952305e-05\n",
            "5.090539232952305e-05\n",
            "0\n",
            "0.0001272577350004882\n",
            "0.0001272577350004882\n",
            "0.0001272577350004882\n",
            "0.0002746884891168006\n",
            "0.004061715235928187\n",
            "0.004061715235928187\n",
            "0.004061715235928187\n",
            "0.004061715235928187\n",
            "0.004061715235928187\n",
            "0\n",
            "0\n",
            "6.915577153746351e-05\n",
            "6.915577153746351e-05\n",
            "6.915577153746351e-05\n",
            "6.915577153746351e-05\n",
            "6.915577153746351e-05\n",
            "6.915577153746351e-05\n",
            "6.915577153746351e-05\n",
            "6.915577153746351e-05\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1.1664615279766189e-06\n",
            "1.4019232321388516e-06\n",
            "1.4019232321388516e-06\n",
            "1.7541649964552162e-06\n",
            "1.7541649964552162e-06\n",
            "6.158356399033876e-06\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1.0841243902449574e-10\n",
            "2.388521361286857e-09\n",
            "2.388521361286857e-09\n",
            "2.388521361286857e-09\n",
            "1.2146461783256045e-05\n",
            "1.2146461783256045e-05\n",
            "0\n",
            "0\n",
            "0.0006373941707534391\n",
            "0.0006373941707534391\n",
            "0.0006373941707534391\n",
            "0.0006373941707534391\n",
            "0.0006373941707534391\n",
            "0.0027907046289367\n",
            "0.0027907046289367\n",
            "0.0052091153816432355\n",
            "0\n",
            "4.294000120712651e-06\n",
            "4.294000120712651e-06\n",
            "4.294000120712651e-06\n",
            "4.0375017043432765e-05\n",
            "4.0375017043432765e-05\n",
            "4.0375017043432765e-05\n",
            "4.0375017043432765e-05\n",
            "4.0375017043432765e-05\n",
            "4.0375017043432765e-05\n",
            "0\n",
            "0\n",
            "1.9357456320571575e-09\n",
            "1.9357456320571575e-09\n",
            "1.9357456320571575e-09\n",
            "5.974389341003258e-07\n",
            "5.974389341003258e-07\n",
            "5.974389341003258e-07\n",
            "5.974389341003258e-07\n",
            "5.974389341003258e-07\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "3.0590458594918988e-09\n",
            "3.0590458594918988e-09\n",
            "3.0590458594918988e-09\n",
            "3.0590458594918988e-09\n",
            "4.861613527631652e-09\n",
            "1.9434594071515485e-06\n",
            "0\n",
            "0\n",
            "3.785410848237306e-07\n",
            "4.219930726312639e-06\n",
            "4.219930726312639e-06\n",
            "4.219930726312639e-06\n",
            "4.219930726312639e-06\n",
            "6.799619968924106e-06\n",
            "6.799619968924106e-06\n",
            "6.799619968924106e-06\n",
            "0\n",
            "0\n",
            "4.136774190089505e-07\n",
            "4.136774190089505e-07\n",
            "4.136774190089505e-07\n",
            "3.4047391787939293e-06\n",
            "3.4047391787939293e-06\n",
            "0.00011529898979857195\n",
            "0.002144181847236142\n",
            "0.002144181847236142\n",
            "0\n",
            "0\n",
            "0\n",
            "0.0002531165797267362\n",
            "0.0002531165797267362\n",
            "0.0002531165797267362\n",
            "0.0005381111187780616\n",
            "0.0005381111187780616\n",
            "0.0005381111187780616\n",
            "0.0005381111187780616\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1.4670295684592754e-09\n",
            "4.69433151622177e-09\n",
            "4.69433151622177e-09\n",
            "4.69433151622177e-09\n",
            "1.934957101828148e-08\n",
            "1.2996662251434267e-07\n",
            "0\n",
            "0\n",
            "7.78156053841386e-10\n",
            "7.78156053841386e-10\n",
            "7.78156053841386e-10\n",
            "2.68186910019123e-05\n",
            "2.68186910019123e-05\n",
            "2.68186910019123e-05\n",
            "2.68186910019123e-05\n",
            "2.68186910019123e-05\n",
            "0\n",
            "0\n",
            "5.479095441007521e-07\n",
            "2.1008872853410656e-05\n",
            "2.1008872853410656e-05\n",
            "2.1008872853410656e-05\n",
            "2.1008872853410656e-05\n",
            "2.1008872853410656e-05\n",
            "7.149955724628398e-05\n",
            "7.149955724628398e-05\n",
            "0\n",
            "2.468376151781223e-09\n",
            "1.4523502299917193e-06\n",
            "1.4523502299917193e-06\n",
            "1.4523502299917193e-06\n",
            "3.7527143247727676e-06\n",
            "3.7527143247727676e-06\n",
            "3.7527143247727676e-06\n",
            "3.7527143247727676e-06\n",
            "3.7527143247727676e-06\n",
            "0\n",
            "0\n",
            "1.6508908140690687e-08\n",
            "1.6508908140690687e-08\n",
            "1.6508908140690687e-08\n",
            "1.6508908140690687e-08\n",
            "1.6508908140690687e-08\n",
            "1.6508908140690687e-08\n",
            "1.6508908140690687e-08\n",
            "1.2075685065397393e-07\n",
            "0\n",
            "1.0619300174487258e-09\n",
            "6.23914974384093e-05\n",
            "6.23914974384093e-05\n",
            "6.23914974384093e-05\n",
            "6.23914974384093e-05\n",
            "6.23914974384093e-05\n",
            "6.23914974384093e-05\n",
            "6.23914974384093e-05\n",
            "8.827207816915832e-05\n",
            "0\n",
            "1.5234400064445666e-08\n",
            "7.632125826793492e-07\n",
            "0.0007855689692162359\n",
            "0.0007855689692162359\n",
            "0.0007855689692162359\n",
            "0.0007855689692162359\n",
            "0.0007855689692162359\n",
            "0.0007855689692162359\n",
            "0.0007855689692162359\n",
            "0\n",
            "0\n",
            "5.768100599105812e-07\n",
            "5.768100599105812e-07\n",
            "7.115726652131495e-07\n",
            "7.115726652131495e-07\n",
            "1.8699147035609388e-05\n",
            "1.8699147035609388e-05\n",
            "1.8699147035609388e-05\n",
            "1.8699147035609388e-05\n",
            "0\n",
            "0\n",
            "0\n",
            "0.00022395754897032094\n",
            "0.00022395754897032094\n",
            "0.00022395754897032094\n",
            "0.00022395754897032094\n",
            "0.00022395754897032094\n",
            "0.00022395754897032094\n",
            "0.00022395754897032094\n",
            "0\n",
            "0\n",
            "0\n",
            "4.396094945005588e-06\n",
            "4.396094945005588e-06\n",
            "0.0002571906341199426\n",
            "0.0002571906341199426\n",
            "0.0002571906341199426\n",
            "0.0002571906341199426\n",
            "0.0002571906341199426\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1.4569183021233356e-08\n",
            "1.4569183021233356e-08\n",
            "1.4569183021233356e-08\n",
            "1.4569183021233356e-08\n",
            "1.4569183021233356e-08\n",
            "2.268307664374861e-06\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1.3282261135060397e-11\n",
            "1.3536119668470956e-09\n",
            "1.3536119668470956e-09\n",
            "1.3536119668470956e-09\n",
            "1.3536119668470956e-09\n",
            "1.3536119668470956e-09\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "8.866184929791368e-08\n",
            "8.866184929791368e-08\n",
            "3.902381061974907e-07\n",
            "3.902381061974907e-07\n",
            "2.596914006454568e-05\n",
            "2.596914006454568e-05\n",
            "0\n",
            "0\n",
            "3.5692379556257204e-09\n",
            "1.5450342478226993e-08\n",
            "1.5450342478226993e-08\n",
            "1.5450342478226993e-08\n",
            "1.5450342478226993e-08\n",
            "1.5450342478226993e-08\n",
            "1.5450342478226993e-08\n",
            "2.668552548773422e-07\n",
            "0\n",
            "0\n",
            "3.30863118590073e-05\n",
            "5.32190986300163e-05\n",
            "5.32190986300163e-05\n",
            "0.00014148035769707065\n",
            "0.00014148035769707065\n",
            "0.0016238277861747332\n",
            "0.0016238277861747332\n",
            "0.0016238277861747332\n",
            "0\n",
            "5.400524575642468e-08\n",
            "3.3132638560492387e-06\n",
            "3.3132638560492387e-06\n",
            "3.3132638560492387e-06\n",
            "4.024894355060907e-06\n",
            "0.00012077447629017278\n",
            "0.00012077447629017278\n",
            "0.00012077447629017278\n",
            "0.00012077447629017278\n",
            "0\n",
            "1.4681306097400487e-07\n",
            "1.4681306097400487e-07\n",
            "1.4681306097400487e-07\n",
            "1.8313601157749603e-05\n",
            "1.8313601157749603e-05\n",
            "3.367070076922636e-05\n",
            "3.367070076922636e-05\n",
            "0.0001247462122490125\n",
            "0.006311799913660114\n",
            "0\n",
            "0\n",
            "0.00023524676691712374\n",
            "0.00023524676691712374\n",
            "0.00023524676691712374\n",
            "0.00023524676691712374\n",
            "0.00023524676691712374\n",
            "0.0002579856635645407\n",
            "0.0002579856635645407\n",
            "0.0002579856635645407\n",
            "0\n",
            "0\n",
            "1.2665578897821063e-07\n",
            "2.112327159392568e-05\n",
            "2.112327159392568e-05\n",
            "2.112327159392568e-05\n",
            "2.112327159392568e-05\n",
            "2.112327159392568e-05\n",
            "2.112327159392568e-05\n",
            "2.112327159392568e-05\n",
            "0\n",
            "2.2902269305674337e-11\n",
            "2.2902269305674337e-11\n",
            "2.6737840691292278e-09\n",
            "2.6737840691292278e-09\n",
            "2.6737840691292278e-09\n",
            "2.6737840691292278e-09\n",
            "2.6737840691292278e-09\n",
            "2.6737840691292278e-09\n",
            "3.0341540211189014e-07\n",
            "0\n",
            "0\n",
            "7.398424256084111e-07\n",
            "1.6936693721856478e-05\n",
            "1.6936693721856478e-05\n",
            "1.9697137926965908e-05\n",
            "1.9697137926965908e-05\n",
            "5.493794254924653e-05\n",
            "5.493794254924653e-05\n",
            "5.493794254924653e-05\n",
            "0\n",
            "1.3534155167235504e-09\n",
            "1.3534155167235504e-09\n",
            "7.237786017087288e-06\n",
            "7.237786017087288e-06\n",
            "3.619870989048077e-05\n",
            "3.619870989048077e-05\n",
            "3.619870989048077e-05\n",
            "3.619870989048077e-05\n",
            "3.619870989048077e-05\n",
            "0\n",
            "1.603319596499891e-09\n",
            "4.518768392946715e-08\n",
            "4.518768392946715e-08\n",
            "4.518768392946715e-08\n",
            "4.518768392946715e-08\n",
            "4.518768392946715e-08\n",
            "4.518768392946715e-08\n",
            "4.518768392946715e-08\n",
            "1.2018789382076107e-06\n",
            "0\n",
            "0\n",
            "0.0004775763858709468\n",
            "0.0004775763858709468\n",
            "0.0004775763858709468\n",
            "0.0004775763858709468\n",
            "0.0004775763858709468\n",
            "0.0004775763858709468\n",
            "0.0004775763858709468\n",
            "0.0004775763858709468\n",
            "0\n",
            "0\n",
            "2.4029370234614996e-06\n",
            "4.563393709846304e-06\n",
            "4.563393709846304e-06\n",
            "4.563393709846304e-06\n",
            "4.563393709846304e-06\n",
            "0.00045241687498128374\n",
            "0.00045241687498128374\n",
            "0.00045241687498128374\n",
            "0\n",
            "0\n",
            "0\n",
            "2.673535786881301e-07\n",
            "2.673535786881301e-07\n",
            "2.673535786881301e-07\n",
            "4.4712343949319666e-07\n",
            "3.3419822005909386e-06\n",
            "3.3419822005909386e-06\n",
            "3.3419822005909386e-06\n",
            "0\n",
            "2.866063228899427e-10\n",
            "2.866063228899427e-10\n",
            "8.718915190781594e-08\n",
            "8.718915190781594e-08\n",
            "8.718915190781594e-08\n",
            "8.718915190781594e-08\n",
            "8.718915190781594e-08\n",
            "1.4201691171648216e-07\n",
            "1.4201691171648216e-07\n",
            "0\n",
            "6.524338180120565e-08\n",
            "1.2620101565656722e-07\n",
            "5.685385672591274e-07\n",
            "5.685385672591274e-07\n",
            "1.1372299939355347e-06\n",
            "1.1372299939355347e-06\n",
            "1.1372299939355347e-06\n",
            "1.1372299939355347e-06\n",
            "3.89876824013292e-05\n",
            "0\n",
            "9.60444144318862e-10\n",
            "2.202514203891697e-09\n",
            "2.202514203891697e-09\n",
            "2.202514203891697e-09\n",
            "1.7008422145770018e-05\n",
            "1.7008422145770018e-05\n",
            "1.7008422145770018e-05\n",
            "1.7008422145770018e-05\n",
            "1.7008422145770018e-05\n",
            "0\n",
            "3.1231579407580727e-09\n",
            "3.9984569579391426e-05\n",
            "3.9984569579391426e-05\n",
            "3.9984569579391426e-05\n",
            "3.9984569579391426e-05\n",
            "3.9984569579391426e-05\n",
            "3.9984569579391426e-05\n",
            "3.9984569579391426e-05\n",
            "8.346716583130803e-05\n",
            "0\n",
            "0.0001738572759456781\n",
            "0.0001738572759456781\n",
            "0.000678765769853987\n",
            "0.000678765769853987\n",
            "0.0016922095135209962\n",
            "0.0016922095135209962\n",
            "0.0036971884308140152\n",
            "0.0036971884308140152\n",
            "0.0036971884308140152\n",
            "0\n",
            "0\n",
            "3.133549765009398e-08\n",
            "3.133549765009398e-08\n",
            "3.133549765009398e-08\n",
            "4.1069269438445186e-07\n",
            "4.1069269438445186e-07\n",
            "4.1069269438445186e-07\n",
            "4.1069269438445186e-07\n",
            "4.1069269438445186e-07\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1.6891984317297857e-05\n",
            "1.6891984317297857e-05\n",
            "1.6891984317297857e-05\n",
            "1.6891984317297857e-05\n",
            "1.6891984317297857e-05\n",
            "0\n",
            "0\n",
            "2.435800884324716e-07\n",
            "2.435800884324716e-07\n",
            "2.435800884324716e-07\n",
            "1.2775497698032869e-05\n",
            "1.2775497698032869e-05\n",
            "1.2775497698032869e-05\n",
            "1.2775497698032869e-05\n",
            "1.2775497698032869e-05\n",
            "0\n",
            "0\n",
            "5.434878287133443e-07\n",
            "5.434878287133443e-07\n",
            "5.434878287133443e-07\n",
            "5.434878287133443e-07\n",
            "5.434878287133443e-07\n",
            "5.434878287133443e-07\n",
            "1.077986293520253e-05\n",
            "0.000494372016892862\n",
            "0\n",
            "0\n",
            "3.848179849917464e-13\n",
            "3.848179849917464e-13\n",
            "3.848179849917464e-13\n",
            "7.462267214547556e-07\n",
            "7.462267214547556e-07\n",
            "7.462267214547556e-07\n",
            "7.462267214547556e-07\n",
            "7.462267214547556e-07\n",
            "0\n",
            "1.3241138544522687e-09\n",
            "2.0283386412648693e-06\n",
            "2.0283386412648693e-06\n",
            "2.0283386412648693e-06\n",
            "0.000701560216673037\n",
            "0.000701560216673037\n",
            "0.000701560216673037\n",
            "0.000701560216673037\n",
            "0.000701560216673037\n",
            "0\n",
            "5.344624077775949e-09\n",
            "3.030491528524618e-07\n",
            "3.030491528524618e-07\n",
            "3.030491528524618e-07\n",
            "5.746044640973956e-07\n",
            "5.746044640973956e-07\n",
            "5.746044640973956e-07\n",
            "9.266810459483427e-06\n",
            "9.266810459483427e-06\n",
            "0\n",
            "3.2826022227133168e-09\n",
            "1.3293115409512237e-07\n",
            "1.3293115409512237e-07\n",
            "1.3293115409512237e-07\n",
            "4.124615069255716e-05\n",
            "4.124615069255716e-05\n",
            "4.124615069255716e-05\n",
            "4.124615069255716e-05\n",
            "4.124615069255716e-05\n",
            "0\n",
            "8.03691964337977e-09\n",
            "1.858124693821722e-07\n",
            "1.858124693821722e-07\n",
            "1.858124693821722e-07\n",
            "1.858124693821722e-07\n",
            "1.858124693821722e-07\n",
            "1.858124693821722e-07\n",
            "2.258379462496602e-05\n",
            "2.258379462496602e-05\n",
            "0\n",
            "3.889814404937595e-05\n",
            "0.0002894414015254627\n",
            "0.0002894414015254627\n",
            "0.0002894414015254627\n",
            "0.0007821402978960306\n",
            "0.0007821402978960306\n",
            "0.0007821402978960306\n",
            "0.0007821402978960306\n",
            "0.0007821402978960306\n",
            "0\n",
            "0\n",
            "8.198843663719282e-07\n",
            "3.8659749581017066e-06\n",
            "3.6254831015127936e-05\n",
            "3.6254831015127936e-05\n",
            "3.6254831015127936e-05\n",
            "3.6254831015127936e-05\n",
            "5.3843712633451706e-05\n",
            "0.007524204903260686\n",
            "0\n",
            "0\n",
            "8.082607365150009e-06\n",
            "8.082607365150009e-06\n",
            "8.082607365150009e-06\n",
            "8.082607365150009e-06\n",
            "8.082607365150009e-06\n",
            "8.082607365150009e-06\n",
            "8.082607365150009e-06\n",
            "8.082607365150009e-06\n",
            "0\n",
            "0\n",
            "9.573925584743184e-13\n",
            "9.573925584743184e-13\n",
            "9.573925584743184e-13\n",
            "6.954283950265263e-08\n",
            "6.954283950265263e-08\n",
            "4.448336999250839e-07\n",
            "4.448336999250839e-07\n",
            "4.448336999250839e-07\n",
            "0\n",
            "4.123079546760461e-08\n",
            "4.123079546760461e-08\n",
            "4.123079546760461e-08\n",
            "4.123079546760461e-08\n",
            "4.123079546760461e-08\n",
            "4.123079546760461e-08\n",
            "4.123079546760461e-08\n",
            "4.123079546760461e-08\n",
            "2.7518310360030315e-05\n",
            "0\n",
            "1.5356828104956e-09\n",
            "3.935902457222225e-06\n",
            "3.935902457222225e-06\n",
            "3.935902457222225e-06\n",
            "3.935902457222225e-06\n",
            "6.992027734122986e-06\n",
            "6.992027734122986e-06\n",
            "2.0359347713240172e-05\n",
            "2.0359347713240172e-05\n",
            "0\n",
            "9.466982232801947e-08\n",
            "9.466982232801947e-08\n",
            "1.1713190126311743e-05\n",
            "1.1713190126311743e-05\n",
            "2.4463537299386294e-05\n",
            "2.9976058233116238e-05\n",
            "2.9976058233116238e-05\n",
            "2.9976058233116238e-05\n",
            "8.441594506003922e-05\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "9.53530639577238e-06\n",
            "9.53530639577238e-06\n",
            "9.53530639577238e-06\n",
            "9.53530639577238e-06\n",
            "9.53530639577238e-06\n",
            "9.53530639577238e-06\n",
            "0\n",
            "0\n",
            "1.7295035516531988e-06\n",
            "1.7295035516531988e-06\n",
            "6.1102335171849716e-06\n",
            "6.1102335171849716e-06\n",
            "8.371289169561936e-06\n",
            "8.371289169561936e-06\n",
            "4.489510131624583e-05\n",
            "0.0015848319325357343\n",
            "0\n",
            "0\n",
            "3.5941453181334474e-06\n",
            "9.331898262254357e-06\n",
            "9.331898262254357e-06\n",
            "9.331898262254357e-06\n",
            "9.331898262254357e-06\n",
            "1.0168153560210724e-05\n",
            "1.0168153560210724e-05\n",
            "1.0168153560210724e-05\n",
            "0\n",
            "4.1580315915838915e-07\n",
            "4.1580315915838915e-07\n",
            "0.0004176881752557708\n",
            "0.0004176881752557708\n",
            "0.0004176881752557708\n",
            "0.0004176881752557708\n",
            "0.0004176881752557708\n",
            "0.0004176881752557708\n",
            "0.00047432809443445715\n",
            "0\n",
            "7.619596286954043e-09\n",
            "2.9822377012069066e-05\n",
            "2.9822377012069066e-05\n",
            "2.9822377012069066e-05\n",
            "2.9822377012069066e-05\n",
            "2.9822377012069066e-05\n",
            "2.9822377012069066e-05\n",
            "2.9822377012069066e-05\n",
            "2.9822377012069066e-05\n",
            "0\n",
            "9.661915113368632e-09\n",
            "5.629790260040002e-08\n",
            "5.629790260040002e-08\n",
            "5.629790260040002e-08\n",
            "5.629790260040002e-08\n",
            "5.629790260040002e-08\n",
            "5.629790260040002e-08\n",
            "1.6967872290260513e-06\n",
            "4.513345287228221e-05\n",
            "0\n",
            "0\n",
            "2.4571188659567503e-07\n",
            "2.6536375702921137e-07\n",
            "2.6536375702921137e-07\n",
            "7.0462785185518055e-06\n",
            "7.0462785185518055e-06\n",
            "7.344662435338762e-06\n",
            "7.344662435338762e-06\n",
            "0.0012031319616585534\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1.2657872974950554e-09\n",
            "1.2657872974950554e-09\n",
            "1.2657872974950554e-09\n",
            "1.2657872974950554e-09\n",
            "4.252946063750726e-08\n",
            "4.8481321162053446e-05\n",
            "0\n",
            "2.4371738002710672e-09\n",
            "2.7007953777779613e-06\n",
            "2.7007953777779613e-06\n",
            "2.7007953777779613e-06\n",
            "2.7007953777779613e-06\n",
            "2.7007953777779613e-06\n",
            "2.7007953777779613e-06\n",
            "2.7007953777779613e-06\n",
            "2.7007953777779613e-06\n",
            "0\n",
            "1.953219224581091e-11\n",
            "1.953219224581091e-11\n",
            "6.443096926285792e-09\n",
            "6.443096926285792e-09\n",
            "1.1064614372770701e-08\n",
            "2.8081607743527334e-08\n",
            "2.8081607743527334e-08\n",
            "2.8081607743527334e-08\n",
            "1.1735175027797108e-07\n",
            "0\n",
            "0\n",
            "0\n",
            "0.17249612650306048\n",
            "0.17249612650306048\n",
            "0.17249612650306048\n",
            "0.17249612650306048\n",
            "0.17249612650306048\n",
            "0.17249612650306048\n",
            "0.17249612650306048\n",
            "0\n",
            "8.361164358107967e-08\n",
            "8.361164358107967e-08\n",
            "2.85176242779537e-07\n",
            "2.85176242779537e-07\n",
            "0.00022465242378667558\n",
            "0.00047861739836433645\n",
            "0.00047861739836433645\n",
            "0.00047861739836433645\n",
            "0.00047861739836433645\n",
            "0\n",
            "0\n",
            "4.317552682243407e-09\n",
            "9.462151882619112e-09\n",
            "2.3731020697602104e-06\n",
            "2.3731020697602104e-06\n",
            "2.3731020697602104e-06\n",
            "2.3731020697602104e-06\n",
            "1.9820574711719488e-05\n",
            "1.9820574711719488e-05\n",
            "0\n",
            "0\n",
            "0\n",
            "6.000371156862819e-09\n",
            "6.000371156862819e-09\n",
            "7.980144243890048e-07\n",
            "7.980144243890048e-07\n",
            "1.1495355614593279e-06\n",
            "1.1495355614593279e-06\n",
            "1.1495355614593279e-06\n",
            "0\n",
            "6.080628832984928e-08\n",
            "6.8876461528102815e-06\n",
            "6.8876461528102815e-06\n",
            "2.5991358515371344e-05\n",
            "2.5991358515371344e-05\n",
            "0.005592068149541466\n",
            "0.005592068149541466\n",
            "0.005592068149541466\n",
            "0.005845248179081006\n",
            "0\n",
            "0\n",
            "7.284499198366756e-06\n",
            "7.284499198366756e-06\n",
            "7.284499198366756e-06\n",
            "7.284499198366756e-06\n",
            "7.284499198366756e-06\n",
            "0.00012329309665949477\n",
            "0.00012329309665949477\n",
            "0.00012329309665949477\n",
            "0\n",
            "1.9127387404622644e-09\n",
            "1.9127387404622644e-09\n",
            "1.9127387404622644e-09\n",
            "1.9127387404622644e-09\n",
            "4.3773828717613434e-07\n",
            "4.3773828717613434e-07\n",
            "1.4265172141288913e-05\n",
            "1.4265172141288913e-05\n",
            "1.4265172141288913e-05\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "4.164814274547597e-08\n",
            "1.1171038651620965e-07\n",
            "1.1171038651620965e-07\n",
            "3.0974653300653374e-05\n",
            "3.0974653300653374e-05\n",
            "4.0511655083359274e-05\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "2.030546447134112e-10\n",
            "3.692251350602022e-08\n",
            "3.692251350602022e-08\n",
            "3.692251350602022e-08\n",
            "3.692251350602022e-08\n",
            "3.692251350602022e-08\n",
            "0\n",
            "0\n",
            "1.2591190015492894e-06\n",
            "0.00027265645490851084\n",
            "0.00027265645490851084\n",
            "0.00027265645490851084\n",
            "0.00027265645490851084\n",
            "0.00027265645490851084\n",
            "0.00027265645490851084\n",
            "0.00027265645490851084\n",
            "0\n",
            "2.8650630815812952e-09\n",
            "5.470558671831516e-08\n",
            "5.470558671831516e-08\n",
            "5.470558671831516e-08\n",
            "1.2374755346056184e-06\n",
            "1.2374755346056184e-06\n",
            "1.2374755346056184e-06\n",
            "1.2374755346056184e-06\n",
            "1.2374755346056184e-06\n",
            "0\n",
            "4.66680321534775e-07\n",
            "4.66680321534775e-07\n",
            "4.66680321534775e-07\n",
            "4.66680321534775e-07\n",
            "4.66680321534775e-07\n",
            "1.7843685144714002e-06\n",
            "1.892612171178045e-06\n",
            "1.892612171178045e-06\n",
            "0.0004064332820184735\n",
            "0\n",
            "0\n",
            "0.00025360082488982776\n",
            "0.00025360082488982776\n",
            "0.00025360082488982776\n",
            "0.000687167358717661\n",
            "0.001065837775488499\n",
            "0.001065837775488499\n",
            "0.001065837775488499\n",
            "0.00276707443598242\n",
            "0\n",
            "0\n",
            "3.4386537514947857e-06\n",
            "3.4386537514947857e-06\n",
            "3.4386537514947857e-06\n",
            "0.005289458544672119\n",
            "0.005289458544672119\n",
            "0.005289458544672119\n",
            "0.005289458544672119\n",
            "0.005289458544672119\n",
            "0\n",
            "0\n",
            "0\n",
            "0.0007893710575567345\n",
            "0.0007893710575567345\n",
            "0.0007893710575567345\n",
            "0.0007893710575567345\n",
            "0.0007893710575567345\n",
            "0.0007893710575567345\n",
            "0.0007893710575567345\n",
            "0\n",
            "0\n",
            "5.711141615384923e-07\n",
            "5.711141615384923e-07\n",
            "5.711141615384923e-07\n",
            "2.4019751000394256e-05\n",
            "2.4019751000394256e-05\n",
            "2.4019751000394256e-05\n",
            "2.4019751000394256e-05\n",
            "2.4019751000394256e-05\n",
            "0\n",
            "0\n",
            "1.65233154812122e-07\n",
            "1.65233154812122e-07\n",
            "1.65233154812122e-07\n",
            "1.889252197349385e-07\n",
            "1.889252197349385e-07\n",
            "1.889252197349385e-07\n",
            "1.889252197349385e-07\n",
            "1.317331317768927e-05\n",
            "0\n",
            "0\n",
            "2.545454088828013e-09\n",
            "2.545454088828013e-09\n",
            "2.545454088828013e-09\n",
            "9.891239379352894e-07\n",
            "9.891239379352894e-07\n",
            "9.891239379352894e-07\n",
            "9.891239379352894e-07\n",
            "3.054046018437534e-05\n",
            "0\n",
            "0\n",
            "1.5752768620613168e-05\n",
            "0.00022975211663106421\n",
            "0.00022975211663106421\n",
            "0.00022975211663106421\n",
            "0.00022975211663106421\n",
            "0.00022975211663106421\n",
            "0.0008447540283072381\n",
            "0.0008447540283072381\n",
            "0\n",
            "0\n",
            "1.143660355522013e-09\n",
            "1.143660355522013e-09\n",
            "1.143660355522013e-09\n",
            "6.127121943520937e-08\n",
            "6.127121943520937e-08\n",
            "3.161144581764741e-06\n",
            "3.161144581764741e-06\n",
            "3.161144581764741e-06\n",
            "0\n",
            "0\n",
            "0.0003567843825419088\n",
            "0.0003567843825419088\n",
            "0.0003567843825419088\n",
            "0.0003567843825419088\n",
            "0.0003567843825419088\n",
            "0.0003567843825419088\n",
            "0.020856462632969135\n",
            "0.020856462632969135\n",
            "0\n",
            "6.858936462776532e-12\n",
            "3.5589441166058426e-10\n",
            "3.5589441166058426e-10\n",
            "5.032597581095686e-10\n",
            "5.032597581095686e-10\n",
            "5.032597581095686e-10\n",
            "5.032597581095686e-10\n",
            "5.032597581095686e-10\n",
            "1.2323268492627485e-06\n",
            "0\n",
            "2.1923330829331392e-07\n",
            "2.1923330829331392e-07\n",
            "2.1923330829331392e-07\n",
            "4.715005733426441e-06\n",
            "0.0003834027150969993\n",
            "0.0003834027150969993\n",
            "0.0004702832993730649\n",
            "0.0004702832993730649\n",
            "0.0004702832993730649\n",
            "0\n",
            "0\n",
            "8.073986963898221e-08\n",
            "1.9267626554159775e-05\n",
            "1.9267626554159775e-05\n",
            "1.9267626554159775e-05\n",
            "1.9267626554159775e-05\n",
            "2.542041264857796e-05\n",
            "2.933983207357887e-05\n",
            "2.933983207357887e-05\n",
            "0\n",
            "0\n",
            "0\n",
            "6.141372048057557e-07\n",
            "6.141372048057557e-07\n",
            "6.141372048057557e-07\n",
            "6.141372048057557e-07\n",
            "6.141372048057557e-07\n",
            "6.141372048057557e-07\n",
            "6.141372048057557e-07\n",
            "0\n",
            "0\n",
            "3.0100652754820806e-08\n",
            "3.0100652754820806e-08\n",
            "3.0100652754820806e-08\n",
            "0.0003245410682569481\n",
            "0.0003245410682569481\n",
            "0.0003245410682569481\n",
            "0.0003245410682569481\n",
            "0.0003245410682569481\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1.8877570099042733e-08\n",
            "1.8877570099042733e-08\n",
            "1.8877570099042733e-08\n",
            "1.8877570099042733e-08\n",
            "6.390647364259088e-07\n",
            "3.1067790945881685e-06\n",
            "0\n",
            "0\n",
            "0\n",
            "7.621939965346006e-05\n",
            "7.621939965346006e-05\n",
            "7.621939965346006e-05\n",
            "7.621939965346006e-05\n",
            "7.621939965346006e-05\n",
            "7.621939965346006e-05\n",
            "7.621939965346006e-05\n",
            "0\n",
            "0\n",
            "3.2136228357871523e-07\n",
            "3.2136228357871523e-07\n",
            "3.2136228357871523e-07\n",
            "0.0014145706452035613\n",
            "0.0014145706452035613\n",
            "0.0014145706452035613\n",
            "0.0014145706452035613\n",
            "0.0014145706452035613\n",
            "0\n",
            "6.1634306803526434e-09\n",
            "6.1634306803526434e-09\n",
            "5.042300502612436e-08\n",
            "5.042300502612436e-08\n",
            "6.6364117263104165e-06\n",
            "6.6364117263104165e-06\n",
            "6.6364117263104165e-06\n",
            "6.6364117263104165e-06\n",
            "6.6364117263104165e-06\n",
            "0\n",
            "1.1408950372055786e-08\n",
            "2.5088739188138265e-05\n",
            "2.5088739188138265e-05\n",
            "2.5088739188138265e-05\n",
            "2.5088739188138265e-05\n",
            "2.5088739188138265e-05\n",
            "2.5088739188138265e-05\n",
            "2.5088739188138265e-05\n",
            "2.5088739188138265e-05\n",
            "0\n",
            "0\n",
            "1.1597312006798117e-06\n",
            "2.337930204578793e-06\n",
            "2.337930204578793e-06\n",
            "2.337930204578793e-06\n",
            "2.337930204578793e-06\n",
            "2.337930204578793e-06\n",
            "0.00017503932579314367\n",
            "0.00017503932579314367\n",
            "0\n",
            "0\n",
            "7.071807974516094e-08\n",
            "7.071807974516094e-08\n",
            "7.071807974516094e-08\n",
            "1.0190813088811279e-05\n",
            "1.0190813088811279e-05\n",
            "1.0190813088811279e-05\n",
            "1.0190813088811279e-05\n",
            "1.0190813088811279e-05\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "4.039420632511137e-08\n",
            "2.582198229444442e-06\n",
            "2.582198229444442e-06\n",
            "2.582198229444442e-06\n",
            "2.582198229444442e-06\n",
            "5.6335061058696665e-06\n",
            "0\n",
            "0\n",
            "1.035053723180058e-09\n",
            "1.035053723180058e-09\n",
            "9.306864597674016e-09\n",
            "0.0006131786306903883\n",
            "0.0006131786306903883\n",
            "0.0006131786306903883\n",
            "0.0006131786306903883\n",
            "0.0006131786306903883\n",
            "0\n",
            "4.600980090751002e-09\n",
            "1.2298910437193826e-06\n",
            "1.2298910437193826e-06\n",
            "1.2298910437193826e-06\n",
            "0.00013903083992378789\n",
            "0.00013903083992378789\n",
            "0.00013903083992378789\n",
            "0.00013903083992378789\n",
            "0.00013903083992378789\n",
            "0\n",
            "8.901488689571043e-09\n",
            "8.901488689571043e-09\n",
            "3.764218575902167e-05\n",
            "3.764218575902167e-05\n",
            "3.764218575902167e-05\n",
            "3.764218575902167e-05\n",
            "3.764218575902167e-05\n",
            "3.764218575902167e-05\n",
            "3.764218575902167e-05\n",
            "0\n",
            "1.0524753863807001e-07\n",
            "8.239239536425469e-05\n",
            "8.239239536425469e-05\n",
            "8.239239536425469e-05\n",
            "8.239239536425469e-05\n",
            "8.239239536425469e-05\n",
            "8.239239536425469e-05\n",
            "8.239239536425469e-05\n",
            "8.239239536425469e-05\n",
            "0\n",
            "0\n",
            "0\n",
            "0.004346448784127689\n",
            "0.004346448784127689\n",
            "0.004346448784127689\n",
            "0.004346448784127689\n",
            "0.004346448784127689\n",
            "0.004346448784127689\n",
            "0.004346448784127689\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "3.4650745196353726e-06\n",
            "3.4650745196353726e-06\n",
            "3.4650745196353726e-06\n",
            "3.4650745196353726e-06\n",
            "1.0769145324174311e-05\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1.7305505058413353e-06\n",
            "0.015434619078345419\n",
            "0.015434619078345419\n",
            "0.015434619078345419\n",
            "0.015434619078345419\n",
            "0.015434619078345419\n",
            "0\n",
            "1.4828846879229015e-07\n",
            "0.0001357540875541825\n",
            "0.0001357540875541825\n",
            "0.0001357540875541825\n",
            "0.0002094901609870318\n",
            "0.0002094901609870318\n",
            "0.0002094901609870318\n",
            "0.0002094901609870318\n",
            "0.0002094901609870318\n",
            "0\n",
            "4.633170811045781e-07\n",
            "0.0002912638562835348\n",
            "0.0002912638562835348\n",
            "0.0002912638562835348\n",
            "0.0002912638562835348\n",
            "0.0002912638562835348\n",
            "0.0002912638562835348\n",
            "0.0002912638562835348\n",
            "0.0002912638562835348\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "2.856772446442084e-10\n",
            "9.519062315180406e-08\n",
            "9.519062315180406e-08\n",
            "9.519062315180406e-08\n",
            "9.845482382679626e-07\n",
            "9.845482382679626e-07\n",
            "0\n",
            "0\n",
            "0\n",
            "0.005834280594066045\n",
            "0.005834280594066045\n",
            "0.005834280594066045\n",
            "0.005834280594066045\n",
            "0.005834280594066045\n",
            "0.005834280594066045\n",
            "0.005834280594066045\n",
            "0\n",
            "4.136624110443369e-08\n",
            "1.5427892154946226e-05\n",
            "1.5427892154946226e-05\n",
            "1.5427892154946226e-05\n",
            "1.5427892154946226e-05\n",
            "2.0423346352471433e-05\n",
            "6.0849851150178326e-05\n",
            "6.0849851150178326e-05\n",
            "6.0849851150178326e-05\n",
            "0\n",
            "1.2481662594663774e-07\n",
            "1.2481662594663774e-07\n",
            "0.0012722063705634886\n",
            "0.0012722063705634886\n",
            "0.0012722063705634886\n",
            "0.0012722063705634886\n",
            "0.0012722063705634886\n",
            "0.0012722063705634886\n",
            "0.0012722063705634886\n",
            "0\n",
            "0\n",
            "0.0006620367983840562\n",
            "0.0006620367983840562\n",
            "0.0006620367983840562\n",
            "0.0006620367983840562\n",
            "0.0006620367983840562\n",
            "0.0006620367983840562\n",
            "0.0006620367983840562\n",
            "0.0006620367983840562\n",
            "0\n",
            "0\n",
            "1.1270977828925137e-06\n",
            "0.0005059324045938276\n",
            "0.0005059324045938276\n",
            "0.0005059324045938276\n",
            "0.0005059324045938276\n",
            "0.0005059324045938276\n",
            "0.0027225383391230222\n",
            "0.0027225383391230222\n",
            "0\n",
            "0\n",
            "4.453283235668809e-06\n",
            "4.453283235668809e-06\n",
            "4.453283235668809e-06\n",
            "4.6286058748652145e-06\n",
            "4.710521235759628e-05\n",
            "4.710521235759628e-05\n",
            "4.710521235759628e-05\n",
            "4.710521235759628e-05\n",
            "0\n",
            "0\n",
            "0.00014868961722033119\n",
            "0.00014868961722033119\n",
            "0.00014868961722033119\n",
            "0.00014868961722033119\n",
            "0.00014868961722033119\n",
            "0.03749343191467794\n",
            "0.03749343191467794\n",
            "0.03749343191467794\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "4.201036972116968e-09\n",
            "4.201036972116968e-09\n",
            "4.201036972116968e-09\n",
            "4.201036972116968e-09\n",
            "6.983030472772357e-06\n",
            "6.983030472772357e-06\n",
            "0\n",
            "7.454998999676021e-08\n",
            "7.454998999676021e-08\n",
            "1.3584360544372306e-07\n",
            "1.3584360544372306e-07\n",
            "1.4021183711014843e-07\n",
            "1.4021183711014843e-07\n",
            "1.4021183711014843e-07\n",
            "1.4021183711014843e-07\n",
            "1.4021183711014843e-07\n",
            "0\n",
            "6.267378386891248e-10\n",
            "2.714858933822606e-06\n",
            "2.714858933822606e-06\n",
            "2.714858933822606e-06\n",
            "8.384794477289139e-06\n",
            "8.384794477289139e-06\n",
            "8.384794477289139e-06\n",
            "8.384794477289139e-06\n",
            "8.384794477289139e-06\n",
            "0\n",
            "4.082441675702888e-08\n",
            "4.082441675702888e-08\n",
            "4.082441675702888e-08\n",
            "4.082441675702888e-08\n",
            "4.082441675702888e-08\n",
            "4.082441675702888e-08\n",
            "4.082441675702888e-08\n",
            "4.082441675702888e-08\n",
            "2.888221349484695e-06\n",
            "0\n",
            "0\n",
            "1.0093395405841322e-06\n",
            "1.0093395405841322e-06\n",
            "1.0093395405841322e-06\n",
            "1.0093395405841322e-06\n",
            "5.517401808803036e-05\n",
            "5.517401808803036e-05\n",
            "5.517401808803036e-05\n",
            "5.517401808803036e-05\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "2.8652990272254735e-08\n",
            "4.6856398880878467e-07\n",
            "4.6856398880878467e-07\n",
            "0.20677509541481867\n",
            "0.20677509541481867\n",
            "0.20677509541481867\n",
            "0\n",
            "0\n",
            "8.06458192001576e-09\n",
            "3.432620597033783e-07\n",
            "3.432620597033783e-07\n",
            "3.432620597033783e-07\n",
            "3.432620597033783e-07\n",
            "3.432620597033783e-07\n",
            "3.432620597033783e-07\n",
            "2.409800977190039e-06\n",
            "0\n",
            "0\n",
            "0\n",
            "0.0003415639281696334\n",
            "0.0003415639281696334\n",
            "0.0003415639281696334\n",
            "0.0003415639281696334\n",
            "0.0003415639281696334\n",
            "0.0003415639281696334\n",
            "0.0003415639281696334\n",
            "0\n",
            "0\n",
            "4.930198829025127e-07\n",
            "4.930198829025127e-07\n",
            "4.930198829025127e-07\n",
            "0.0006291031787776822\n",
            "0.0006291031787776822\n",
            "0.0006291031787776822\n",
            "0.0006291031787776822\n",
            "0.0006291031787776822\n",
            "0\n",
            "0\n",
            "0\n",
            "0.006615244878465285\n",
            "0.006615244878465285\n",
            "0.006615244878465285\n",
            "0.006615244878465285\n",
            "0.006615244878465285\n",
            "0.006615244878465285\n",
            "0.006615244878465285\n",
            "0\n",
            "5.1992390801958015e-09\n",
            "5.1992390801958015e-09\n",
            "9.669018028029771e-05\n",
            "9.669018028029771e-05\n",
            "9.669018028029771e-05\n",
            "9.669018028029771e-05\n",
            "9.669018028029771e-05\n",
            "9.669018028029771e-05\n",
            "9.669018028029771e-05\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "3.8325435000429005e-06\n",
            "3.8325435000429005e-06\n",
            "3.8325435000429005e-06\n",
            "3.8325435000429005e-06\n",
            "3.8325435000429005e-06\n",
            "3.8325435000429005e-06\n",
            "0\n",
            "0\n",
            "2.1758344865480568e-05\n",
            "2.1758344865480568e-05\n",
            "2.1758344865480568e-05\n",
            "0.01598424472034885\n",
            "0.01598424472034885\n",
            "0.01598424472034885\n",
            "0.01598424472034885\n",
            "0.03994721750243334\n",
            "0\n",
            "2.272603167639785e-09\n",
            "7.148377340869287e-08\n",
            "7.148377340869287e-08\n",
            "7.148377340869287e-08\n",
            "2.4370548580398286e-06\n",
            "2.4370548580398286e-06\n",
            "2.4370548580398286e-06\n",
            "2.4370548580398286e-06\n",
            "2.4370548580398286e-06\n",
            "0\n",
            "0\n",
            "4.231452701126542e-12\n",
            "4.231452701126542e-12\n",
            "2.498201379543845e-10\n",
            "2.498201379543845e-10\n",
            "2.498201379543845e-10\n",
            "4.3942141704020326e-09\n",
            "1.6257921544786642e-07\n",
            "2.391076667501284e-07\n",
            "0\n",
            "1.095803969211972e-09\n",
            "3.0124564819339023e-05\n",
            "3.0124564819339023e-05\n",
            "3.0124564819339023e-05\n",
            "3.0124564819339023e-05\n",
            "3.0124564819339023e-05\n",
            "3.0124564819339023e-05\n",
            "3.0124564819339023e-05\n",
            "3.0124564819339023e-05\n",
            "0\n",
            "0\n",
            "8.496551545619328e-06\n",
            "8.496551545619328e-06\n",
            "8.496551545619328e-06\n",
            "8.496551545619328e-06\n",
            "2.225051976692386e-05\n",
            "0.00042565221753173676\n",
            "0.00042565221753173676\n",
            "0.00042565221753173676\n",
            "0\n",
            "0\n",
            "0\n",
            "5.102851408561616e-06\n",
            "5.102851408561616e-06\n",
            "1.991789220464073e-05\n",
            "2.2634985297915208e-05\n",
            "2.2634985297915208e-05\n",
            "2.2634985297915208e-05\n",
            "0.00014690522755292915\n",
            "0\n",
            "1.60880337553052e-07\n",
            "4.6179929600378534e-06\n",
            "8.469934910708406e-06\n",
            "8.469934910708406e-06\n",
            "8.469934910708406e-06\n",
            "8.469934910708406e-06\n",
            "8.469934910708406e-06\n",
            "1.1826402670833741e-05\n",
            "1.1826402670833741e-05\n",
            "0\n",
            "0\n",
            "3.217398221605762e-07\n",
            "3.217398221605762e-07\n",
            "2.737292519638913e-06\n",
            "2.332804092012695e-05\n",
            "0.0001087107935712582\n",
            "0.0001087107935712582\n",
            "0.0001087107935712582\n",
            "0.0001087107935712582\n",
            "0\n",
            "0\n",
            "0\n",
            "6.648294071215933e-10\n",
            "6.648294071215933e-10\n",
            "2.2522438952878482e-07\n",
            "2.2522438952878482e-07\n",
            "2.2522438952878482e-07\n",
            "2.2522438952878482e-07\n",
            "2.2522438952878482e-07\n",
            "0\n",
            "0\n",
            "4.551535401940763e-08\n",
            "4.551535401940763e-08\n",
            "4.551535401940763e-08\n",
            "0.001170391075452382\n",
            "0.001170391075452382\n",
            "0.001170391075452382\n",
            "0.001170391075452382\n",
            "0.001170391075452382\n",
            "0\n",
            "1.4719127109319365e-07\n",
            "1.4719127109319365e-07\n",
            "1.4719127109319365e-07\n",
            "1.409369346083059e-06\n",
            "1.409369346083059e-06\n",
            "1.409369346083059e-06\n",
            "1.409369346083059e-06\n",
            "3.379666674865236e-06\n",
            "4.875874316160307e-06\n",
            "0\n",
            "0\n",
            "0\n",
            "0.00010109919281010908\n",
            "0.00010109919281010908\n",
            "0.00010109919281010908\n",
            "0.00010109919281010908\n",
            "0.00010109919281010908\n",
            "0.0015185430873733783\n",
            "0.0015185430873733783\n",
            "0\n",
            "0\n",
            "4.700865513346097e-05\n",
            "0.0002893961289683222\n",
            "0.0002893961289683222\n",
            "0.0002893961289683222\n",
            "0.0002893961289683222\n",
            "0.0002893961289683222\n",
            "0.002113022953602814\n",
            "0.002113022953602814\n",
            "0\n",
            "0\n",
            "1.238103147239141e-08\n",
            "1.3156972645518912e-07\n",
            "1.3156972645518912e-07\n",
            "1.3156972645518912e-07\n",
            "1.3156972645518912e-07\n",
            "1.3156972645518912e-07\n",
            "6.781540601538504e-07\n",
            "1.2232994120520324e-06\n",
            "0\n",
            "0\n",
            "5.506651964723274e-12\n",
            "5.506651964723274e-12\n",
            "1.2897329190461832e-07\n",
            "1.2897329190461832e-07\n",
            "1.2897329190461832e-07\n",
            "4.41412754293679e-06\n",
            "4.41412754293679e-06\n",
            "4.41412754293679e-06\n",
            "0\n",
            "0\n",
            "5.657758498139608e-06\n",
            "5.657758498139608e-06\n",
            "5.657758498139608e-06\n",
            "5.657758498139608e-06\n",
            "5.657758498139608e-06\n",
            "0.00611414912663672\n",
            "0.00611414912663672\n",
            "0.00611414912663672\n",
            "0\n",
            "2.994045600842251e-09\n",
            "2.994045600842251e-09\n",
            "2.994045600842251e-09\n",
            "2.994045600842251e-09\n",
            "6.484914938342977e-06\n",
            "6.484914938342977e-06\n",
            "3.6179962040849775e-05\n",
            "3.6179962040849775e-05\n",
            "0.004187190643344294\n",
            "0\n",
            "0\n",
            "1.867473746250172e-07\n",
            "5.158144651301112e-05\n",
            "5.158144651301112e-05\n",
            "5.158144651301112e-05\n",
            "5.158144651301112e-05\n",
            "5.158144651301112e-05\n",
            "5.158144651301112e-05\n",
            "5.158144651301112e-05\n",
            "0\n",
            "0\n",
            "1.7004538953087246e-10\n",
            "1.7004538953087246e-10\n",
            "1.7004538953087246e-10\n",
            "1.6036763932089363e-07\n",
            "1.6036763932089363e-07\n",
            "2.6895906829247604e-06\n",
            "2.6895906829247604e-06\n",
            "2.6895906829247604e-06\n",
            "0\n",
            "0\n",
            "4.500142299661197e-11\n",
            "1.3956683601786032e-08\n",
            "1.3956683601786032e-08\n",
            "1.3956683601786032e-08\n",
            "1.3956683601786032e-08\n",
            "1.3956683601786032e-08\n",
            "8.601669685391692e-07\n",
            "8.601669685391692e-07\n",
            "0\n",
            "7.01450002973664e-08\n",
            "7.01450002973664e-08\n",
            "7.01450002973664e-08\n",
            "7.01450002973664e-08\n",
            "1.5045023338803413e-07\n",
            "1.5045023338803413e-07\n",
            "2.4908636397882736e-07\n",
            "2.4908636397882736e-07\n",
            "2.4908636397882736e-07\n",
            "0\n",
            "0\n",
            "1.6144845722902746e-09\n",
            "1.6144845722902746e-09\n",
            "2.5930379866646902e-09\n",
            "2.5930379866646902e-09\n",
            "4.56828834805423e-07\n",
            "4.56828834805423e-07\n",
            "6.004805532636735e-05\n",
            "6.004805532636735e-05\n",
            "0\n",
            "3.628441125437191e-08\n",
            "0.000672121475866895\n",
            "0.000672121475866895\n",
            "0.000672121475866895\n",
            "0.000672121475866895\n",
            "0.000672121475866895\n",
            "0.000672121475866895\n",
            "0.0016473064006256636\n",
            "0.0016473064006256636\n",
            "0\n",
            "0\n",
            "5.672938722397237e-10\n",
            "5.672938722397237e-10\n",
            "2.9726342355981507e-07\n",
            "0.00017454092921450332\n",
            "0.00017454092921450332\n",
            "0.00017454092921450332\n",
            "0.00017454092921450332\n",
            "0.00382465308138063\n",
            "0\n",
            "0\n",
            "0\n",
            "4.116070793615472e-08\n",
            "4.553932419989545e-07\n",
            "4.553932419989545e-07\n",
            "4.553932419989545e-07\n",
            "4.553932419989545e-07\n",
            "4.553932419989545e-07\n",
            "4.553932419989545e-07\n",
            "0\n",
            "9.00902374667942e-06\n",
            "9.00902374667942e-06\n",
            "0.002369799378362362\n",
            "0.002369799378362362\n",
            "0.002369799378362362\n",
            "0.002369799378362362\n",
            "0.002369799378362362\n",
            "0.002369799378362362\n",
            "0.002369799378362362\n",
            "0\n",
            "0\n",
            "2.8171704277608303e-06\n",
            "3.261638068494966e-05\n",
            "3.261638068494966e-05\n",
            "3.261638068494966e-05\n",
            "3.261638068494966e-05\n",
            "3.261638068494966e-05\n",
            "3.261638068494966e-05\n",
            "0.00036312196530192774\n",
            "0\n",
            "0\n",
            "6.965975904796615e-08\n",
            "6.965975904796615e-08\n",
            "1.230649316668901e-07\n",
            "1.230649316668901e-07\n",
            "1.230649316668901e-07\n",
            "4.1496558585353026e-07\n",
            "4.1496558585353026e-07\n",
            "0.004033693680811155\n",
            "0\n",
            "0\n",
            "0\n",
            "1.730448759510053e-08\n",
            "1.3279436311082512e-06\n",
            "1.3279436311082512e-06\n",
            "1.3279436311082512e-06\n",
            "1.3279436311082512e-06\n",
            "1.7843943295131466e-06\n",
            "1.7843943295131466e-06\n",
            "0\n",
            "4.041248103254367e-08\n",
            "4.041248103254367e-08\n",
            "4.041248103254367e-08\n",
            "4.041248103254367e-08\n",
            "4.041248103254367e-08\n",
            "4.041248103254367e-08\n",
            "4.041248103254367e-08\n",
            "5.072302776677106e-07\n",
            "5.072302776677106e-07\n",
            "0\n",
            "3.044545533680758e-07\n",
            "3.044545533680758e-07\n",
            "6.165697121376433e-07\n",
            "6.165697121376433e-07\n",
            "6.165697121376433e-07\n",
            "6.165697121376433e-07\n",
            "6.165697121376433e-07\n",
            "6.165697121376433e-07\n",
            "2.9315626684345757e-05\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "7.797640306747982e-10\n",
            "4.214467825282767e-09\n",
            "4.214467825282767e-09\n",
            "4.214467825282767e-09\n",
            "1.0780662658188075e-06\n",
            "0.0027896753054139733\n",
            "0\n",
            "0\n",
            "0\n",
            "2.1559138727893745e-06\n",
            "2.1559138727893745e-06\n",
            "2.1559138727893745e-06\n",
            "2.1559138727893745e-06\n",
            "2.1559138727893745e-06\n",
            "2.1559138727893745e-06\n",
            "2.1559138727893745e-06\n",
            "0\n",
            "4.0165248215969197e-07\n",
            "4.0165248215969197e-07\n",
            "4.0165248215969197e-07\n",
            "4.0165248215969197e-07\n",
            "4.0165248215969197e-07\n",
            "4.0165248215969197e-07\n",
            "4.0165248215969197e-07\n",
            "0.00019817461052124087\n",
            "0.00019817461052124087\n",
            "0\n",
            "4.328595686950967e-10\n",
            "4.328595686950967e-10\n",
            "3.785006715637841e-08\n",
            "4.4672785995921986e-07\n",
            "4.4672785995921986e-07\n",
            "4.4672785995921986e-07\n",
            "4.4672785995921986e-07\n",
            "4.4672785995921986e-07\n",
            "4.4672785995921986e-07\n",
            "0\n",
            "0\n",
            "3.009251476045257e-08\n",
            "3.009251476045257e-08\n",
            "3.009251476045257e-08\n",
            "1.0275768420949335e-06\n",
            "1.0275768420949335e-06\n",
            "1.0275768420949335e-06\n",
            "2.8251464958264145e-06\n",
            "1.4909285244195105e-05\n",
            "0\n",
            "0\n",
            "1.895222567598516e-11\n",
            "1.705901113393646e-07\n",
            "1.705901113393646e-07\n",
            "1.705901113393646e-07\n",
            "1.705901113393646e-07\n",
            "1.705901113393646e-07\n",
            "1.705901113393646e-07\n",
            "1.705901113393646e-07\n",
            "0\n",
            "3.343294526274825e-08\n",
            "3.343294526274825e-08\n",
            "3.343294526274825e-08\n",
            "7.309500856516542e-08\n",
            "3.075777936665939e-05\n",
            "3.075777936665939e-05\n",
            "3.075777936665939e-05\n",
            "3.5699212659920243e-05\n",
            "3.5699212659920243e-05\n",
            "0\n",
            "1.5396211285781565e-07\n",
            "1.5396211285781565e-07\n",
            "1.5396211285781565e-07\n",
            "1.5396211285781565e-07\n",
            "5.58780978389369e-05\n",
            "5.58780978389369e-05\n",
            "5.58780978389369e-05\n",
            "5.58780978389369e-05\n",
            "5.58780978389369e-05\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1.3671149954420424e-12\n",
            "1.3236218135272403e-08\n",
            "1.3236218135272403e-08\n",
            "1.3236218135272403e-08\n",
            "1.3236218135272403e-08\n",
            "1.3236218135272403e-08\n",
            "0\n",
            "0\n",
            "8.532896700369269e-08\n",
            "8.532896700369269e-08\n",
            "8.532896700369269e-08\n",
            "8.532896700369269e-08\n",
            "8.532896700369269e-08\n",
            "0.0006948044293401675\n",
            "0.0006948044293401675\n",
            "0.0006948044293401675\n",
            "0\n",
            "0\n",
            "2.874369969259042e-07\n",
            "0.00104021993119175\n",
            "0.005192041808540412\n",
            "0.005192041808540412\n",
            "0.005192041808540412\n",
            "0.005192041808540412\n",
            "0.005192041808540412\n",
            "0.005192041808540412\n",
            "0\n",
            "1.9377091747235575e-09\n",
            "1.6612389445197865e-08\n",
            "1.6612389445197865e-08\n",
            "1.6612389445197865e-08\n",
            "2.526864823815464e-07\n",
            "2.526864823815464e-07\n",
            "1.5931178255847224e-05\n",
            "1.5931178255847224e-05\n",
            "0.0018600046111732485\n",
            "0\n",
            "0\n",
            "8.457139942425412e-06\n",
            "8.457139942425412e-06\n",
            "8.457139942425412e-06\n",
            "8.457139942425412e-06\n",
            "8.457139942425412e-06\n",
            "8.457139942425412e-06\n",
            "8.457139942425412e-06\n",
            "8.457139942425412e-06\n",
            "0\n",
            "0\n",
            "4.7897137119342697e-08\n",
            "2.694353356421001e-07\n",
            "2.694353356421001e-07\n",
            "2.694353356421001e-07\n",
            "2.694353356421001e-07\n",
            "2.694353356421001e-07\n",
            "2.694353356421001e-07\n",
            "6.641686408027751e-06\n",
            "0\n",
            "0\n",
            "0\n",
            "1.153471335352316e-06\n",
            "1.153471335352316e-06\n",
            "1.642760688779038e-06\n",
            "1.8747374254475814e-05\n",
            "0.00011003599492566612\n",
            "0.00011003599492566612\n",
            "0.00011839713566977492\n",
            "0\n",
            "0\n",
            "5.618934168326501e-08\n",
            "6.0370398003289905e-06\n",
            "6.0370398003289905e-06\n",
            "6.0370398003289905e-06\n",
            "6.0370398003289905e-06\n",
            "6.0370398003289905e-06\n",
            "8.116619420263354e-06\n",
            "8.116619420263354e-06\n",
            "0\n",
            "0\n",
            "8.030887057718645e-05\n",
            "0.0008623922780197933\n",
            "0.0008623922780197933\n",
            "0.015238447219815085\n",
            "0.015238447219815085\n",
            "0.015238447219815085\n",
            "0.015238447219815085\n",
            "0.015238447219815085\n",
            "0\n",
            "8.310037601345509e-10\n",
            "8.310037601345509e-10\n",
            "2.948558276943022e-08\n",
            "2.948558276943022e-08\n",
            "2.2117591098785933e-06\n",
            "2.2117591098785933e-06\n",
            "2.2117591098785933e-06\n",
            "2.2117591098785933e-06\n",
            "2.2117591098785933e-06\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1.9221470049044407e-10\n",
            "3.266981867318308e-05\n",
            "3.266981867318308e-05\n",
            "3.266981867318308e-05\n",
            "3.266981867318308e-05\n",
            "3.266981867318308e-05\n",
            "0\n",
            "0\n",
            "1.8735837498110157e-09\n",
            "1.8735837498110157e-09\n",
            "1.8735837498110157e-09\n",
            "9.876438201184101e-06\n",
            "9.876438201184101e-06\n",
            "9.876438201184101e-06\n",
            "9.876438201184101e-06\n",
            "5.561588041587341e-05\n",
            "0\n",
            "2.0587307177534845e-10\n",
            "2.0587307177534845e-10\n",
            "2.0287417883599353e-06\n",
            "2.0287417883599353e-06\n",
            "2.0287417883599353e-06\n",
            "2.0287417883599353e-06\n",
            "2.0287417883599353e-06\n",
            "2.0287417883599353e-06\n",
            "2.0287417883599353e-06\n",
            "0\n",
            "0\n",
            "1.6299023833704854e-08\n",
            "5.2307860706629735e-08\n",
            "5.2307860706629735e-08\n",
            "5.2307860706629735e-08\n",
            "5.2307860706629735e-08\n",
            "5.2307860706629735e-08\n",
            "5.2307860706629735e-08\n",
            "2.3900280343219947e-06\n",
            "0\n",
            "1.4923547177380852e-08\n",
            "1.4923547177380852e-08\n",
            "1.4923547177380852e-08\n",
            "7.1990258975179575e-06\n",
            "7.1990258975179575e-06\n",
            "7.1990258975179575e-06\n",
            "7.1990258975179575e-06\n",
            "7.1990258975179575e-06\n",
            "7.1990258975179575e-06\n",
            "0\n",
            "0\n",
            "0.0004365489281133575\n",
            "0.0004365489281133575\n",
            "0.0004365489281133575\n",
            "0.0004365489281133575\n",
            "0.0004365489281133575\n",
            "0.0004365489281133575\n",
            "0.0004365489281133575\n",
            "0.0004365489281133575\n",
            "0\n",
            "0\n",
            "8.171531542955609e-07\n",
            "4.483497866533145e-05\n",
            "4.483497866533145e-05\n",
            "4.483497866533145e-05\n",
            "4.483497866533145e-05\n",
            "4.483497866533145e-05\n",
            "4.483497866533145e-05\n",
            "4.483497866533145e-05\n",
            "0\n",
            "0\n",
            "3.504303842840185e-07\n",
            "3.504303842840185e-07\n",
            "3.569339236003429e-06\n",
            "8.567715667991999e-06\n",
            "8.567715667991999e-06\n",
            "8.567715667991999e-06\n",
            "8.567715667991999e-06\n",
            "0.00011682930270816054\n",
            "0\n",
            "6.719813270529385e-05\n",
            "6.719813270529385e-05\n",
            "6.719813270529385e-05\n",
            "6.719813270529385e-05\n",
            "0.00011668933734631719\n",
            "0.00011668933734631719\n",
            "0.00011668933734631719\n",
            "0.00011668933734631719\n",
            "0.00011668933734631719\n",
            "0\n",
            "0\n",
            "0\n",
            "1.0835788180595364e-07\n",
            "2.7486288102486797e-07\n",
            "2.7486288102486797e-07\n",
            "3.018408313370413e-07\n",
            "3.018408313370413e-07\n",
            "3.018408313370413e-07\n",
            "3.018408313370413e-07\n",
            "0\n",
            "3.5484091645862757e-09\n",
            "3.5484091645862757e-09\n",
            "1.3657324359651686e-07\n",
            "1.3657324359651686e-07\n",
            "3.6679370109301787e-05\n",
            "3.6679370109301787e-05\n",
            "3.6679370109301787e-05\n",
            "3.6679370109301787e-05\n",
            "3.6679370109301787e-05\n",
            "0\n",
            "0\n",
            "0.023867470599664483\n",
            "0.023867470599664483\n",
            "0.023867470599664483\n",
            "0.023867470599664483\n",
            "0.023867470599664483\n",
            "0.023867470599664483\n",
            "0.035990644300380424\n",
            "0.035990644300380424\n",
            "0\n",
            "1.6745026262485968e-07\n",
            "1.6745026262485968e-07\n",
            "2.122298711747045e-07\n",
            "2.122298711747045e-07\n",
            "2.122298711747045e-07\n",
            "2.122298711747045e-07\n",
            "4.841163310960363e-07\n",
            "3.30540507729412e-06\n",
            "3.30540507729412e-06\n",
            "0\n",
            "1.0445516143554554e-08\n",
            "1.0445516143554554e-08\n",
            "1.0445516143554554e-08\n",
            "7.243049381101078e-06\n",
            "7.243049381101078e-06\n",
            "7.243049381101078e-06\n",
            "7.243049381101078e-06\n",
            "7.243049381101078e-06\n",
            "7.243049381101078e-06\n",
            "0\n",
            "0\n",
            "1.0448849520681168e-07\n",
            "4.079063099071728e-06\n",
            "4.079063099071728e-06\n",
            "4.079063099071728e-06\n",
            "4.079063099071728e-06\n",
            "4.079063099071728e-06\n",
            "4.079063099071728e-06\n",
            "0.0014647353836691876\n",
            "0\n",
            "6.283518185356465e-09\n",
            "6.283518185356465e-09\n",
            "6.283518185356465e-09\n",
            "6.283518185356465e-09\n",
            "4.69663824157412e-08\n",
            "4.69663824157412e-08\n",
            "4.69663824157412e-08\n",
            "2.8285372427122607e-07\n",
            "2.8285372427122607e-07\n",
            "0\n",
            "0\n",
            "3.9355057958253373e-10\n",
            "3.9355057958253373e-10\n",
            "3.9355057958253373e-10\n",
            "2.3160766537015932e-07\n",
            "2.3160766537015932e-07\n",
            "4.843698686835986e-06\n",
            "4.843698686835986e-06\n",
            "4.005803547283099e-05\n",
            "0\n",
            "0\n",
            "1.0069287881987588e-05\n",
            "1.0069287881987588e-05\n",
            "1.0069287881987588e-05\n",
            "1.7913196505494713e-05\n",
            "1.7913196505494713e-05\n",
            "1.7913196505494713e-05\n",
            "0.0001481678883542074\n",
            "0.0001481678883542074\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "2.9075440755899407e-06\n",
            "2.9075440755899407e-06\n",
            "0.00020577173550681533\n",
            "0.00020577173550681533\n",
            "0.00020577173550681533\n",
            "0\n",
            "8.401739812126813e-09\n",
            "2.5516596916616814e-06\n",
            "2.7408038546694274e-06\n",
            "2.7408038546694274e-06\n",
            "4.797646496993872e-06\n",
            "7.825002223500052e-06\n",
            "4.6302664371065484e-05\n",
            "4.6302664371065484e-05\n",
            "0.002005016730957149\n",
            "0\n",
            "7.536359445642031e-08\n",
            "7.536359445642031e-08\n",
            "7.119283561560528e-06\n",
            "7.119283561560528e-06\n",
            "7.119283561560528e-06\n",
            "7.119283561560528e-06\n",
            "7.119283561560528e-06\n",
            "0.0001332425244555293\n",
            "0.0001332425244555293\n",
            "0\n",
            "1.0459456907158785e-08\n",
            "1.0459456907158785e-08\n",
            "1.0459456907158785e-08\n",
            "1.0459456907158785e-08\n",
            "8.129868173286411e-06\n",
            "8.129868173286411e-06\n",
            "8.129868173286411e-06\n",
            "8.129868173286411e-06\n",
            "8.129868173286411e-06\n",
            "0\n",
            "0\n",
            "1.2683773886211492e-08\n",
            "1.2683773886211492e-08\n",
            "1.573279307630983e-07\n",
            "0.00022423889403430983\n",
            "0.00022423889403430983\n",
            "0.00022423889403430983\n",
            "0.00104371107346173\n",
            "0.00104371107346173\n",
            "0\n",
            "0\n",
            "4.0607992502327555e-07\n",
            "5.381650897037741e-06\n",
            "5.381650897037741e-06\n",
            "5.381650897037741e-06\n",
            "5.381650897037741e-06\n",
            "5.381650897037741e-06\n",
            "9.395565863776814e-05\n",
            "9.395565863776814e-05\n",
            "0\n",
            "7.09817306402104e-10\n",
            "5.7360797291722286e-06\n",
            "5.7360797291722286e-06\n",
            "5.7360797291722286e-06\n",
            "5.7360797291722286e-06\n",
            "5.7360797291722286e-06\n",
            "5.7360797291722286e-06\n",
            "5.7360797291722286e-06\n",
            "5.7360797291722286e-06\n",
            "0\n",
            "0\n",
            "6.013262313527417e-11\n",
            "6.013262313527417e-11\n",
            "6.013262313527417e-11\n",
            "7.04539363063752e-05\n",
            "7.04539363063752e-05\n",
            "7.04539363063752e-05\n",
            "7.04539363063752e-05\n",
            "7.04539363063752e-05\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1.0915279056219258e-09\n",
            "1.0915279056219258e-09\n",
            "1.0915279056219258e-09\n",
            "1.0915279056219258e-09\n",
            "1.0915279056219258e-09\n",
            "5.942099172303298e-06\n",
            "0\n",
            "0\n",
            "2.7245892798315492e-05\n",
            "2.7245892798315492e-05\n",
            "2.7245892798315492e-05\n",
            "0.0002812663098535414\n",
            "0.0002812663098535414\n",
            "0.0002812663098535414\n",
            "0.0002812663098535414\n",
            "0.0002812663098535414\n",
            "0\n",
            "1.5055918091363158e-06\n",
            "1.5055918091363158e-06\n",
            "0.08376509280439609\n",
            "0.08376509280439609\n",
            "0.08376509280439609\n",
            "0.08376509280439609\n",
            "0.08376509280439609\n",
            "0.08376509280439609\n",
            "0.08376509280439609\n",
            "0\n",
            "8.951849278380673e-09\n",
            "5.617338588385431e-07\n",
            "5.617338588385431e-07\n",
            "5.617338588385431e-07\n",
            "6.2420041623830505e-06\n",
            "8.747857756950967e-06\n",
            "8.747857756950967e-06\n",
            "8.747857756950967e-06\n",
            "8.747857756950967e-06\n",
            "0\n",
            "2.120709994895842e-10\n",
            "2.120709994895842e-10\n",
            "2.120709994895842e-10\n",
            "2.120709994895842e-10\n",
            "6.886082602785219e-06\n",
            "6.886082602785219e-06\n",
            "5.8481159035527204e-05\n",
            "5.8481159035527204e-05\n",
            "5.8481159035527204e-05\n",
            "0\n",
            "1.0053327110833532e-08\n",
            "0.0026413610508007387\n",
            "0.0026413610508007387\n",
            "0.0026413610508007387\n",
            "0.0026413610508007387\n",
            "0.0026413610508007387\n",
            "0.0026413610508007387\n",
            "0.0026413610508007387\n",
            "0.0026413610508007387\n",
            "0\n",
            "1.377558608327627e-09\n",
            "1.377558608327627e-09\n",
            "3.43533372027808e-05\n",
            "3.43533372027808e-05\n",
            "3.43533372027808e-05\n",
            "3.43533372027808e-05\n",
            "3.43533372027808e-05\n",
            "3.43533372027808e-05\n",
            "3.43533372027808e-05\n",
            "0\n",
            "2.579587859457171e-07\n",
            "2.579587859457171e-07\n",
            "3.421171936905918e-06\n",
            "3.421171936905918e-06\n",
            "3.421171936905918e-06\n",
            "3.421171936905918e-06\n",
            "3.421171936905918e-06\n",
            "3.421171936905918e-06\n",
            "8.85300520519137e-06\n",
            "0\n",
            "0\n",
            "3.925912817737076e-08\n",
            "3.925912817737076e-08\n",
            "3.925912817737076e-08\n",
            "3.925912817737076e-08\n",
            "1.8449391510008363e-07\n",
            "1.8449391510008363e-07\n",
            "2.0953426449203062e-05\n",
            "2.0953426449203062e-05\n",
            "0\n",
            "0\n",
            "3.890474707588449e-06\n",
            "3.890474707588449e-06\n",
            "3.890474707588449e-06\n",
            "3.890474707588449e-06\n",
            "3.890474707588449e-06\n",
            "3.890474707588449e-06\n",
            "3.890474707588449e-06\n",
            "2.7898012938068883e-05\n",
            "0\n",
            "0\n",
            "7.205476539612387e-08\n",
            "7.205476539612387e-08\n",
            "7.205476539612387e-08\n",
            "3.863038361936936e-06\n",
            "3.863038361936936e-06\n",
            "3.863038361936936e-06\n",
            "3.863038361936936e-06\n",
            "9.703569970776741e-06\n",
            "0\n",
            "1.0961965964925083e-06\n",
            "1.0961965964925083e-06\n",
            "1.5264854556645917e-05\n",
            "1.5264854556645917e-05\n",
            "1.5264854556645917e-05\n",
            "1.5264854556645917e-05\n",
            "1.5264854556645917e-05\n",
            "1.5264854556645917e-05\n",
            "0.08062534159742779\n",
            "0\n",
            "1.1371037346774481e-07\n",
            "2.8954645829096237e-05\n",
            "2.8954645829096237e-05\n",
            "2.8954645829096237e-05\n",
            "2.8954645829096237e-05\n",
            "2.8954645829096237e-05\n",
            "2.8954645829096237e-05\n",
            "2.8954645829096237e-05\n",
            "2.8954645829096237e-05\n",
            "0\n",
            "0\n",
            "6.139454699048717e-08\n",
            "6.139454699048717e-08\n",
            "8.032319653319432e-07\n",
            "8.032319653319432e-07\n",
            "0.00010706712232851857\n",
            "0.00010706712232851857\n",
            "0.00010706712232851857\n",
            "0.00010706712232851857\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "7.3620285814602305e-06\n",
            "7.3620285814602305e-06\n",
            "0.0010020918599712501\n",
            "0.0010020918599712501\n",
            "0.0027183815236850374\n",
            "0\n",
            "0\n",
            "2.6574329260425533e-12\n",
            "2.7634206476420083e-06\n",
            "2.7634206476420083e-06\n",
            "2.7634206476420083e-06\n",
            "2.7634206476420083e-06\n",
            "0.0002194275282104352\n",
            "0.0002194275282104352\n",
            "0.0002194275282104352\n",
            "0\n",
            "1.4019244864787801e-08\n",
            "1.4019244864787801e-08\n",
            "1.6157924892525446e-08\n",
            "1.6157924892525446e-08\n",
            "1.9952347913379958e-05\n",
            "4.740041291146112e-05\n",
            "4.740041291146112e-05\n",
            "4.740041291146112e-05\n",
            "0.0002612801515123691\n",
            "0\n",
            "0\n",
            "9.02786928187387e-09\n",
            "9.02786928187387e-09\n",
            "9.02786928187387e-09\n",
            "1.8834136306679718e-06\n",
            "1.8834136306679718e-06\n",
            "1.8834136306679718e-06\n",
            "1.8834136306679718e-06\n",
            "1.8834136306679718e-06\n",
            "0\n",
            "2.759760051838101e-08\n",
            "2.759760051838101e-08\n",
            "2.759760051838101e-08\n",
            "2.1075246334800274e-07\n",
            "1.694170531273063e-06\n",
            "1.694170531273063e-06\n",
            "1.694170531273063e-06\n",
            "3.397756218019463e-06\n",
            "3.397756218019463e-06\n",
            "0\n",
            "3.160424267976518e-08\n",
            "3.160424267976518e-08\n",
            "3.160424267976518e-08\n",
            "3.160424267976518e-08\n",
            "8.87326842391392e-06\n",
            "8.87326842391392e-06\n",
            "8.87326842391392e-06\n",
            "8.87326842391392e-06\n",
            "8.87326842391392e-06\n",
            "0\n",
            "2.9849493056494007e-07\n",
            "2.9849493056494007e-07\n",
            "4.34304383071668e-07\n",
            "4.952952509659699e-06\n",
            "4.952952509659699e-06\n",
            "4.952952509659699e-06\n",
            "4.952952509659699e-06\n",
            "4.952952509659699e-06\n",
            "4.952952509659699e-06\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1.6322740162757253e-06\n",
            "5.924158085570549e-06\n",
            "5.924158085570549e-06\n",
            "5.924158085570549e-06\n",
            "5.924158085570549e-06\n",
            "5.924158085570549e-06\n",
            "0\n",
            "4.142423123931173e-09\n",
            "7.359008340008966e-08\n",
            "7.359008340008966e-08\n",
            "7.359008340008966e-08\n",
            "3.9072691265534405e-05\n",
            "3.9072691265534405e-05\n",
            "3.9072691265534405e-05\n",
            "3.9072691265534405e-05\n",
            "3.9072691265534405e-05\n",
            "0\n",
            "1.094154552662657e-07\n",
            "4.4467391516289494e-06\n",
            "4.4467391516289494e-06\n",
            "4.4467391516289494e-06\n",
            "4.4814292945192194e-05\n",
            "4.4814292945192194e-05\n",
            "4.4814292945192194e-05\n",
            "4.4814292945192194e-05\n",
            "4.4814292945192194e-05\n",
            "0\n",
            "0\n",
            "0\n",
            "3.159419381367674e-12\n",
            "2.283725530072423e-08\n",
            "4.1288495368117733e-07\n",
            "4.1288495368117733e-07\n",
            "4.1288495368117733e-07\n",
            "4.1288495368117733e-07\n",
            "4.1288495368117733e-07\n",
            "0\n",
            "0\n",
            "0\n",
            "0.00017190418149278907\n",
            "0.00017190418149278907\n",
            "0.00017190418149278907\n",
            "0.006587844520338809\n",
            "0.006587844520338809\n",
            "0.006587844520338809\n",
            "0.006587844520338809\n",
            "0\n",
            "0.0005055821890444283\n",
            "0.0010333452704552784\n",
            "0.0010333452704552784\n",
            "0.0010333452704552784\n",
            "0.021425798604575722\n",
            "0.021425798604575722\n",
            "0.18859439697108848\n",
            "0.18859439697108848\n",
            "0.18859439697108848\n",
            "0\n",
            "0\n",
            "6.297386594620845e-10\n",
            "6.297386594620845e-10\n",
            "6.297386594620845e-10\n",
            "3.943831152097194e-08\n",
            "3.943831152097194e-08\n",
            "3.943831152097194e-08\n",
            "3.943831152097194e-08\n",
            "3.943831152097194e-08\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "2.0142499378836066e-09\n",
            "0.00039553691421215323\n",
            "0.00039553691421215323\n",
            "0.00039553691421215323\n",
            "0.00039553691421215323\n",
            "0.00039553691421215323\n",
            "0\n",
            "0\n",
            "1.021390551153049e-05\n",
            "1.021390551153049e-05\n",
            "1.021390551153049e-05\n",
            "2.753225050425861e-05\n",
            "3.561304437740893e-05\n",
            "3.561304437740893e-05\n",
            "3.561304437740893e-05\n",
            "3.561304437740893e-05\n",
            "0\n",
            "0\n",
            "2.408330336217956e-10\n",
            "2.408330336217956e-10\n",
            "2.408330336217956e-10\n",
            "1.5061917596917953e-06\n",
            "1.5061917596917953e-06\n",
            "1.5061917596917953e-06\n",
            "1.5061917596917953e-06\n",
            "1.5093395294937553e-05\n",
            "0\n",
            "0\n",
            "0.00302129300600313\n",
            "0.00302129300600313\n",
            "0.00302129300600313\n",
            "0.00302129300600313\n",
            "0.16725665053001246\n",
            "0.16725665053001246\n",
            "0.16725665053001246\n",
            "0.16725665053001246\n",
            "0\n",
            "2.1886561436333943e-08\n",
            "2.1886561436333943e-08\n",
            "1.999116711993488e-07\n",
            "1.999116711993488e-07\n",
            "1.999116711993488e-07\n",
            "3.604481065059227e-07\n",
            "3.604481065059227e-07\n",
            "1.7172458873241248e-05\n",
            "1.7172458873241248e-05\n",
            "0\n",
            "0\n",
            "0\n",
            "6.407118609108596e-08\n",
            "6.407118609108596e-08\n",
            "6.407118609108596e-08\n",
            "6.407118609108596e-08\n",
            "6.407118609108596e-08\n",
            "6.407118609108596e-08\n",
            "6.407118609108596e-08\n",
            "0\n",
            "1.1017664992220172e-08\n",
            "1.5686061763011906e-07\n",
            "1.5686061763011906e-07\n",
            "1.5686061763011906e-07\n",
            "1.5686061763011906e-07\n",
            "1.5686061763011906e-07\n",
            "1.5686061763011906e-07\n",
            "1.5686061763011906e-07\n",
            "1.5686061763011906e-07\n",
            "0\n",
            "0\n",
            "0\n",
            "6.643268578540626e-09\n",
            "6.643268578540626e-09\n",
            "6.643268578540626e-09\n",
            "6.643268578540626e-09\n",
            "6.643268578540626e-09\n",
            "7.017533517975688e-09\n",
            "7.017533517975688e-09\n",
            "0\n",
            "3.076186219564367e-10\n",
            "3.076186219564367e-10\n",
            "3.076186219564367e-10\n",
            "3.076186219564367e-10\n",
            "2.1885615335793281e-07\n",
            "1.562895223747177e-05\n",
            "1.562895223747177e-05\n",
            "1.562895223747177e-05\n",
            "1.562895223747177e-05\n",
            "0\n",
            "0\n",
            "0\n",
            "0.0003189473614316999\n",
            "0.0003189473614316999\n",
            "0.001567192181881191\n",
            "0.009236978914828562\n",
            "0.009236978914828562\n",
            "0.009236978914828562\n",
            "0.009236978914828562\n",
            "0\n",
            "2.436582471393046e-10\n",
            "6.75914561157957e-06\n",
            "6.75914561157957e-06\n",
            "6.75914561157957e-06\n",
            "6.75914561157957e-06\n",
            "6.75914561157957e-06\n",
            "6.75914561157957e-06\n",
            "6.75914561157957e-06\n",
            "3.698435206007581e-05\n",
            "0\n",
            "0\n",
            "0\n",
            "0.2001267490077088\n",
            "0.2001267490077088\n",
            "0.2001267490077088\n",
            "0.2001267490077088\n",
            "0.2001267490077088\n",
            "0.2001267490077088\n",
            "0.3586191509207494\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "2.0622374332946292e-08\n",
            "1.8128635178871384e-06\n",
            "1.8128635178871384e-06\n",
            "1.8128635178871384e-06\n",
            "1.8128635178871384e-06\n",
            "1.0939726413018507e-05\n",
            "0\n",
            "0\n",
            "0\n",
            "3.8148420007109476e-05\n",
            "3.8148420007109476e-05\n",
            "3.8148420007109476e-05\n",
            "3.8148420007109476e-05\n",
            "3.8148420007109476e-05\n",
            "3.8148420007109476e-05\n",
            "3.8148420007109476e-05\n",
            "0\n",
            "0\n",
            "0\n",
            "2.1345118097041165e-07\n",
            "2.1345118097041165e-07\n",
            "2.1345118097041165e-07\n",
            "2.1345118097041165e-07\n",
            "2.1345118097041165e-07\n",
            "2.1345118097041165e-07\n",
            "1.0831411133786864e-06\n",
            "0\n",
            "0\n",
            "4.2739008477860604e-08\n",
            "1.8033210996327047e-05\n",
            "1.8033210996327047e-05\n",
            "1.8033210996327047e-05\n",
            "0.00013869426283793566\n",
            "0.00013869426283793566\n",
            "0.00013869426283793566\n",
            "0.0003097331674854871\n",
            "0\n",
            "2.2142960995000299e-07\n",
            "2.2142960995000299e-07\n",
            "3.263598096779203e-07\n",
            "5.304111283302661e-06\n",
            "5.304111283302661e-06\n",
            "5.304111283302661e-06\n",
            "5.304111283302661e-06\n",
            "5.304111283302661e-06\n",
            "5.304111283302661e-06\n",
            "0\n",
            "0\n",
            "0\n",
            "8.587224565174636e-08\n",
            "5.2759721522718756e-05\n",
            "5.2759721522718756e-05\n",
            "5.2759721522718756e-05\n",
            "5.2759721522718756e-05\n",
            "5.2759721522718756e-05\n",
            "5.2759721522718756e-05\n",
            "0\n",
            "0\n",
            "7.699950942786876e-12\n",
            "7.699950942786876e-12\n",
            "8.36703191214998e-07\n",
            "8.36703191214998e-07\n",
            "8.36703191214998e-07\n",
            "8.36703191214998e-07\n",
            "8.36703191214998e-07\n",
            "8.36703191214998e-07\n",
            "0\n",
            "0\n",
            "6.0814959791788795e-05\n",
            "6.0814959791788795e-05\n",
            "6.0814959791788795e-05\n",
            "6.0814959791788795e-05\n",
            "6.0814959791788795e-05\n",
            "0.0004036999040358186\n",
            "0.0004036999040358186\n",
            "0.0004036999040358186\n",
            "0\n",
            "0\n",
            "0\n",
            "2.5034010148264853e-07\n",
            "2.5034010148264853e-07\n",
            "1.054419487486516e-05\n",
            "1.054419487486516e-05\n",
            "8.578965244774729e-05\n",
            "8.578965244774729e-05\n",
            "0.0016729385655619787\n",
            "0\n",
            "2.769913169940868e-10\n",
            "2.769913169940868e-10\n",
            "2.769913169940868e-10\n",
            "2.143897964966146e-09\n",
            "5.0700007819057175e-05\n",
            "5.0700007819057175e-05\n",
            "5.0700007819057175e-05\n",
            "5.0700007819057175e-05\n",
            "5.0700007819057175e-05\n",
            "0\n",
            "0.0001470779988923988\n",
            "0.0006068940714249354\n",
            "0.0006068940714249354\n",
            "0.0006068940714249354\n",
            "0.0006068940714249354\n",
            "0.0006068940714249354\n",
            "0.0006068940714249354\n",
            "0.020452860240948056\n",
            "0.020452860240948056\n",
            "0\n",
            "0\n",
            "1.236480977640694e-09\n",
            "1.236480977640694e-09\n",
            "1.236480977640694e-09\n",
            "1.5839413857447885e-07\n",
            "1.5839413857447885e-07\n",
            "1.5197171684532788e-05\n",
            "1.5197171684532788e-05\n",
            "1.5197171684532788e-05\n",
            "0\n",
            "4.5534779318318964e-10\n",
            "5.45298426979724e-06\n",
            "5.45298426979724e-06\n",
            "5.45298426979724e-06\n",
            "5.45298426979724e-06\n",
            "5.45298426979724e-06\n",
            "1.1606434324744146e-05\n",
            "1.1606434324744146e-05\n",
            "1.1606434324744146e-05\n",
            "0\n",
            "0\n",
            "2.31086288266035e-05\n",
            "0.00014838588155094418\n",
            "0.00014838588155094418\n",
            "0.00014838588155094418\n",
            "0.00014838588155094418\n",
            "0.00014838588155094418\n",
            "0.00014838588155094418\n",
            "0.00014838588155094418\n",
            "0\n",
            "4.868356108278216e-10\n",
            "4.868356108278216e-10\n",
            "4.868356108278216e-10\n",
            "4.868356108278216e-10\n",
            "6.879144733786555e-08\n",
            "6.879144733786555e-08\n",
            "6.879144733786555e-08\n",
            "6.879144733786555e-08\n",
            "6.879144733786555e-08\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "9.51712482341628e-12\n",
            "5.219145212022366e-07\n",
            "5.219145212022366e-07\n",
            "5.219145212022366e-07\n",
            "5.219145212022366e-07\n",
            "5.219145212022366e-07\n",
            "0\n",
            "5.6284537156159833e-08\n",
            "5.6284537156159833e-08\n",
            "7.440778689294933e-06\n",
            "7.440778689294933e-06\n",
            "7.440778689294933e-06\n",
            "7.440778689294933e-06\n",
            "0.00023679369895364475\n",
            "0.00023679369895364475\n",
            "0.0015812482549790632\n",
            "0\n",
            "0\n",
            "2.459503249898889e-07\n",
            "5.377005910219918e-05\n",
            "5.377005910219918e-05\n",
            "5.377005910219918e-05\n",
            "5.377005910219918e-05\n",
            "5.377005910219918e-05\n",
            "5.377005910219918e-05\n",
            "5.377005910219918e-05\n",
            "0\n",
            "0\n",
            "5.33662022961973e-05\n",
            "5.33662022961973e-05\n",
            "5.33662022961973e-05\n",
            "8.570179000164156e-05\n",
            "8.570179000164156e-05\n",
            "8.570179000164156e-05\n",
            "8.570179000164156e-05\n",
            "8.570179000164156e-05\n",
            "0\n",
            "0\n",
            "4.451125038047208e-06\n",
            "4.451125038047208e-06\n",
            "4.451125038047208e-06\n",
            "4.451125038047208e-06\n",
            "4.451125038047208e-06\n",
            "4.451125038047208e-06\n",
            "4.451125038047208e-06\n",
            "4.451125038047208e-06\n",
            "0\n",
            "3.285659160277227e-08\n",
            "3.285659160277227e-08\n",
            "3.285659160277227e-08\n",
            "1.7867581790201806e-06\n",
            "1.7867581790201806e-06\n",
            "4.8969424562216435e-06\n",
            "4.8969424562216435e-06\n",
            "0.000329283940973123\n",
            "0.000329283940973123\n",
            "0\n",
            "0\n",
            "2.083025060366697e-06\n",
            "8.569433279990974e-05\n",
            "0.00014929883757402052\n",
            "0.00014929883757402052\n",
            "0.00014929883757402052\n",
            "0.00014929883757402052\n",
            "0.002384856249194495\n",
            "0.002384856249194495\n",
            "0\n",
            "6.198914655299987e-08\n",
            "7.108757912944045e-05\n",
            "7.108757912944045e-05\n",
            "7.108757912944045e-05\n",
            "7.108757912944045e-05\n",
            "0.00026119326986459714\n",
            "0.00039933972455276356\n",
            "0.00039933972455276356\n",
            "0.00039933972455276356\n",
            "0\n",
            "0\n",
            "0\n",
            "0.01226355180103041\n",
            "0.01226355180103041\n",
            "0.01226355180103041\n",
            "0.01226355180103041\n",
            "0.01226355180103041\n",
            "0.01226355180103041\n",
            "0.01226355180103041\n",
            "0\n",
            "0\n",
            "5.618697643096103e-07\n",
            "6.501314245905968e-05\n",
            "6.501314245905968e-05\n",
            "6.501314245905968e-05\n",
            "6.501314245905968e-05\n",
            "6.501314245905968e-05\n",
            "6.501314245905968e-05\n",
            "6.501314245905968e-05\n",
            "0\n",
            "4.67231043386669e-06\n",
            "0.0011941064262996994\n",
            "0.0011941064262996994\n",
            "0.0011941064262996994\n",
            "0.015624276978440323\n",
            "0.015624276978440323\n",
            "0.015624276978440323\n",
            "0.03980569867908864\n",
            "0.12128367139592278\n",
            "0\n",
            "0\n",
            "0\n",
            "7.146150686790617e-07\n",
            "7.146150686790617e-07\n",
            "7.146150686790617e-07\n",
            "7.146150686790617e-07\n",
            "7.146150686790617e-07\n",
            "7.146150686790617e-07\n",
            "7.146150686790617e-07\n",
            "0\n",
            "0\n",
            "6.811378632470388e-07\n",
            "0.0013355396776728009\n",
            "0.0013355396776728009\n",
            "0.0013355396776728009\n",
            "0.0013355396776728009\n",
            "0.0013355396776728009\n",
            "0.006516760396835837\n",
            "0.006516760396835837\n",
            "0\n",
            "3.967188896712677e-08\n",
            "3.967188896712677e-08\n",
            "6.112750451276397e-07\n",
            "6.112750451276397e-07\n",
            "3.0677709342985816e-05\n",
            "3.9044832821943835e-05\n",
            "3.9044832821943835e-05\n",
            "0.00033376563345605386\n",
            "0.1372547643034678\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1.7233594377817091e-06\n",
            "1.7233594377817091e-06\n",
            "2.6517875359045588e-05\n",
            "2.6517875359045588e-05\n",
            "2.6517875359045588e-05\n",
            "0\n",
            "5.341553092927397e-08\n",
            "5.341553092927397e-08\n",
            "5.341553092927397e-08\n",
            "5.341553092927397e-08\n",
            "5.341553092927397e-08\n",
            "5.341553092927397e-08\n",
            "5.341553092927397e-08\n",
            "1.0558334687051346e-06\n",
            "1.0558334687051346e-06\n",
            "0\n",
            "2.7956873763407976e-09\n",
            "2.2735357666428386e-06\n",
            "2.2735357666428386e-06\n",
            "2.2735357666428386e-06\n",
            "2.2735357666428386e-06\n",
            "2.2735357666428386e-06\n",
            "2.2735357666428386e-06\n",
            "2.2735357666428386e-06\n",
            "2.2735357666428386e-06\n",
            "0\n",
            "0\n",
            "0\n",
            "4.595546050079506e-06\n",
            "4.595546050079506e-06\n",
            "4.595546050079506e-06\n",
            "4.595546050079506e-06\n",
            "4.595546050079506e-06\n",
            "0.0037476286096134867\n",
            "0.0037476286096134867\n",
            "0\n",
            "0\n",
            "0.0002465020496933288\n",
            "0.0002465020496933288\n",
            "0.0002465020496933288\n",
            "0.0002465020496933288\n",
            "0.001024767750165554\n",
            "0.001024767750165554\n",
            "0.001024767750165554\n",
            "0.001024767750165554\n",
            "0\n",
            "7.050201434229013e-08\n",
            "7.050201434229013e-08\n",
            "7.050201434229013e-08\n",
            "8.748060046368714e-06\n",
            "8.748060046368714e-06\n",
            "8.748060046368714e-06\n",
            "8.748060046368714e-06\n",
            "8.748060046368714e-06\n",
            "8.748060046368714e-06\n",
            "0\n",
            "0\n",
            "0\n",
            "2.5888860631688914e-07\n",
            "7.832936750812544e-07\n",
            "3.109885489197288e-05\n",
            "3.109885489197288e-05\n",
            "3.109885489197288e-05\n",
            "3.109885489197288e-05\n",
            "3.109885489197288e-05\n",
            "0\n",
            "4.603241749942441e-07\n",
            "4.603241749942441e-07\n",
            "4.603241749942441e-07\n",
            "2.9826337981308116e-06\n",
            "2.9826337981308116e-06\n",
            "2.9826337981308116e-06\n",
            "2.9826337981308116e-06\n",
            "2.9826337981308116e-06\n",
            "2.9826337981308116e-06\n",
            "0\n",
            "0\n",
            "0\n",
            "4.063087508981845e-08\n",
            "4.063087508981845e-08\n",
            "2.887904686527325e-07\n",
            "2.887904686527325e-07\n",
            "2.887904686527325e-07\n",
            "2.887904686527325e-07\n",
            "2.887904686527325e-07\n",
            "0\n",
            "0\n",
            "0\n",
            "2.161528870952661e-06\n",
            "2.161528870952661e-06\n",
            "3.2225001238378983e-06\n",
            "3.2225001238378983e-06\n",
            "3.2225001238378983e-06\n",
            "1.2292957546337958e-05\n",
            "1.2292957546337958e-05\n",
            "0\n",
            "4.419127519170705e-05\n",
            "0.00028522414322841626\n",
            "0.00028522414322841626\n",
            "0.00028522414322841626\n",
            "0.00028522414322841626\n",
            "0.00028522414322841626\n",
            "0.0049979661153049925\n",
            "0.0049979661153049925\n",
            "0.0049979661153049925\n",
            "0\n",
            "0\n",
            "4.4958894387900594e-06\n",
            "9.886217224608764e-06\n",
            "9.886217224608764e-06\n",
            "1.3834781923834532e-05\n",
            "1.3834781923834532e-05\n",
            "0.016583431194230957\n",
            "0.016583431194230957\n",
            "0.016583431194230957\n",
            "0\n",
            "0\n",
            "6.79453893562185e-06\n",
            "9.696721083998452e-06\n",
            "9.696721083998452e-06\n",
            "9.696721083998452e-06\n",
            "9.696721083998452e-06\n",
            "0.00025032576764998695\n",
            "0.00025032576764998695\n",
            "0.00025032576764998695\n",
            "0\n",
            "1.4547615357989058e-11\n",
            "1.4547615357989058e-11\n",
            "1.9479073872606856e-07\n",
            "1.9479073872606856e-07\n",
            "1.9479073872606856e-07\n",
            "1.9479073872606856e-07\n",
            "1.9479073872606856e-07\n",
            "1.9479073872606856e-07\n",
            "1.9479073872606856e-07\n",
            "0\n",
            "0\n",
            "0.00018849542015039134\n",
            "0.00018849542015039134\n",
            "0.00018849542015039134\n",
            "0.00018849542015039134\n",
            "0.00018849542015039134\n",
            "0.00018849542015039134\n",
            "0.00018849542015039134\n",
            "0.00018849542015039134\n",
            "0\n",
            "0\n",
            "3.2299101026429456e-06\n",
            "6.79599852840292e-05\n",
            "6.79599852840292e-05\n",
            "6.79599852840292e-05\n",
            "6.79599852840292e-05\n",
            "6.79599852840292e-05\n",
            "6.79599852840292e-05\n",
            "6.79599852840292e-05\n"
          ]
        }
      ],
      "source": [
        "# Finding the second best prediction:\n",
        "def second_best(p):\n",
        "\n",
        "  for i in range(len(p)):\n",
        "    first = 0\n",
        "    second = 0\n",
        "    \n",
        "    for j in range(len(p[i])):\n",
        "      if p[i][j] > first:\n",
        "        first = p[i][j]\n",
        "\n",
        "      if p[i][j] > second and p[i][j] != first:\n",
        "        second = p[i][j]\n",
        "\n",
        "      print(second)\n",
        "\n",
        "second_best(probability_predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "eJRo9M4H8fu-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "272ae562-6ab9-456e-b268-53107efd3176"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of prediction:\n",
            "(10000,)\n",
            "==========================\n",
            "Probability of predictions:\n",
            "(10000, 10)\n",
            "==========================\n",
            "First 3 predictions:\n",
            "[0 9 6]\n",
            "==========================\n",
            "First probability predictions:\n",
            "[8.93506856e-01 5.12124432e-03 4.44354289e-04 2.56825568e-06\n",
            " 3.55837000e-07 9.30964953e-02 4.88407751e-04 1.72358476e-04\n",
            " 7.16446826e-03 2.89193352e-06]\n",
            "==========================\n",
            "Second probability predictions:\n",
            "[1.19671628e-08 6.63172938e-09 4.50602950e-08 4.17303612e-06\n",
            " 1.92386090e-04 1.58056688e-07 2.05624373e-09 4.34669518e-06\n",
            " 1.86544183e-07 9.99798684e-01]\n",
            "==========================\n",
            "Third probability predictions:\n",
            "[8.35642722e-04 7.99829621e-08 2.88228624e-06 4.29855046e-09\n",
            " 4.74490226e-05 2.48949918e-09 9.99109566e-01 3.88456824e-06\n",
            " 5.37877704e-08 4.35203786e-07]\n"
          ]
        }
      ],
      "source": [
        "# Checking predictions and probabilities\n",
        "print('Shape of prediction:')\n",
        "print(predictions.shape)\n",
        "print(\"==========================\")\n",
        "print('Probability of predictions:')\n",
        "print(probability_predictions.shape)\n",
        "print(\"==========================\")\n",
        "print('First 3 predictions:')\n",
        "print(predictions[0:3])\n",
        "print(\"==========================\")\n",
        "print('First probability predictions:')\n",
        "print(probability_predictions[0])\n",
        "print(\"==========================\")\n",
        "print('Second probability predictions:')\n",
        "print(probability_predictions[1])\n",
        "print(\"==========================\")\n",
        "print('Third probability predictions:')\n",
        "print(probability_predictions[2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "lAER0shb8s3i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de801e3c-51f2-4886-f844-99e7b32886b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question 0-1 answered:\n",
            "==========================\n",
            "Labels of points with second highest probability:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5, 4, 0, ..., 3, 1, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "# Each label in \"predictions\" is the index of the highest value in \"probability_predictions\"\n",
        "# In order to obtain the label of the second highest value, we must call the index\n",
        "# Question 0-1 (cont'd) here:\n",
        "p = probability_predictions\n",
        "\n",
        "results = []\n",
        "\n",
        "for i in range(len(p)):\n",
        "    \n",
        "    first = 0\n",
        "    second = 0\n",
        "    \n",
        "    for j in range(len(p[i])):\n",
        "        if p[i][j] > first:\n",
        "            first = p[i][j]\n",
        "            \n",
        "    for j in range(len(p[i])):\n",
        "        if p[i][j] > second and p[i][j] != first:\n",
        "            second = p[i][j]\n",
        "            \n",
        "    q = np.where(p[i] == second)[0][0]\n",
        "    results.append(q)\n",
        "\n",
        "second_predictions = np.array(results)\n",
        "\n",
        "print(\"Question 0-1 answered:\")\n",
        "print(\"==========================\")\n",
        "print(\"Labels of points with second highest probability:\")\n",
        "second_predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "d3ZqtfJTI1qq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e785651e-ac45-465e-d15d-a0f080f49c43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model evaluation and metrics report:\n",
            "==========================\n",
            "[[ 973    0    2    0    0    0    4    0    3    4]\n",
            " [   0 1113    3    2    1    1    0    2    1    2]\n",
            " [   3    4  973    4    1    2    2    3    6    1]\n",
            " [   4    0   11  969    0   16    1    5    8    6]\n",
            " [   1    1    5    0  936    0   10    4    3   15]\n",
            " [   3    0    2   13    2  864    2    2    7    7]\n",
            " [   7    1    3    0    1    5  959    0    6    0]\n",
            " [   3    2    9    3    3    1    0 1008    1   12]\n",
            " [   5    3    3    8    4    6    4    1  941    0]\n",
            " [   6    1    1    5   17    3    1    7    7  946]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.99      0.98       986\n",
            "           1       0.99      0.99      0.99      1125\n",
            "           2       0.96      0.97      0.97       999\n",
            "           3       0.97      0.95      0.96      1020\n",
            "           4       0.97      0.96      0.96       975\n",
            "           5       0.96      0.96      0.96       902\n",
            "           6       0.98      0.98      0.98       982\n",
            "           7       0.98      0.97      0.97      1042\n",
            "           8       0.96      0.97      0.96       975\n",
            "           9       0.95      0.95      0.95       994\n",
            "\n",
            "    accuracy                           0.97     10000\n",
            "   macro avg       0.97      0.97      0.97     10000\n",
            "weighted avg       0.97      0.97      0.97     10000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Question 0-1 (cont'd) here:\n",
        "# Model evaluation and metrics\n",
        "print(\"Model evaluation and metrics report:\")\n",
        "print(\"==========================\")\n",
        "print(confusion_matrix(y_test,predictions))\n",
        "print(classification_report(y_test,predictions))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7bbDT1CI1qq"
      },
      "source": [
        "**Q0-2**: For each data point misclassified by the model, find if the second best prediction is actually the correct label. Calculate the percentage of missclassified points, that are correctly classified based on the second best prediction. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "CVEQ7pLPI1qq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0aeb4882-e1e5-4a68-df26-8bf3acc38670"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5 9 6 ... 2 7 1]\n"
          ]
        }
      ],
      "source": [
        "# Question 0-2 (answer) here:\n",
        "# Checking labels of actual points\n",
        "y_test_actual = np.asarray(y_test)\n",
        "print(y_test_actual)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "6ojUJjCY9JLJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa17622c-710c-48ba-c8dd-db749f1d8fca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 9 6 ... 2 7 1]\n"
          ]
        }
      ],
      "source": [
        "# Question 0-2 (cont'd) here:\n",
        "# Predicted labels of points with highest probability\n",
        "print(predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "2WGA5fP79MG0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82115f5e-ac27-4c5e-aad9-14d66196768c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(array([   0,   34,   36,   63,   82,   93,   98,  102,  110,  119,  126,\n",
            "        136,  179,  241,  279,  472,  512,  515,  546,  591,  662,  683,\n",
            "        765,  772,  776,  783,  784,  819,  824,  837,  840,  875,  935,\n",
            "       1010, 1011, 1053, 1113, 1130, 1176, 1191, 1257, 1325, 1331, 1358,\n",
            "       1367, 1405, 1429, 1432, 1461, 1514, 1583, 1588, 1591, 1592, 1608,\n",
            "       1618, 1636, 1772, 1783, 1820, 1833, 1933, 1949, 1979, 1986, 2048,\n",
            "       2063, 2102, 2108, 2109, 2148, 2156, 2170, 2213, 2220, 2225, 2234,\n",
            "       2239, 2250, 2259, 2313, 2315, 2343, 2446, 2498, 2500, 2520, 2570,\n",
            "       2650, 2716, 2718, 2726, 2748, 2831, 2881, 2882, 2889, 2893, 2963,\n",
            "       2973, 3040, 3064, 3124, 3152, 3177, 3182, 3242, 3271, 3327, 3438,\n",
            "       3453, 3466, 3481, 3495, 3501, 3503, 3513, 3535, 3599, 3601, 3656,\n",
            "       3701, 3733, 3807, 3815, 3915, 3931, 4016, 4062, 4136, 4149, 4201,\n",
            "       4225, 4249, 4279, 4283, 4353, 4359, 4400, 4408, 4410, 4416, 4456,\n",
            "       4551, 4566, 4672, 4690, 4742, 4772, 4808, 4819, 4830, 4933, 4955,\n",
            "       4967, 4977, 5032, 5076, 5195, 5204, 5214, 5232, 5239, 5324, 5343,\n",
            "       5387, 5493, 5494, 5510, 5529, 5564, 5569, 5598, 5606, 5611, 5628,\n",
            "       5716, 5728, 5754, 5773, 5778, 5856, 5870, 5881, 5902, 5919, 5923,\n",
            "       5980, 6033, 6069, 6103, 6126, 6140, 6157, 6209, 6282, 6303, 6342,\n",
            "       6346, 6385, 6448, 6477, 6504, 6528, 6537, 6542, 6552, 6558, 6592,\n",
            "       6614, 6622, 6629, 6689, 6724, 6742, 6777, 6831, 6859, 6862, 6883,\n",
            "       6934, 7031, 7037, 7058, 7105, 7108, 7117, 7175, 7190, 7225, 7254,\n",
            "       7256, 7299, 7332, 7338, 7370, 7373, 7375, 7393, 7405, 7407, 7427,\n",
            "       7483, 7496, 7528, 7532, 7534, 7585, 7614, 7617, 7740, 7742, 7851,\n",
            "       7857, 7887, 7921, 7950, 7972, 7988, 7992, 7999, 8101, 8115, 8130,\n",
            "       8142, 8160, 8169, 8202, 8266, 8319, 8335, 8338, 8354, 8385, 8402,\n",
            "       8457, 8503, 8536, 8595, 8596, 8622, 8625, 8688, 8697, 8737, 8761,\n",
            "       8789, 8861, 8952, 8988, 9006, 9042, 9084, 9103, 9142, 9159, 9171,\n",
            "       9216, 9222, 9225, 9251, 9288, 9306, 9358, 9368, 9375, 9378, 9432,\n",
            "       9441, 9471, 9539, 9546, 9590, 9705, 9760, 9770, 9817, 9881]),)\n"
          ]
        }
      ],
      "source": [
        "# Question 0-2 (cont'd) here:\n",
        "# Index for misclassified points\n",
        "misclassified = np.where(y_test_actual != predictions)\n",
        "print(misclassified)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "iiBRPR1X9OLb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53b85ad4-e03e-47c0-b2f4-efe14861d2df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5 3 5 1 8 2 4 5 5 4 5 8 5 4 9 3 6 9 6 2 7 9 8 8 7 5 3 9 3 9 7 4 9 7 9 3 2\n",
            " 8 2 4 4 8 7 8 7 3 7 9 5 9 7 8 8 4 7 5 3 7 5 2 5 8 4 9 5 0 3 6 9 5 3 8 6 4\n",
            " 5 3 3 3 7 4 8 3 6 6 9 4 3 4 4 3 7 2 4 3 6 5 8 2 5 9 9 0 5 7 8 0 4 3 6 1 0\n",
            " 2 2 3 2 5 8 6 5 3 7 9 4 8 6 5 1 4 3 2 3 9 7 6 2 9 9 8 6 4 4 1 4 5 9 8 7 2\n",
            " 9 3 5 0 6 9 9 0 5 3 4 8 8 2 4 5 3 9 7 7 9 8 1 8 5 3 4 9 3 1 2 9 7 3 9 3 2\n",
            " 8 0 9 7 3 9 4 9 5 6 1 9 2 4 8 4 7 9 8 5 7 4 8 3 9 6 9 6 6 5 2 7 9 7 6 0 8\n",
            " 2 9 8 3 9 4 8 3 3 2 2 9 9 5 7 3 5 0 3 3 3 7 1 2 3 3 4 3 1 5 5 7 3 6 3 0 0\n",
            " 9 3 9 4 5 4 3 8 9 9 3 5 2 9 8 9 7 3 7 4 1 4 6 4 8 3 9 4 0 5 4 6 3 7 5 1 3\n",
            " 9 5 7 8 7 8 0 5 9 1 3 2 7 4 6 2 7 6 4 2 3 4]\n"
          ]
        }
      ],
      "source": [
        "# Question 0-2 (cont'd) here:\n",
        "# Actual labels where predictions misclassified\n",
        "mis_actual = [y_test_actual[index] for index in misclassified]\n",
        "mis_actual = np.array(mis_actual[0])\n",
        "print(mis_actual)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "KwCgwz1K9OHp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9919bbf-36e9-4c5c-9281-221723322eee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5 9 5 1 7 2 4 5 0 9 5 8 7 6 9 3 5 9 6 2 7 9 8 8 7 8 3 9 3 2 7 4 9 7 9 3 2\n",
            " 8 2 4 7 8 3 8 7 3 6 9 5 9 7 8 3 4 7 5 5 7 5 2 7 8 9 8 5 3 3 5 9 5 1 8 7 0\n",
            " 5 3 0 5 7 4 8 5 5 4 9 4 3 4 4 3 7 2 4 3 1 5 8 2 5 9 9 9 5 7 8 0 2 7 5 1 0\n",
            " 2 8 3 2 2 8 6 5 3 7 8 4 8 6 6 1 4 3 2 3 2 1 6 2 9 7 8 6 4 4 1 6 5 0 8 7 8\n",
            " 3 3 3 0 9 9 9 0 7 3 4 8 8 2 0 5 3 9 7 7 8 8 1 0 8 5 4 9 7 9 2 9 7 8 0 3 2\n",
            " 8 0 9 7 3 0 4 6 5 6 4 9 2 9 8 4 8 9 8 7 7 0 6 6 9 6 9 2 6 6 2 7 8 7 6 0 8\n",
            " 2 9 8 3 9 7 8 9 9 2 2 9 9 5 7 1 5 5 3 3 3 7 3 8 5 3 4 3 1 5 6 7 3 6 3 6 9\n",
            " 9 3 9 7 6 4 3 8 7 9 3 5 2 9 8 9 7 2 5 4 6 4 6 4 8 4 9 4 0 5 4 6 8 2 9 2 3\n",
            " 7 3 4 3 2 2 0 5 9 1 3 8 7 4 5 2 7 6 4 2 3 7]\n"
          ]
        }
      ],
      "source": [
        "# Question 0-2 (cont'd) here:\n",
        "# Second highest prediction labels\n",
        "mis_predictions = [second_predictions[index] for index in misclassified]\n",
        "mis_predictions = np.array(mis_predictions[0])\n",
        "print(mis_predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "4358lRaI9WGl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0fa9f92-d853-489f-8ff6-ac2eaeb55dd7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(array([  0,   2,   3,   5,   6,   7,  10,  11,  14,  15,  17,  18,  19,\n",
            "        20,  21,  22,  23,  24,  26,  27,  28,  30,  31,  32,  33,  34,\n",
            "        35,  36,  37,  38,  39,  41,  43,  44,  45,  47,  48,  49,  50,\n",
            "        51,  53,  54,  55,  57,  58,  59,  61,  64,  66,  68,  69,  71,\n",
            "        74,  75,  78,  79,  80,  84,  85,  86,  87,  88,  89,  90,  91,\n",
            "        92,  93,  95,  96,  97,  98,  99, 100, 102, 103, 104, 105, 109,\n",
            "       110, 111, 113, 114, 116, 117, 118, 119, 120, 122, 123, 124, 126,\n",
            "       127, 128, 129, 130, 133, 134, 135, 137, 138, 139, 140, 141, 143,\n",
            "       145, 146, 149, 151, 153, 154, 155, 157, 158, 159, 160, 161, 163,\n",
            "       164, 165, 166, 167, 169, 170, 174, 175, 178, 179, 180, 183, 184,\n",
            "       185, 186, 187, 188, 189, 191, 193, 194, 196, 197, 199, 200, 202,\n",
            "       203, 205, 209, 210, 211, 213, 215, 216, 218, 219, 220, 221, 222,\n",
            "       223, 224, 225, 226, 228, 231, 232, 233, 234, 235, 236, 238, 240,\n",
            "       241, 242, 243, 247, 248, 249, 250, 251, 253, 254, 255, 256, 259,\n",
            "       260, 261, 264, 265, 266, 268, 269, 270, 271, 272, 273, 274, 275,\n",
            "       278, 280, 281, 282, 283, 285, 286, 287, 288, 289, 290, 295, 302,\n",
            "       303, 304, 305, 306, 308, 309, 311, 312, 313, 314, 315, 316]),)\n"
          ]
        }
      ],
      "source": [
        "# Question 0-2 (cont'd) here:\n",
        "# Indices for misclassified points classified correctly by second best prediction\n",
        "second_misclassified = np.where(mis_actual == mis_predictions)\n",
        "print(second_misclassified)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "tVmLekPqaacB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65177404-4814-4d20-bfa3-360be9a39145"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(318,)\n",
            "(10000,)\n",
            "662\n"
          ]
        }
      ],
      "source": [
        "# Question 0-2 (cont'd) here:\n",
        "# Checking the index of a misclassified sample\n",
        "idx = np.where(predictions != y_test_actual)\n",
        "idx = idx[0]\n",
        "print(idx.shape)\n",
        "print(predictions.shape)\n",
        "\n",
        "\n",
        "print(idx[20])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "UqRfC2joa2mR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5920236d-727f-44cb-c532-7e0e1d16d272"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5\n",
            "3\n",
            "[9.44944042e-04 3.40060187e-04 1.02193454e-03 6.70980245e-01\n",
            " 2.64505545e-03 4.00218507e-02 9.44401066e-07 2.82513486e-01\n",
            " 2.19585441e-04 1.31189448e-03]\n",
            "==========================\n",
            "We are confusing the correct label 5, with the label 3\n"
          ]
        }
      ],
      "source": [
        "# Question 0-2 (cont'd) here:\n",
        "# Checking the correct label of a mis-classified sample\n",
        "print(y_test_actual[idx[12]])\n",
        "print(predictions[idx[12]])\n",
        "\n",
        "print(probability_predictions[idx[12]])\n",
        "print(\"==========================\")\n",
        "print('We are confusing the correct label 5, with the label 3')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "zjyblekC9axp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce9f0dc7-1eda-4ace-c5d7-84d8cc1388c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question 0-2 answered:\n",
            "==========================\n",
            "Test size: (10000,)\n",
            "Number of points classified correctly: 9682\n",
            "Number of points classified incorrectly: 318\n",
            "Number of misclassified points where second best prediction is actually correct: 220\n",
            "Percentage of misclassified points correctly classified based on the second best prediction: 69.18%\n"
          ]
        }
      ],
      "source": [
        "# Question 0-2 (cont'd) here:\n",
        "# Analyzing the test data and performance\n",
        "print(\"Question 0-2 answered:\")\n",
        "print(\"==========================\")\n",
        "\n",
        "print('Test size:' , y_test_actual.shape)\n",
        "print('Number of points classified correctly:', len(y_test_actual) - len(misclassified[0]))\n",
        "print('Number of points classified incorrectly:', len(misclassified[0]))\n",
        "print('Number of misclassified points where second best prediction is actually correct:', len(second_misclassified[0]))\n",
        "\n",
        "correct = len(second_misclassified[0]) / len(misclassified[0])\n",
        "print('Percentage of misclassified points correctly classified based on the second best prediction: %.2f%%' % (correct * 100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HyKWPye99dqz"
      },
      "source": [
        "** Question 0-2 (cont'd) here:**</n>\n",
        "\n",
        "<n>*Comments:*</n>\n",
        "- <n>It is very suprising that only 318 points were misclassified. Of the 10,000 testing samples, **9,682** labels were correclty classified, giving an accuracy of **96.82%**.</n>\n",
        "\n",
        "- <n>The number of misclassified points where second highest probabilites were misclassified dropped to **220**. However, the percentage of misclassified points based on second highest prediction is **69.18%**.</n>\n",
        "\n",
        "- <n>This suggests that the classifier is performing well, but can be overlooked, due to testing sample size.</n>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 0-2 (cont'd) here:\n",
        "# Verifying test accuracy\n",
        "\n",
        "y_test_pred = predictions\n",
        "acc = (np.sum(y_test == y_test_pred)\n",
        "       .astype(np.float) / X_test.shape[0])\n",
        "\n",
        "print('Test accuracy: %.2f%%' % (acc * 100))\n",
        "print(\"==========================\")"
      ],
      "metadata": {
        "id": "hREJCcWGxf6y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39c5f830-329b-4d96-a873-f83e9f748d50"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 96.82%\n",
            "==========================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 0-2 (cont'd) here:\n",
        "# Visualizing the data point images\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "X_train = X_train.to_numpy()\n",
        "X_test = X_test.to_numpy()\n",
        "\n",
        "miscl_img = X_test[y_test != y_test_pred][:25]\n",
        "correct_lab = y_test[y_test != y_test_pred][:25]\n",
        "miscl_lab = predictions[y_test != y_test_pred][:25]\n",
        "\n",
        "fig, ax = plt.subplots(nrows=5, ncols=5, sharex=True, sharey=True)\n",
        "ax = ax.flatten()\n",
        "for i in range(25):\n",
        "    img = miscl_img[i].reshape(28, 28)\n",
        "    ax[i].imshow(img, cmap='Greys', interpolation='nearest')\n",
        "    ax[i].set_title('%d) t: %d p: %d' % (i+1, correct_lab.values[i], miscl_lab[i]))\n",
        "\n",
        "ax[0].set_xticks([])\n",
        "ax[0].set_yticks([])\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wXY4QETGfKLi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "1c31ae44-14dc-4bea-d8fd-6ba1b2d24e3a"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 25 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPkG0fb2I1qq"
      },
      "source": [
        "**Q0-3**: From a human point of view, we can perhaps see a frequent similarity between handritten digits 3 and 5 (i.e. sometimes we have to pay closer attention to figure out what the true written digit is). We may want to ask if the MLP also detects digit similarities.    \n",
        "<br>\n",
        " Consider then the subset $X_i$ of data points that have label $i$ (e.g. all data points with label 5). For each label $j\\neq i$, report the frequency by which label $j$ (e.g. label 3) shows up as second best prediction for points in $X_i$. \n",
        "<br>\n",
        "<br> \n",
        "***Further Clarification***: Suppose we have 100 data points with label i=5. Let's say that for 28 of these 100 points, the second predicted label is j=3. Then, the frequency of j=3 as second-best prediction to i=5 is 28%.  We want to compute that frequency for every i and j.  \n",
        "<br>  \n",
        " Give your code and comment on your findings "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "ZL_sboa1I1qq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98a70086-48ce-4477-8320-7f47528f4c98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5 9 6 ... 2 7 1]\n"
          ]
        }
      ],
      "source": [
        "# Question 0-3 (answer) here: \n",
        "print(y_test_actual)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 0-3 (cont'd) here:\n",
        "# Similiarity count created for all test Labels\n",
        "# Print frequency of values for label 4.\n",
        "similarity = np.zeros((10, 10))\n",
        "\n",
        "for i in range(10000):\n",
        "  similarity[predictions[i],second_predictions[i]] += 1\n",
        "\n",
        "print(similarity[4,:])\n",
        "np.sum(similarity[4,:])\n",
        "print(\"==========================\")\n",
        "print(\"The second most chosen value is 9 when the label is 4\")"
      ],
      "metadata": {
        "id": "STze0dtA1gwr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca1c6266-3571-45dc-d31c-e6672ea1556c"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 11.   7.  97.   8.   0.  90.  83. 172.  21. 476.]\n",
            "==========================\n",
            "The second most chosen value is 9 when the label is 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Confusing 4 can be seen as common as there are many ways to draw this number, including open and closed forms."
      ],
      "metadata": {
        "id": "TOduMXxi72kI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "IllLoXxGAIIo"
      },
      "outputs": [],
      "source": [
        "# For grader use only\n",
        "\n",
        "# insert grade here\n",
        "# G[0,0] =   \n",
        "# G[0,1] = \n",
        "# G[0,2] =\n",
        "# G[0,3] = \n",
        "\n",
        "\n",
        "maxScore = maxScore + 16\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oko-ZnXEpBMc"
      },
      "source": [
        "------------------------------\n",
        "------------------------------\n",
        "------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bescj6mdI1qq"
      },
      "source": [
        "## <font color='blue'>Question 1: Working with warm start for focused training\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmDZTC1UI1qq"
      },
      "source": [
        "**Q1-0**. MLP classifiers in scikit-learn have an option for so-called 'warm start'. Do a bit of research to find out what it is. Then:\n",
        "\n",
        "Give an example of warm start. Specifically\n",
        "train the network for $k$ epochs, and then using warm start\n",
        "continue the training for another $k$ epochs (4 points).\n",
        "Here $k$ is a parameter of your choice.  \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "8wY3lNEgI1qq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec037b39-a910-47ca-d9d9-675225c9732e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 0.77659209\n",
            "Iteration 2, loss = 0.35169476\n",
            "Iteration 3, loss = 0.29370323\n",
            "Iteration 4, loss = 0.26160018\n",
            "Iteration 5, loss = 0.23835006\n",
            "Iteration 6, loss = 0.22020206\n",
            "Iteration 7, loss = 0.20524977\n",
            "Iteration 8, loss = 0.19250908\n",
            "Iteration 9, loss = 0.18166501\n",
            "Iteration 10, loss = 0.17223005\n",
            "Iteration 11, loss = 0.16398489\n",
            "Iteration 12, loss = 0.15674479\n",
            "Iteration 13, loss = 0.15032197\n",
            "Iteration 14, loss = 0.14438893\n",
            "Iteration 15, loss = 0.13925911\n",
            "Iteration 16, loss = 0.13410892\n",
            "Iteration 17, loss = 0.12973445\n",
            "Iteration 18, loss = 0.12544996\n",
            "Iteration 19, loss = 0.12168089\n",
            "Iteration 20, loss = 0.11826818\n",
            "Iteration 21, loss = 0.11486403\n",
            "Iteration 22, loss = 0.11185901\n",
            "Iteration 23, loss = 0.10913528\n",
            "Iteration 24, loss = 0.10600094\n",
            "Iteration 25, loss = 0.10378532\n",
            "Iteration 26, loss = 0.10133948\n",
            "Iteration 27, loss = 0.09901220\n",
            "Iteration 28, loss = 0.09697084\n",
            "Iteration 29, loss = 0.09496202\n",
            "Iteration 30, loss = 0.09295482\n",
            "Iteration 31, loss = 0.09074511\n",
            "Iteration 32, loss = 0.08912391\n",
            "Iteration 33, loss = 0.08738903\n",
            "Iteration 34, loss = 0.08565472\n",
            "Iteration 35, loss = 0.08420766\n",
            "Iteration 36, loss = 0.08272366\n",
            "Iteration 37, loss = 0.08120384\n",
            "Iteration 38, loss = 0.07996620\n",
            "Iteration 39, loss = 0.07851583\n",
            "Iteration 40, loss = 0.07702592\n",
            "Iteration 41, loss = 0.07591513\n",
            "Iteration 42, loss = 0.07467269\n",
            "Iteration 43, loss = 0.07376388\n",
            "Iteration 44, loss = 0.07244570\n",
            "Iteration 45, loss = 0.07137564\n",
            "Iteration 46, loss = 0.07032046\n",
            "Iteration 47, loss = 0.06939159\n",
            "Iteration 48, loss = 0.06840130\n",
            "Iteration 49, loss = 0.06723987\n",
            "Iteration 50, loss = 0.06638381\n",
            "Iteration 51, loss = 0.06555810\n",
            "Iteration 52, loss = 0.06460751\n",
            "Iteration 53, loss = 0.06360214\n",
            "Iteration 54, loss = 0.06291040\n",
            "Iteration 55, loss = 0.06216025\n",
            "Iteration 56, loss = 0.06152016\n",
            "Iteration 57, loss = 0.06061969\n",
            "Iteration 58, loss = 0.05980390\n",
            "Iteration 59, loss = 0.05905587\n",
            "Iteration 60, loss = 0.05845287\n",
            "Iteration 61, loss = 0.05767261\n",
            "Iteration 62, loss = 0.05686698\n",
            "Iteration 63, loss = 0.05637241\n",
            "Iteration 64, loss = 0.05583489\n",
            "Iteration 65, loss = 0.05512632\n",
            "Iteration 66, loss = 0.05443720\n",
            "Iteration 67, loss = 0.05388250\n",
            "Iteration 68, loss = 0.05327557\n",
            "Iteration 69, loss = 0.05277201\n",
            "Iteration 70, loss = 0.05213810\n",
            "Iteration 71, loss = 0.05150209\n",
            "Iteration 72, loss = 0.05100463\n",
            "Iteration 73, loss = 0.05051139\n",
            "Iteration 74, loss = 0.04998574\n",
            "Iteration 75, loss = 0.04954391\n",
            "Iteration 76, loss = 0.04879035\n",
            "Iteration 77, loss = 0.04855296\n",
            "Iteration 78, loss = 0.04804704\n",
            "Iteration 79, loss = 0.04744306\n",
            "Iteration 80, loss = 0.04739455\n",
            "Iteration 81, loss = 0.04670309\n",
            "Iteration 82, loss = 0.04610933\n",
            "Iteration 83, loss = 0.04579732\n",
            "Iteration 84, loss = 0.04527204\n",
            "Iteration 85, loss = 0.04505492\n",
            "Iteration 86, loss = 0.04451223\n",
            "Iteration 87, loss = 0.04400939\n",
            "Iteration 88, loss = 0.04364029\n",
            "Iteration 89, loss = 0.04345397\n",
            "Iteration 90, loss = 0.04289662\n",
            "Iteration 91, loss = 0.04255368\n",
            "Iteration 92, loss = 0.04221875\n",
            "Iteration 93, loss = 0.04181239\n",
            "Iteration 94, loss = 0.04146468\n",
            "Iteration 95, loss = 0.04119931\n",
            "Iteration 96, loss = 0.04077622\n",
            "Iteration 97, loss = 0.04047715\n",
            "Iteration 98, loss = 0.04015814\n",
            "Iteration 99, loss = 0.03981521\n",
            "Iteration 100, loss = 0.03955621\n",
            "Iteration 101, loss = 0.03915517\n",
            "Iteration 102, loss = 0.03904632\n",
            "Iteration 103, loss = 0.03854489\n",
            "Iteration 104, loss = 0.03834123\n",
            "Iteration 105, loss = 0.03802273\n",
            "Iteration 106, loss = 0.03763450\n",
            "Iteration 107, loss = 0.03738968\n",
            "Iteration 108, loss = 0.03698652\n",
            "Iteration 109, loss = 0.03682003\n",
            "Iteration 110, loss = 0.03661497\n",
            "Iteration 111, loss = 0.03632661\n",
            "Iteration 112, loss = 0.03613755\n",
            "Iteration 113, loss = 0.03595112\n",
            "Iteration 114, loss = 0.03556792\n",
            "Iteration 115, loss = 0.03524888\n",
            "Iteration 116, loss = 0.03508341\n",
            "Iteration 117, loss = 0.03485294\n",
            "Iteration 118, loss = 0.03456703\n",
            "Iteration 119, loss = 0.03433396\n",
            "Iteration 120, loss = 0.03415643\n",
            "Iteration 121, loss = 0.03392089\n",
            "Iteration 122, loss = 0.03369334\n",
            "Iteration 123, loss = 0.03359918\n",
            "Iteration 124, loss = 0.03327821\n",
            "Iteration 125, loss = 0.03306742\n",
            "Iteration 126, loss = 0.03285904\n",
            "Iteration 127, loss = 0.03261904\n",
            "Iteration 128, loss = 0.03239006\n",
            "Iteration 129, loss = 0.03220186\n",
            "Iteration 130, loss = 0.03190238\n",
            "Iteration 131, loss = 0.03174641\n",
            "Iteration 132, loss = 0.03177540\n",
            "Iteration 133, loss = 0.03148466\n",
            "Iteration 134, loss = 0.03126944\n",
            "Iteration 135, loss = 0.03109514\n",
            "Iteration 136, loss = 0.03089911\n",
            "Iteration 137, loss = 0.03077134\n",
            "Iteration 138, loss = 0.03074872\n",
            "Iteration 139, loss = 0.03041143\n",
            "Iteration 140, loss = 0.03029596\n",
            "Iteration 141, loss = 0.03014552\n",
            "Iteration 142, loss = 0.02993112\n",
            "Iteration 143, loss = 0.02974024\n",
            "Iteration 144, loss = 0.02957005\n",
            "Iteration 145, loss = 0.02950086\n",
            "Iteration 146, loss = 0.02925733\n",
            "Iteration 147, loss = 0.02917756\n",
            "Iteration 148, loss = 0.02902424\n",
            "Iteration 149, loss = 0.02887811\n",
            "Iteration 150, loss = 0.02867300\n",
            "Iteration 151, loss = 0.02855578\n",
            "Iteration 152, loss = 0.02844942\n",
            "Iteration 153, loss = 0.02834028\n",
            "Iteration 154, loss = 0.02813598\n",
            "Iteration 155, loss = 0.02800036\n",
            "Iteration 156, loss = 0.02792343\n",
            "Iteration 157, loss = 0.02771059\n",
            "Iteration 158, loss = 0.02768987\n",
            "Iteration 159, loss = 0.02752203\n",
            "Iteration 160, loss = 0.02734819\n",
            "Iteration 161, loss = 0.02724592\n",
            "Iteration 162, loss = 0.02708185\n",
            "Iteration 163, loss = 0.02702623\n",
            "Iteration 164, loss = 0.02692941\n",
            "Iteration 165, loss = 0.02676813\n",
            "Iteration 166, loss = 0.02673706\n",
            "Iteration 167, loss = 0.02654885\n",
            "Iteration 168, loss = 0.02638828\n",
            "Iteration 169, loss = 0.02636107\n",
            "Iteration 170, loss = 0.02626703\n",
            "Iteration 171, loss = 0.02611144\n",
            "Iteration 172, loss = 0.02601080\n",
            "Iteration 173, loss = 0.02599704\n",
            "Iteration 174, loss = 0.02575943\n",
            "Iteration 175, loss = 0.02575072\n",
            "Iteration 176, loss = 0.02561648\n",
            "Iteration 177, loss = 0.02560790\n",
            "Iteration 178, loss = 0.02537408\n",
            "Iteration 179, loss = 0.02533754\n",
            "Iteration 180, loss = 0.02532227\n",
            "Iteration 181, loss = 0.02521857\n",
            "Iteration 182, loss = 0.02508274\n",
            "Iteration 183, loss = 0.02497259\n",
            "Iteration 184, loss = 0.02487126\n",
            "Iteration 185, loss = 0.02475029\n",
            "Iteration 186, loss = 0.02474162\n",
            "Iteration 187, loss = 0.02459089\n",
            "Iteration 188, loss = 0.02453062\n",
            "Iteration 189, loss = 0.02445432\n",
            "Iteration 190, loss = 0.02437926\n",
            "Iteration 191, loss = 0.02429475\n",
            "Iteration 192, loss = 0.02425932\n",
            "Iteration 193, loss = 0.02417743\n",
            "Iteration 194, loss = 0.02396331\n",
            "Iteration 195, loss = 0.02391551\n",
            "Iteration 196, loss = 0.02396394\n",
            "Iteration 197, loss = 0.02385143\n",
            "Iteration 198, loss = 0.02376792\n",
            "Iteration 199, loss = 0.02374289\n",
            "Iteration 200, loss = 0.02360476\n",
            "Iteration 201, loss = 0.02357534\n",
            "Iteration 202, loss = 0.02351188\n",
            "Iteration 203, loss = 0.02344980\n",
            "Iteration 204, loss = 0.02331842\n",
            "Iteration 205, loss = 0.02326380\n",
            "Iteration 206, loss = 0.02316166\n",
            "Iteration 207, loss = 0.02318271\n",
            "Iteration 208, loss = 0.02309092\n",
            "Iteration 209, loss = 0.02301490\n",
            "Iteration 210, loss = 0.02293142\n",
            "Iteration 211, loss = 0.02290801\n",
            "Iteration 212, loss = 0.02279947\n",
            "Iteration 213, loss = 0.02272150\n",
            "Iteration 214, loss = 0.02266641\n",
            "Iteration 215, loss = 0.02269754\n",
            "Iteration 216, loss = 0.02261850\n",
            "Iteration 217, loss = 0.02249412\n",
            "Iteration 218, loss = 0.02249087\n",
            "Iteration 219, loss = 0.02239473\n",
            "Iteration 220, loss = 0.02234531\n",
            "Iteration 221, loss = 0.02227755\n",
            "Iteration 222, loss = 0.02226609\n",
            "Iteration 223, loss = 0.02223867\n",
            "Iteration 224, loss = 0.02218460\n",
            "Iteration 225, loss = 0.02211229\n",
            "Iteration 226, loss = 0.02202673\n",
            "Iteration 227, loss = 0.02199307\n",
            "Iteration 228, loss = 0.02193633\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "time taken to run: 1371.2754018659998\n"
          ]
        }
      ],
      "source": [
        "# Question 1-0 (answer) here: Your code here. Also give some comments about your findings\n",
        "# Initialize Training\n",
        "import time\n",
        "t1 = time.perf_counter()\n",
        "k = 250\n",
        "batches = 10\n",
        "\n",
        "mlp_warm = MLPClassifier(verbose = True, hidden_layer_sizes=(50,), batch_size = batches, \\\n",
        "                          max_iter=k, solver='sgd', activation='logistic',\\\n",
        "                          learning_rate = 'constant', learning_rate_init = 0.001, random_state=1, \\\n",
        "                          warm_start = True)\n",
        "\n",
        "#First Training\n",
        "mlp_warm.fit(X_train,y_train)\n",
        "t2 = time.perf_counter()\n",
        "print('time taken to run:',t2-t1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 1-0 (cont'd) here:\n",
        "# Score of the first warm_start training with comparisons\n",
        "print('Test accuracy: %.2f%%' % (mlp_warm.score(X_test,y_test) * 100))\n",
        "print('==========================')\n",
        "print('time taken to run without warm start: 1306.2878746459992')\n",
        "print('loss: 0.02193633')\n",
        "print('==========================')\n",
        "print('time taken to run with warm start: 1371.2754018659998')\n",
        "print('loss: 0.02193633')"
      ],
      "metadata": {
        "id": "8x33H90Z9QGo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8c4ca80-e63a-485c-dbb0-eb5353765479"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 96.82%\n",
            "==========================\n",
            "time taken to run without warm start: 1306.2878746459992\n",
            "loss: 0.02193633\n",
            "==========================\n",
            "time taken to run with warm start: 1371.2754018659998\n",
            "loss: 0.02193633\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwK_llxsI1qq"
      },
      "source": [
        "\n",
        "The training set usally remains the same after a warm start. Here, we will use a very similar bit not identical dataset after the warm start:\n",
        "<br>\n",
        "<br>\n",
        "**Q1-1**. Randomly divide the MNIST dataset into three part: train1 (50000), train2(10000), test (10000). Make sure that all number classes are presented in both train1, and train2. [4 points]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "DwlNHIdcI1qq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7076ccaa-5140-47f4-fab6-590860fdddc4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train1: (50000, 784)\n",
            "X_train2: (10000, 784)\n",
            "X_test: (10000, 784)\n"
          ]
        }
      ],
      "source": [
        "# Question 1-1 (answer) here: \n",
        "X_train1, X_train2, y_train1, y_train2 = train_test_split(\n",
        "    X_train, y_train, test_size=10000, random_state=3, stratify=y_train)\n",
        "\n",
        "print('X_train1:', X_train1.shape)\n",
        "print('X_train2:', X_train2.shape)\n",
        "print('X_test:', X_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 1-1 (cont'd) here:\n",
        "# Validating that each set accounts for each label\n",
        "train1_num = y_train1.unique()\n",
        "train1_num.sort()\n",
        "\n",
        "train2_num = y_train2.unique()\n",
        "train2_num.sort()\n",
        "\n",
        "test_num = y_test.unique()\n",
        "test_num.sort()\n",
        "\n",
        "print('Number classes in train1:', train1_num)\n",
        "print('Number classes in train2:', train2_num)\n",
        "print('Number classes in test:', test_num)"
      ],
      "metadata": {
        "id": "WjwZXzkf_oUJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1696935b-2504-4a19-ad81-746d939d5cd5"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number classes in train1: [0 1 2 3 4 5 6 7 8 9]\n",
            "Number classes in train2: [0 1 2 3 4 5 6 7 8 9]\n",
            "Number classes in test: [0 1 2 3 4 5 6 7 8 9]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qj639NWSI1qq"
      },
      "source": [
        "**Q1-2**. Use warm start and train your model using the train1 dataset for $2k$ epochs. Then, using warm start, continute the training with train2 dataset for another $2k$ epochs. [4 points]\n",
        "<br>\n",
        "<br>\n",
        "Here $k$ is the same parameter you used above. Notice that each point\n",
        "is considered $2k$ times in this training, exactly as in part 0. \n",
        "<br>\n",
        "<br>\n",
        "In effect this process of training tries spend more time learning half of the training set, and then moves to the second part of the training set to spend equal time. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "Qrr0rwxrI1qq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a396066-0df9-41bf-f9f6-2817d792ab3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 0.86617445\n",
            "Iteration 2, loss = 0.37202389\n",
            "Iteration 3, loss = 0.30751505\n",
            "Iteration 4, loss = 0.27471456\n",
            "Iteration 5, loss = 0.25180193\n",
            "Iteration 6, loss = 0.23404477\n",
            "Iteration 7, loss = 0.21906038\n",
            "Iteration 8, loss = 0.20647544\n",
            "Iteration 9, loss = 0.19532608\n",
            "Iteration 10, loss = 0.18592321\n",
            "Iteration 11, loss = 0.17738911\n",
            "Iteration 12, loss = 0.16997145\n",
            "Iteration 13, loss = 0.16288138\n",
            "Iteration 14, loss = 0.15691085\n",
            "Iteration 15, loss = 0.15090127\n",
            "Iteration 16, loss = 0.14560828\n",
            "Iteration 17, loss = 0.14083938\n",
            "Iteration 18, loss = 0.13661482\n",
            "Iteration 19, loss = 0.13235547\n",
            "Iteration 20, loss = 0.12846187\n",
            "Iteration 21, loss = 0.12483226\n",
            "Iteration 22, loss = 0.12147366\n",
            "Iteration 23, loss = 0.11804410\n",
            "Iteration 24, loss = 0.11541575\n",
            "Iteration 25, loss = 0.11229509\n",
            "Iteration 26, loss = 0.10960960\n",
            "Iteration 27, loss = 0.10698635\n",
            "Iteration 28, loss = 0.10448960\n",
            "Iteration 29, loss = 0.10234489\n",
            "Iteration 30, loss = 0.10035784\n",
            "Iteration 31, loss = 0.09840593\n",
            "Iteration 32, loss = 0.09635484\n",
            "Iteration 33, loss = 0.09417086\n",
            "Iteration 34, loss = 0.09248678\n",
            "Iteration 35, loss = 0.09085817\n",
            "Iteration 36, loss = 0.08907647\n",
            "Iteration 37, loss = 0.08740674\n",
            "Iteration 38, loss = 0.08599159\n",
            "Iteration 39, loss = 0.08446465\n",
            "Iteration 40, loss = 0.08315446\n",
            "Iteration 41, loss = 0.08154732\n",
            "Iteration 42, loss = 0.08013873\n",
            "Iteration 43, loss = 0.07900596\n",
            "Iteration 44, loss = 0.07795980\n",
            "Iteration 45, loss = 0.07667467\n",
            "Iteration 46, loss = 0.07564805\n",
            "Iteration 47, loss = 0.07440706\n",
            "Iteration 48, loss = 0.07306375\n",
            "Iteration 49, loss = 0.07211756\n",
            "Iteration 50, loss = 0.07108695\n",
            "Iteration 51, loss = 0.07020668\n",
            "Iteration 52, loss = 0.06913557\n",
            "Iteration 53, loss = 0.06824383\n",
            "Iteration 54, loss = 0.06716687\n",
            "Iteration 55, loss = 0.06636263\n",
            "Iteration 56, loss = 0.06525183\n",
            "Iteration 57, loss = 0.06472485\n",
            "Iteration 58, loss = 0.06355368\n",
            "Iteration 59, loss = 0.06294313\n",
            "Iteration 60, loss = 0.06212535\n",
            "Iteration 61, loss = 0.06150764\n",
            "Iteration 62, loss = 0.06081017\n",
            "Iteration 63, loss = 0.06002471\n",
            "Iteration 64, loss = 0.05915613\n",
            "Iteration 65, loss = 0.05836851\n",
            "Iteration 66, loss = 0.05779318\n",
            "Iteration 67, loss = 0.05725909\n",
            "Iteration 68, loss = 0.05648658\n",
            "Iteration 69, loss = 0.05579930\n",
            "Iteration 70, loss = 0.05522600\n",
            "Iteration 71, loss = 0.05474793\n",
            "Iteration 72, loss = 0.05417290\n",
            "Iteration 73, loss = 0.05351481\n",
            "Iteration 74, loss = 0.05287459\n",
            "Iteration 75, loss = 0.05240669\n",
            "Iteration 76, loss = 0.05185018\n",
            "Iteration 77, loss = 0.05134180\n",
            "Iteration 78, loss = 0.05082003\n",
            "Iteration 79, loss = 0.05027796\n",
            "Iteration 80, loss = 0.04990022\n",
            "Iteration 81, loss = 0.04932627\n",
            "Iteration 82, loss = 0.04869096\n",
            "Iteration 83, loss = 0.04835639\n",
            "Iteration 84, loss = 0.04788976\n",
            "Iteration 85, loss = 0.04727328\n",
            "Iteration 86, loss = 0.04690970\n",
            "Iteration 87, loss = 0.04641371\n",
            "Iteration 88, loss = 0.04610804\n",
            "Iteration 89, loss = 0.04558818\n",
            "Iteration 90, loss = 0.04527141\n",
            "Iteration 91, loss = 0.04470116\n",
            "Iteration 92, loss = 0.04443804\n",
            "Iteration 93, loss = 0.04397312\n",
            "Iteration 94, loss = 0.04356498\n",
            "Iteration 95, loss = 0.04337068\n",
            "Iteration 96, loss = 0.04279577\n",
            "Iteration 97, loss = 0.04248862\n",
            "Iteration 98, loss = 0.04217879\n",
            "Iteration 99, loss = 0.04164056\n",
            "Iteration 100, loss = 0.04148008\n",
            "Iteration 101, loss = 0.04112731\n",
            "Iteration 102, loss = 0.04068651\n",
            "Iteration 103, loss = 0.04040656\n",
            "Iteration 104, loss = 0.04003702\n",
            "Iteration 105, loss = 0.03976268\n",
            "Iteration 106, loss = 0.03949953\n",
            "Iteration 107, loss = 0.03909185\n",
            "Iteration 108, loss = 0.03884059\n",
            "Iteration 109, loss = 0.03861403\n",
            "Iteration 110, loss = 0.03825560\n",
            "Iteration 111, loss = 0.03795875\n",
            "Iteration 112, loss = 0.03758968\n",
            "Iteration 113, loss = 0.03740843\n",
            "Iteration 114, loss = 0.03717483\n",
            "Iteration 115, loss = 0.03665463\n",
            "Iteration 116, loss = 0.03662634\n",
            "Iteration 117, loss = 0.03620269\n",
            "Iteration 118, loss = 0.03601930\n",
            "Iteration 119, loss = 0.03579287\n",
            "Iteration 120, loss = 0.03542106\n",
            "Iteration 121, loss = 0.03521819\n",
            "Iteration 122, loss = 0.03505473\n",
            "Iteration 123, loss = 0.03475703\n",
            "Iteration 124, loss = 0.03448871\n",
            "Iteration 125, loss = 0.03425300\n",
            "Iteration 126, loss = 0.03404487\n",
            "Iteration 127, loss = 0.03386306\n",
            "Iteration 128, loss = 0.03363068\n",
            "Iteration 129, loss = 0.03339086\n",
            "Iteration 130, loss = 0.03309542\n",
            "Iteration 131, loss = 0.03291011\n",
            "Iteration 132, loss = 0.03274908\n",
            "Iteration 133, loss = 0.03251358\n",
            "Iteration 134, loss = 0.03236271\n",
            "Iteration 135, loss = 0.03218430\n",
            "Iteration 136, loss = 0.03199551\n",
            "Iteration 137, loss = 0.03175825\n",
            "Iteration 138, loss = 0.03155289\n",
            "Iteration 139, loss = 0.03139012\n",
            "Iteration 140, loss = 0.03121529\n",
            "Iteration 141, loss = 0.03101149\n",
            "Iteration 142, loss = 0.03073431\n",
            "Iteration 143, loss = 0.03072753\n",
            "Iteration 144, loss = 0.03040944\n",
            "Iteration 145, loss = 0.03032887\n",
            "Iteration 146, loss = 0.03013463\n",
            "Iteration 147, loss = 0.02992066\n",
            "Iteration 148, loss = 0.02983302\n",
            "Iteration 149, loss = 0.02958254\n",
            "Iteration 150, loss = 0.02945941\n",
            "Iteration 151, loss = 0.02929479\n",
            "Iteration 152, loss = 0.02919296\n",
            "Iteration 153, loss = 0.02884840\n",
            "Iteration 154, loss = 0.02885429\n",
            "Iteration 155, loss = 0.02863017\n",
            "Iteration 156, loss = 0.02842519\n",
            "Iteration 157, loss = 0.02834842\n",
            "Iteration 158, loss = 0.02816767\n",
            "Iteration 159, loss = 0.02805074\n",
            "Iteration 160, loss = 0.02785684\n",
            "Iteration 161, loss = 0.02775695\n",
            "Iteration 162, loss = 0.02751199\n",
            "Iteration 163, loss = 0.02756260\n",
            "Iteration 164, loss = 0.02733834\n",
            "Iteration 165, loss = 0.02723590\n",
            "Iteration 166, loss = 0.02717419\n",
            "Iteration 167, loss = 0.02693856\n",
            "Iteration 168, loss = 0.02688663\n",
            "Iteration 169, loss = 0.02668228\n",
            "Iteration 170, loss = 0.02654742\n",
            "Iteration 171, loss = 0.02643883\n",
            "Iteration 172, loss = 0.02632110\n",
            "Iteration 173, loss = 0.02632934\n",
            "Iteration 174, loss = 0.02618712\n",
            "Iteration 175, loss = 0.02602676\n",
            "Iteration 176, loss = 0.02587139\n",
            "Iteration 177, loss = 0.02582809\n",
            "Iteration 178, loss = 0.02563041\n",
            "Iteration 179, loss = 0.02557795\n",
            "Iteration 180, loss = 0.02547119\n",
            "Iteration 181, loss = 0.02538989\n",
            "Iteration 182, loss = 0.02527625\n",
            "Iteration 183, loss = 0.02514987\n",
            "Iteration 184, loss = 0.02503548\n",
            "Iteration 185, loss = 0.02488652\n",
            "Iteration 186, loss = 0.02485668\n",
            "Iteration 187, loss = 0.02470518\n",
            "Iteration 188, loss = 0.02458125\n",
            "Iteration 189, loss = 0.02457176\n",
            "Iteration 190, loss = 0.02442734\n",
            "Iteration 191, loss = 0.02441431\n",
            "Iteration 192, loss = 0.02429833\n",
            "Iteration 193, loss = 0.02418964\n",
            "Iteration 194, loss = 0.02410184\n",
            "Iteration 195, loss = 0.02399666\n",
            "Iteration 196, loss = 0.02394213\n",
            "Iteration 197, loss = 0.02380396\n",
            "Iteration 198, loss = 0.02374542\n",
            "Iteration 199, loss = 0.02365272\n",
            "Iteration 200, loss = 0.02354698\n",
            "Iteration 201, loss = 0.02347662\n",
            "Iteration 202, loss = 0.02340696\n",
            "Iteration 203, loss = 0.02329866\n",
            "Iteration 204, loss = 0.02322383\n",
            "Iteration 205, loss = 0.02327170\n",
            "Iteration 206, loss = 0.02309970\n",
            "Iteration 207, loss = 0.02304308\n",
            "Iteration 208, loss = 0.02299083\n",
            "Iteration 209, loss = 0.02280366\n",
            "Iteration 210, loss = 0.02276486\n",
            "Iteration 211, loss = 0.02276017\n",
            "Iteration 212, loss = 0.02267860\n",
            "Iteration 213, loss = 0.02261913\n",
            "Iteration 214, loss = 0.02253541\n",
            "Iteration 215, loss = 0.02247242\n",
            "Iteration 216, loss = 0.02237370\n",
            "Iteration 217, loss = 0.02231710\n",
            "Iteration 218, loss = 0.02232020\n",
            "Iteration 219, loss = 0.02220689\n",
            "Iteration 220, loss = 0.02211628\n",
            "Iteration 221, loss = 0.02209120\n",
            "Iteration 222, loss = 0.02202961\n",
            "Iteration 223, loss = 0.02193414\n",
            "Iteration 224, loss = 0.02185797\n",
            "Iteration 225, loss = 0.02184760\n",
            "Iteration 226, loss = 0.02181835\n",
            "Iteration 227, loss = 0.02172616\n",
            "Iteration 228, loss = 0.02164975\n",
            "Iteration 229, loss = 0.02163600\n",
            "Iteration 230, loss = 0.02159751\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Score of train1 data: 0.99936\n",
            "==========================\n",
            "Iteration 231, loss = 0.12347617\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Score of train2 data: 0.9707\n",
            "==========================\n",
            "time taken to run: -1202842.300238404\n"
          ]
        }
      ],
      "source": [
        "# Question 1-2 (answer) here:\n",
        "import time\n",
        "t1 = time.perf_counter()\n",
        "k = 250\n",
        "batches = 10\n",
        "\n",
        "mlp1_warm = MLPClassifier(verbose = True, hidden_layer_sizes=(50,), batch_size = batches, \\\n",
        "                          max_iter=2*k, solver='sgd', activation='logistic',\\\n",
        "                          learning_rate = 'constant', learning_rate_init = 0.001, random_state=3, \\\n",
        "                          warm_start = True)\n",
        "\n",
        "# Training with train1 data shuffled\n",
        "mlp1_warm.fit(X_train1,y_train1)\n",
        "print('Score of train1 data:' , mlp1_warm.score(X_train1, y_train1))\n",
        "print(\"==========================\")\n",
        "mlp1_warm.fit(X_train2,y_train2)\n",
        "print('Score of train2 data:' , mlp1_warm.score(X_train2, y_train2))\n",
        "print(\"==========================\")\n",
        "t2 = time.perf_counter()\n",
        "print('time taken to run:',t2-t1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 1-2 (cont'd) here:\n",
        "print('Test accuracy: %.2f%%' % (mlp1_warm.score(X_test,y_test) * 100))"
      ],
      "metadata": {
        "id": "OT0gIGv1JYas",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7432315f-4121-42bb-aacd-0f05fbeb919c"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 96.42%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVOatk-gVU4O"
      },
      "source": [
        "**Q1-3**. Which of the two above strategies\n",
        "works better? (Q1-0 or Q1-2). Feel free to experiment and report your findings."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<n>**Question 1-3 (cont'd) here:**<n/>\n",
        "\n",
        "- <n>The strategy in Q1-2 enabled the classifier to run more iterations in less time.<n/>\n",
        "\n",
        "- <n><n>Strategy Q1-0 lead to a test score of %96 after the first training and 0.9674 after the second training (a marginal but noticeable improvement), the Q1-2 stategy lead to a test score of 0.9642 after the first training and 0.9655 after the second training - going from a score that was actually less than the second score for Q1-0, to a score that was lower than all of the scores.<n/>\n"
      ],
      "metadata": {
        "id": "buO0T8W1ZoYZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "5r9YkG3xsS5l"
      },
      "outputs": [],
      "source": [
        "# For grader use only\n",
        "\n",
        "# insert grade here\n",
        "# G[1,0] =   \n",
        "# G[1,1] = \n",
        "# G[1,2] =\n",
        "# G[1,3] = \n",
        "\n",
        "\n",
        "maxScore = maxScore + 16\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AgUw1wklsZOi"
      },
      "source": [
        "--------------------------\n",
        "--------------------------\n",
        "--------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBaZ0tdBVvDT"
      },
      "source": [
        "##  <font color='blue'> Q2. How Powerful are Neural Networks? \n",
        "\n",
        "There is a lot of hype around Neural Networks, and their ability to solver problems. This is of course justified, but we should always be aware of their limitations. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrCrmP4tWnGx"
      },
      "source": [
        "Recall that every row in the matrix $X$  above comes from an image. Every entry of a row corresponds to a (grayscale) pixel. In particular, the first colum of $X$ contains the grayscale values of the first pixels of the images in the data set. \n",
        "<br>\n",
        "<br>\n",
        "We will now 'hide' the label of the image, inside the image itself. \n",
        "Here is one way to do that: Replace the top-left pixel of the image (which is a value from 0 to 255) with the label for that image. For example, if the image\n",
        "contains number 3, then the first pixel of the image will now be set to 3. (In fact, let's set this to 3/255 because we already divided all pixel values by 255)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dI8SbkQ2Xh6b"
      },
      "source": [
        "**Q2-0.** Create a new dataset $X1$, where each row is the same as the corresponding row of $X$, except in the first column, which contains the label of the corresponding image, divided by 255. Then split\n",
        "the set $X1$ into a training set and a test set, as we did with $X$. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "nxGaaYSbZc3y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a9c4be1-07ac-4f4f-c4ae-6e438539f190"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(70000, 784)\n"
          ]
        }
      ],
      "source": [
        "# Questions 2-0 (answer) here:\n",
        "# Creating new dataset and replacing first columns with values of labels y\n",
        "Xx, yy = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
        "yy = yy.astype(int)\n",
        "Xx = ((Xx / 255.) - .5) * 2\n",
        "\n",
        "Xx['pixel1'] = yy/255\n",
        "print(Xx.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 2-0 (cont'd) here:\n",
        "# New training and test set from X\n",
        "Xx_train, Xx_test, yy_train, yy_test = train_test_split(\n",
        "    X, y, test_size=10000, random_state=333, stratify=y)\n",
        "\n",
        "print(Xx_train,yy_train)"
      ],
      "metadata": {
        "id": "BFdFWwxdLIlt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e7684e6-bcf1-4a33-9385-88787e8e02f2"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  pixel9  \\\n",
            "40131    -1.0    -1.0    -1.0    -1.0    -1.0    -1.0    -1.0    -1.0    -1.0   \n",
            "17964    -1.0    -1.0    -1.0    -1.0    -1.0    -1.0    -1.0    -1.0    -1.0   \n",
            "29446    -1.0    -1.0    -1.0    -1.0    -1.0    -1.0    -1.0    -1.0    -1.0   \n",
            "2843     -1.0    -1.0    -1.0    -1.0    -1.0    -1.0    -1.0    -1.0    -1.0   \n",
            "19060    -1.0    -1.0    -1.0    -1.0    -1.0    -1.0    -1.0    -1.0    -1.0   \n",
            "...       ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
            "6106     -1.0    -1.0    -1.0    -1.0    -1.0    -1.0    -1.0    -1.0    -1.0   \n",
            "32745    -1.0    -1.0    -1.0    -1.0    -1.0    -1.0    -1.0    -1.0    -1.0   \n",
            "15521    -1.0    -1.0    -1.0    -1.0    -1.0    -1.0    -1.0    -1.0    -1.0   \n",
            "48852    -1.0    -1.0    -1.0    -1.0    -1.0    -1.0    -1.0    -1.0    -1.0   \n",
            "59428    -1.0    -1.0    -1.0    -1.0    -1.0    -1.0    -1.0    -1.0    -1.0   \n",
            "\n",
            "       pixel10  ...  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
            "40131     -1.0  ...      -1.0      -1.0      -1.0      -1.0      -1.0   \n",
            "17964     -1.0  ...      -1.0      -1.0      -1.0      -1.0      -1.0   \n",
            "29446     -1.0  ...      -1.0      -1.0      -1.0      -1.0      -1.0   \n",
            "2843      -1.0  ...      -1.0      -1.0      -1.0      -1.0      -1.0   \n",
            "19060     -1.0  ...      -1.0      -1.0      -1.0      -1.0      -1.0   \n",
            "...        ...  ...       ...       ...       ...       ...       ...   \n",
            "6106      -1.0  ...      -1.0      -1.0      -1.0      -1.0      -1.0   \n",
            "32745     -1.0  ...      -1.0      -1.0      -1.0      -1.0      -1.0   \n",
            "15521     -1.0  ...      -1.0      -1.0      -1.0      -1.0      -1.0   \n",
            "48852     -1.0  ...      -1.0      -1.0      -1.0      -1.0      -1.0   \n",
            "59428     -1.0  ...      -1.0      -1.0      -1.0      -1.0      -1.0   \n",
            "\n",
            "       pixel780  pixel781  pixel782  pixel783  pixel784  \n",
            "40131      -1.0      -1.0      -1.0      -1.0      -1.0  \n",
            "17964      -1.0      -1.0      -1.0      -1.0      -1.0  \n",
            "29446      -1.0      -1.0      -1.0      -1.0      -1.0  \n",
            "2843       -1.0      -1.0      -1.0      -1.0      -1.0  \n",
            "19060      -1.0      -1.0      -1.0      -1.0      -1.0  \n",
            "...         ...       ...       ...       ...       ...  \n",
            "6106       -1.0      -1.0      -1.0      -1.0      -1.0  \n",
            "32745      -1.0      -1.0      -1.0      -1.0      -1.0  \n",
            "15521      -1.0      -1.0      -1.0      -1.0      -1.0  \n",
            "48852      -1.0      -1.0      -1.0      -1.0      -1.0  \n",
            "59428      -1.0      -1.0      -1.0      -1.0      -1.0  \n",
            "\n",
            "[60000 rows x 784 columns] 40131    8\n",
            "17964    1\n",
            "29446    0\n",
            "2843     1\n",
            "19060    0\n",
            "        ..\n",
            "6106     3\n",
            "32745    9\n",
            "15521    4\n",
            "48852    7\n",
            "59428    1\n",
            "Name: class, Length: 60000, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDmOC16kZgqM"
      },
      "source": [
        "**Q2-1.** With the label of the image 'planted' in the image itself, a Neural Network can achieve accuracy 100%. This is because all the weights connecting to pixels $2...28^2$ can be set to 0, and so effectively the only feature that will be taken into account is the first pixel. The question is whether an MLP can indeed 'discover' this weighting via training.\n",
        "<br>\n",
        "<br>\n",
        "Use an MLP classifier  with one hidden layer of any size you want. Train it on $X'$ and report the accuracy. Does it reach 100\\%? Is the accuracy better relative to training with the original input $X$? \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 2-1 (answer) here:\n",
        "import time\n",
        "t1 = time.perf_counter()\n",
        "iterations = 250\n",
        "batches = 10\n",
        "\n",
        "mlp_hide = MLPClassifier(verbose = True, hidden_layer_sizes=(25,), batch_size = batches, \\\n",
        "                          max_iter=iterations, solver='sgd', activation='logistic',\\\n",
        "                          learning_rate = 'constant', learning_rate_init = 0.001, random_state=1)\n",
        "\n",
        "mlp_hide.fit(Xx_train,yy_train)\n",
        "print(\"==========================\")\n",
        "mlp_hide.score(Xx_test,yy_test)\n",
        "print(\"==========================\")\n",
        "print('Score of train2 data:' , mlp_hide.score(Xx_train, yy_train) * 100)\n",
        "print(\"==========================\")\n",
        "t2 = time.perf_counter()\n",
        "print('time taken to run:',t2-t1)"
      ],
      "metadata": {
        "id": "QFPeaiavVS1D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bf0dd0d-90e3-49dd-f769-e690ce5ca94a"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 2.00230258\n",
            "Iteration 2, loss = 1.47521933\n",
            "Iteration 3, loss = 1.13004155\n",
            "Iteration 4, loss = 0.91381261\n",
            "Iteration 5, loss = 0.77059380\n",
            "Iteration 6, loss = 0.67022218\n",
            "Iteration 7, loss = 0.59742292\n",
            "Iteration 8, loss = 0.54313948\n",
            "Iteration 9, loss = 0.50182582\n",
            "Iteration 10, loss = 0.46960917\n",
            "Iteration 11, loss = 0.44376390\n",
            "Iteration 12, loss = 0.42283136\n",
            "Iteration 13, loss = 0.40538050\n",
            "Iteration 14, loss = 0.39081869\n",
            "Iteration 15, loss = 0.37817264\n",
            "Iteration 16, loss = 0.36719122\n",
            "Iteration 17, loss = 0.35755239\n",
            "Iteration 18, loss = 0.34890131\n",
            "Iteration 19, loss = 0.34128331\n",
            "Iteration 20, loss = 0.33431175\n",
            "Iteration 21, loss = 0.32813368\n",
            "Iteration 22, loss = 0.32224953\n",
            "Iteration 23, loss = 0.31686142\n",
            "Iteration 24, loss = 0.31204854\n",
            "Iteration 25, loss = 0.30739701\n",
            "Iteration 26, loss = 0.30296136\n",
            "Iteration 27, loss = 0.29896326\n",
            "Iteration 28, loss = 0.29517345\n",
            "Iteration 29, loss = 0.29159889\n",
            "Iteration 30, loss = 0.28823589\n",
            "Iteration 31, loss = 0.28479794\n",
            "Iteration 32, loss = 0.28190365\n",
            "Iteration 33, loss = 0.27899390\n",
            "Iteration 34, loss = 0.27601131\n",
            "Iteration 35, loss = 0.27334939\n",
            "Iteration 36, loss = 0.27080478\n",
            "Iteration 37, loss = 0.26841678\n",
            "Iteration 38, loss = 0.26587513\n",
            "Iteration 39, loss = 0.26351977\n",
            "Iteration 40, loss = 0.26136860\n",
            "Iteration 41, loss = 0.25923854\n",
            "Iteration 42, loss = 0.25713303\n",
            "Iteration 43, loss = 0.25506002\n",
            "Iteration 44, loss = 0.25313091\n",
            "Iteration 45, loss = 0.25123117\n",
            "Iteration 46, loss = 0.24932834\n",
            "Iteration 47, loss = 0.24758698\n",
            "Iteration 48, loss = 0.24582500\n",
            "Iteration 49, loss = 0.24412109\n",
            "Iteration 50, loss = 0.24251868\n",
            "Iteration 51, loss = 0.24090890\n",
            "Iteration 52, loss = 0.23918508\n",
            "Iteration 53, loss = 0.23777384\n",
            "Iteration 54, loss = 0.23629421\n",
            "Iteration 55, loss = 0.23467832\n",
            "Iteration 56, loss = 0.23326960\n",
            "Iteration 57, loss = 0.23191414\n",
            "Iteration 58, loss = 0.23050767\n",
            "Iteration 59, loss = 0.22923508\n",
            "Iteration 60, loss = 0.22795556\n",
            "Iteration 61, loss = 0.22658792\n",
            "Iteration 62, loss = 0.22529957\n",
            "Iteration 63, loss = 0.22398821\n",
            "Iteration 64, loss = 0.22287597\n",
            "Iteration 65, loss = 0.22164199\n",
            "Iteration 66, loss = 0.22054018\n",
            "Iteration 67, loss = 0.21927842\n",
            "Iteration 68, loss = 0.21815277\n",
            "Iteration 69, loss = 0.21708264\n",
            "Iteration 70, loss = 0.21588591\n",
            "Iteration 71, loss = 0.21483766\n",
            "Iteration 72, loss = 0.21377053\n",
            "Iteration 73, loss = 0.21270642\n",
            "Iteration 74, loss = 0.21179456\n",
            "Iteration 75, loss = 0.21069918\n",
            "Iteration 76, loss = 0.20963391\n",
            "Iteration 77, loss = 0.20860519\n",
            "Iteration 78, loss = 0.20782584\n",
            "Iteration 79, loss = 0.20679313\n",
            "Iteration 80, loss = 0.20577970\n",
            "Iteration 81, loss = 0.20476397\n",
            "Iteration 82, loss = 0.20403969\n",
            "Iteration 83, loss = 0.20311202\n",
            "Iteration 84, loss = 0.20221042\n",
            "Iteration 85, loss = 0.20135282\n",
            "Iteration 86, loss = 0.20041682\n",
            "Iteration 87, loss = 0.19963474\n",
            "Iteration 88, loss = 0.19871144\n",
            "Iteration 89, loss = 0.19790622\n",
            "Iteration 90, loss = 0.19711278\n",
            "Iteration 91, loss = 0.19619576\n",
            "Iteration 92, loss = 0.19540477\n",
            "Iteration 93, loss = 0.19466989\n",
            "Iteration 94, loss = 0.19386125\n",
            "Iteration 95, loss = 0.19297082\n",
            "Iteration 96, loss = 0.19232842\n",
            "Iteration 97, loss = 0.19155820\n",
            "Iteration 98, loss = 0.19091196\n",
            "Iteration 99, loss = 0.18997108\n",
            "Iteration 100, loss = 0.18942354\n",
            "Iteration 101, loss = 0.18861195\n",
            "Iteration 102, loss = 0.18785730\n",
            "Iteration 103, loss = 0.18724890\n",
            "Iteration 104, loss = 0.18653900\n",
            "Iteration 105, loss = 0.18583512\n",
            "Iteration 106, loss = 0.18511215\n",
            "Iteration 107, loss = 0.18444274\n",
            "Iteration 108, loss = 0.18377774\n",
            "Iteration 109, loss = 0.18317879\n",
            "Iteration 110, loss = 0.18253922\n",
            "Iteration 111, loss = 0.18180213\n",
            "Iteration 112, loss = 0.18117878\n",
            "Iteration 113, loss = 0.18061197\n",
            "Iteration 114, loss = 0.17992398\n",
            "Iteration 115, loss = 0.17941445\n",
            "Iteration 116, loss = 0.17862187\n",
            "Iteration 117, loss = 0.17809508\n",
            "Iteration 118, loss = 0.17749413\n",
            "Iteration 119, loss = 0.17689392\n",
            "Iteration 120, loss = 0.17629528\n",
            "Iteration 121, loss = 0.17574021\n",
            "Iteration 122, loss = 0.17522795\n",
            "Iteration 123, loss = 0.17463552\n",
            "Iteration 124, loss = 0.17397239\n",
            "Iteration 125, loss = 0.17344990\n",
            "Iteration 126, loss = 0.17297794\n",
            "Iteration 127, loss = 0.17241857\n",
            "Iteration 128, loss = 0.17179857\n",
            "Iteration 129, loss = 0.17127555\n",
            "Iteration 130, loss = 0.17073285\n",
            "Iteration 131, loss = 0.17021766\n",
            "Iteration 132, loss = 0.16967076\n",
            "Iteration 133, loss = 0.16913905\n",
            "Iteration 134, loss = 0.16859468\n",
            "Iteration 135, loss = 0.16814661\n",
            "Iteration 136, loss = 0.16758570\n",
            "Iteration 137, loss = 0.16718697\n",
            "Iteration 138, loss = 0.16663779\n",
            "Iteration 139, loss = 0.16617392\n",
            "Iteration 140, loss = 0.16569326\n",
            "Iteration 141, loss = 0.16525153\n",
            "Iteration 142, loss = 0.16468164\n",
            "Iteration 143, loss = 0.16430510\n",
            "Iteration 144, loss = 0.16380507\n",
            "Iteration 145, loss = 0.16322082\n",
            "Iteration 146, loss = 0.16285413\n",
            "Iteration 147, loss = 0.16236933\n",
            "Iteration 148, loss = 0.16187923\n",
            "Iteration 149, loss = 0.16150412\n",
            "Iteration 150, loss = 0.16101082\n",
            "Iteration 151, loss = 0.16046369\n",
            "Iteration 152, loss = 0.16008875\n",
            "Iteration 153, loss = 0.15968436\n",
            "Iteration 154, loss = 0.15928907\n",
            "Iteration 155, loss = 0.15877192\n",
            "Iteration 156, loss = 0.15829911\n",
            "Iteration 157, loss = 0.15790992\n",
            "Iteration 158, loss = 0.15749211\n",
            "Iteration 159, loss = 0.15717034\n",
            "Iteration 160, loss = 0.15660255\n",
            "Iteration 161, loss = 0.15614118\n",
            "Iteration 162, loss = 0.15576931\n",
            "Iteration 163, loss = 0.15547364\n",
            "Iteration 164, loss = 0.15504202\n",
            "Iteration 165, loss = 0.15468560\n",
            "Iteration 166, loss = 0.15419338\n",
            "Iteration 167, loss = 0.15383906\n",
            "Iteration 168, loss = 0.15341479\n",
            "Iteration 169, loss = 0.15314030\n",
            "Iteration 170, loss = 0.15271257\n",
            "Iteration 171, loss = 0.15227317\n",
            "Iteration 172, loss = 0.15189414\n",
            "Iteration 173, loss = 0.15159214\n",
            "Iteration 174, loss = 0.15125860\n",
            "Iteration 175, loss = 0.15082184\n",
            "Iteration 176, loss = 0.15038460\n",
            "Iteration 177, loss = 0.15015916\n",
            "Iteration 178, loss = 0.14968966\n",
            "Iteration 179, loss = 0.14928607\n",
            "Iteration 180, loss = 0.14892466\n",
            "Iteration 181, loss = 0.14862231\n",
            "Iteration 182, loss = 0.14837189\n",
            "Iteration 183, loss = 0.14796592\n",
            "Iteration 184, loss = 0.14762834\n",
            "Iteration 185, loss = 0.14719804\n",
            "Iteration 186, loss = 0.14690377\n",
            "Iteration 187, loss = 0.14663135\n",
            "Iteration 188, loss = 0.14619998\n",
            "Iteration 189, loss = 0.14589663\n",
            "Iteration 190, loss = 0.14556082\n",
            "Iteration 191, loss = 0.14527855\n",
            "Iteration 192, loss = 0.14483881\n",
            "Iteration 193, loss = 0.14452074\n",
            "Iteration 194, loss = 0.14427426\n",
            "Iteration 195, loss = 0.14390485\n",
            "Iteration 196, loss = 0.14348461\n",
            "Iteration 197, loss = 0.14331446\n",
            "Iteration 198, loss = 0.14294861\n",
            "Iteration 199, loss = 0.14273632\n",
            "Iteration 200, loss = 0.14235388\n",
            "Iteration 201, loss = 0.14203129\n",
            "Iteration 202, loss = 0.14172685\n",
            "Iteration 203, loss = 0.14136397\n",
            "Iteration 204, loss = 0.14107164\n",
            "Iteration 205, loss = 0.14076904\n",
            "Iteration 206, loss = 0.14048498\n",
            "Iteration 207, loss = 0.14026966\n",
            "Iteration 208, loss = 0.13997132\n",
            "Iteration 209, loss = 0.13964572\n",
            "Iteration 210, loss = 0.13935184\n",
            "Iteration 211, loss = 0.13901527\n",
            "Iteration 212, loss = 0.13871966\n",
            "Iteration 213, loss = 0.13852350\n",
            "Iteration 214, loss = 0.13819859\n",
            "Iteration 215, loss = 0.13789828\n",
            "Iteration 216, loss = 0.13765287\n",
            "Iteration 217, loss = 0.13731660\n",
            "Iteration 218, loss = 0.13705420\n",
            "Iteration 219, loss = 0.13671918\n",
            "Iteration 220, loss = 0.13649600\n",
            "Iteration 221, loss = 0.13626154\n",
            "Iteration 222, loss = 0.13588394\n",
            "Iteration 223, loss = 0.13564488\n",
            "Iteration 224, loss = 0.13540303\n",
            "Iteration 225, loss = 0.13509427\n",
            "Iteration 226, loss = 0.13483300\n",
            "Iteration 227, loss = 0.13460529\n",
            "Iteration 228, loss = 0.13440162\n",
            "Iteration 229, loss = 0.13407579\n",
            "Iteration 230, loss = 0.13396828\n",
            "Iteration 231, loss = 0.13355243\n",
            "Iteration 232, loss = 0.13326894\n",
            "Iteration 233, loss = 0.13311958\n",
            "Iteration 234, loss = 0.13278086\n",
            "Iteration 235, loss = 0.13258732\n",
            "Iteration 236, loss = 0.13230458\n",
            "Iteration 237, loss = 0.13206782\n",
            "Iteration 238, loss = 0.13181532\n",
            "Iteration 239, loss = 0.13154495\n",
            "Iteration 240, loss = 0.13137746\n",
            "Iteration 241, loss = 0.13108891\n",
            "Iteration 242, loss = 0.13085605\n",
            "Iteration 243, loss = 0.13052048\n",
            "Iteration 244, loss = 0.13039276\n",
            "Iteration 245, loss = 0.13007715\n",
            "Iteration 246, loss = 0.12981794\n",
            "Iteration 247, loss = 0.12963708\n",
            "Iteration 248, loss = 0.12940257\n",
            "Iteration 249, loss = 0.12916490\n",
            "Iteration 250, loss = 0.12900858\n",
            "==========================\n",
            "==========================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (250) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score of train2 data: 96.47166666666666\n",
            "==========================\n",
            "time taken to run: 286.69135541300057\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2-1 Comments:\n",
        "\n",
        "With 25 layers, the model does not reach **100%**. The accuracy of the modified training set **%96.47** scored lower than the accuracy of the original training set **%96.82**.\n",
        "    \n",
        "</font>"
      ],
      "metadata": {
        "id": "5At7ECAQY1_t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "Y-3zk90UufrX"
      },
      "outputs": [],
      "source": [
        "# For grader use only\n",
        "\n",
        "# insert grade here\n",
        "# G[2,0] =   \n",
        "# G[2,1] = \n",
        "# G[2,2] =\n",
        "\n",
        "\n",
        "maxScore = maxScore + 12\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "fydXo8GRGkbp"
      },
      "outputs": [],
      "source": [
        "# for grader use\n",
        "\n",
        "# Total Grade Calculation\n",
        "rawScore = np.sum(G)\n",
        "score = rawScore*100/maxScore"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "wmd_assignment5.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}